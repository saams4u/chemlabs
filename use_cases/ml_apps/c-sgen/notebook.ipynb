{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p9TlCfd8PMj"
      },
      "source": [
        "## C-SGEN\n",
        "\n",
        "A PyTorch implementation of \"Molecule Property Prediction based on Spatial Graph Embedding\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU5VwGLD9PxM",
        "outputId": "1333db8b-db50-4dfa-9b0f-36cde3940a47"
      },
      "source": [
        "!git clone https://github.com/wxfsd/C-SGEN.git\n",
        "%cd /content/C-SGEN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/C-SGEN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYx31A8J-6u9"
      },
      "source": [
        "# Install dependencies / requirements\n",
        "!pip install theano==1.0.3 numpy==1.16.4 scipy==1.3.0\n",
        "!pip install sklearn==0.0 deepchem torch==1.4.0 torchvision==0.5.0 torchtext==0.5.0\n",
        "\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse==latest+cu101 \\\n",
        "  torch-scatter==latest+cu101 \\\n",
        "  torch-cluster==latest+cu101 \\\n",
        "  torch-spline-conv==latest+cu101 \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "\n",
        "# Install RDKit \n",
        "!pip install rdkit-pypi==2021.3.1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfguPLHV81-y"
      },
      "source": [
        "### Example: C-SGEN_train\n",
        "\n",
        "Training C-SGEN model on the default dataset, the data is ready to be saved in a folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AgpTuwB8Jzv"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timeit\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEl7-6xz8vZT"
      },
      "source": [
        "def load_pickle(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def rms_score(y_true, y_pred):\n",
        "    \"\"\"Computes RMS error.\"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "def load_tensor(file_name, dtype):\n",
        "    return [dtype(d).to(device) for d in np.load(file_name + '.npy')]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGmaCv4N9H1W"
      },
      "source": [
        "class mydataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,dataset):\n",
        "\n",
        "        self.Features = load_tensor('./FreeSolv/decrease/' + dataset +'/Features', torch.FloatTensor)\n",
        "        self.Normed_adj = load_tensor('./FreeSolv/decrease/' + dataset +'/Normed_adj', torch.FloatTensor)\n",
        "        self.Fringer = load_tensor('./FreeSolv/decrease/' + dataset +'/fingerprint_stand', torch.FloatTensor)\n",
        "        self.interactions = load_tensor('./FreeSolv/decrease/' + dataset +'/Interactions', torch.FloatTensor)\n",
        "\n",
        "        self.dataset = list(zip(np.array(self.Features), np.array(self.Normed_adj), np.array(self.Fringer),np.array(self.interactions)))\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data_batch = self.dataset[item]\n",
        "\n",
        "        return data_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCm2dmE19XFE"
      },
      "source": [
        "class C_SGEN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(C_SGEN, self).__init__()\n",
        "        self.layer1 = nn.Linear(75, 4 * ch_num)\n",
        "        self.dropout_feature = nn.Dropout(p=0.5)\n",
        "        self.dropout_adj = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.conv1d_1 = nn.Conv1d(in_channels=4 * ch_num, out_channels=5 * ch_num // 2, kernel_size=(k + 1) // 2 + 1)\n",
        "        self.conv1d_2 = nn.Conv1d(in_channels=5 * ch_num // 2, out_channels=ch_num, kernel_size=k // 2 + 1)\n",
        "        self.bn = nn.BatchNorm1d(ch_num)\n",
        "\n",
        "        self.conv1d_3 = nn.Conv1d(in_channels=5 * ch_num, out_channels=6 * ch_num // 2, kernel_size=(k + 1) // 2 + 1)\n",
        "        self.conv1d_4 = nn.Conv1d(in_channels=6 * ch_num // 2, out_channels=ch_num, kernel_size=k // 2 + 1)\n",
        "\n",
        "        self.conv1d_5 = nn.Conv1d(in_channels=6 * ch_num, out_channels=7 * ch_num // 2, kernel_size=(k + 1) // 2 + 1)\n",
        "        self.conv1d_6 = nn.Conv1d(in_channels=7 * ch_num // 2, out_channels=ch_num, kernel_size=k // 2 + 1)\n",
        "\n",
        "        self.conv1d_7 = nn.Conv1d(in_channels=7 * ch_num, out_channels=8 * ch_num // 2, kernel_size=(k + 1) // 2 + 1)\n",
        "        self.conv1d_8 = nn.Conv1d(in_channels=8 * ch_num // 2, out_channels=ch_num, kernel_size=k // 2 + 1)\n",
        "\n",
        "        self.conv1d_9 = nn.Conv1d(in_channels=8 * ch_num, out_channels=9 * ch_num // 2, kernel_size=(k + 1) // 2 + 1)\n",
        "        self.conv1d_10 = nn.Conv1d(in_channels=9 * ch_num // 2, out_channels=ch_num, kernel_size=k // 2 + 1)\n",
        "\n",
        "        self.conv1d_11 = nn.Conv1d(in_channels=9 * ch_num, out_channels=10 * ch_num // 2, kernel_size=(k + 1) // 2 + 1)\n",
        "        self.conv1d_12 = nn.Conv1d(in_channels=10 * ch_num // 2, out_channels=ch_num, kernel_size=k // 2 + 1)\n",
        "\n",
        "        self.layer2 = nn.Linear(6 * ch_num, ch_num)\n",
        "\n",
        "        self.predict_property = nn.Linear(ch_num+363, 1)\n",
        "        self.conv1d = nn.Conv1d(batch * 16, k, 3, stride=1, padding=1)\n",
        "\n",
        "        self.W_cnn = nn.ModuleList([nn.Conv2d(\n",
        "            in_channels=1, out_channels=1, kernel_size=2 * window + 1,\n",
        "            stride=1, padding=window) for _ in range(3)])\n",
        "\n",
        "        self.cnn_line = nn.Linear(363, 363)\n",
        "\n",
        "        self.dnn1 = nn.Linear(363, 512)\n",
        "        self.dnn2 = nn.Linear(512, 1024)\n",
        "        self.dnn3 = nn.Linear(1024, 363)\n",
        "\n",
        "    def pad(self, matrices, value):\n",
        "        \"\"\"Pad adjacency matrices for batch processing.\"\"\"\n",
        "        sizes = [d.shape[0] for d in matrices]\n",
        "        D = sum(sizes)\n",
        "        pad_matrices = value + np.zeros((D, D))\n",
        "        m = 0\n",
        "        for i, d in enumerate(matrices):\n",
        "            s_i = sizes[i]\n",
        "            pad_matrices[m:m + s_i, m:m + s_i] = d.cpu()\n",
        "            m += s_i\n",
        "        return torch.FloatTensor(pad_matrices).to(device)\n",
        "\n",
        "    def sum_axis(self, xs, axis):\n",
        "        y = list(map(lambda x: torch.sum(x, 0), torch.split(xs, axis)))\n",
        "        return torch.stack(y)\n",
        "\n",
        "    def simple_conv1(self, Normed_adj, Features):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.dropout_feature(Features)\n",
        "        outs = self.layer1(outs)\n",
        "        outs = torch.matmul(adj_m, outs)\n",
        "        return outs\n",
        "\n",
        "    def DNN(self, x_words):\n",
        "\n",
        "        x_words = F.relu(self.dnn1(x_words))\n",
        "        x_words = F.relu(self.dnn2(x_words))\n",
        "        x_words = self.dnn3(x_words)\n",
        "\n",
        "        return x_words\n",
        "\n",
        "    def cnn_process(self, x, layer):\n",
        "        \"\"\"Controlled experiments, CNN Processing Molecular Fingerprints.\"\"\"\n",
        "        for i in range(layer):\n",
        "            hs = self.cnn(x, i)\n",
        "            x = torch.relu(self.cnn_line(hs))\n",
        "        return x\n",
        "\n",
        "    def cnn(self, xs, i):\n",
        "        xs = torch.unsqueeze(torch.unsqueeze(xs, 0), 0)\n",
        "        hs = torch.relu(self.W_cnn[i](xs))\n",
        "        return torch.squeeze(torch.squeeze(hs, 0), 0)\n",
        "\n",
        "    def conv1d_spatial_graph_matrix(self, adj_m, fea_m):\n",
        "        \"\"\"\n",
        "        After the 1-d convolution processes spatial graph matrix,\n",
        "        it is concatenated with the initial atomic features.\n",
        "        \"\"\"\n",
        "        adj_m_graph1 = torch.unsqueeze(adj_m, 1)\n",
        "        fea_m_graph1 = torch.unsqueeze(fea_m, -1)\n",
        "        feas = torch.mul(adj_m_graph1, fea_m_graph1)\n",
        "        feas = feas.permute(2, 1, 0)\n",
        "        features = feas.permute(0, 2, 1)\n",
        "\n",
        "        spatial_feature = self.conv1d(features)\n",
        "        spatial_feature = spatial_feature.permute(0, 2, 1)\n",
        "        outs = torch.cat([fea_m_graph1, spatial_feature], 2)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        return outs\n",
        "\n",
        "    def C_SGEL1(self, Normed_adj, input_feature):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.conv1d_spatial_graph_matrix(adj_m, input_feature)\n",
        "        outs = self.dropout_feature(outs)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        outs_conv1d_1 = self.conv1d_1(outs)\n",
        "        outs_conv1d_1_relu = F.relu(outs_conv1d_1)\n",
        "        outs_conv1d_1_relu_drought = self.dropout_feature(outs_conv1d_1_relu)\n",
        "        outs_conv1d_2 = self.conv1d_2(outs_conv1d_1_relu_drought)\n",
        "        outs_conv1d_2_relu = F.relu(outs_conv1d_2)\n",
        "        outs_conv1d_2_permute = outs_conv1d_2_relu.permute(0, 2, 1)\n",
        "        outs_conv1d_2_permute_unbind = torch.unbind(outs_conv1d_2_permute, dim=1)\n",
        "        outs_conv1d_2_batch_norm = self.bn(outs_conv1d_2_permute_unbind[0])\n",
        "        return outs_conv1d_2_batch_norm\n",
        "\n",
        "    def C_SGEL2(self, Normed_adj, input_feature):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.conv1d_spatial_graph_matrix(adj_m, input_feature)\n",
        "        outs = self.dropout_feature(outs)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        outs_conv1d_3 = self.conv1d_3(outs)\n",
        "        outs_conv1d_3_relu = F.relu(outs_conv1d_3)\n",
        "        outs_conv1d_3_relu_drought = self.dropout_feature(outs_conv1d_3_relu)\n",
        "\n",
        "        outs_conv1d_4 = self.conv1d_4(outs_conv1d_3_relu_drought)\n",
        "        outs_conv1d_4 = F.relu(outs_conv1d_4)\n",
        "        outs_conv1d_4_permute = outs_conv1d_4.permute(0, 2, 1)\n",
        "        outs_conv1d_4_permute_unbind = torch.unbind(outs_conv1d_4_permute, dim=1)\n",
        "        outs_conv1d_4_batch_norm = self.bn(outs_conv1d_4_permute_unbind[0])\n",
        "        return outs_conv1d_4_batch_norm\n",
        "\n",
        "    def C_SGEL3(self, Normed_adj, input_feature):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.conv1d_spatial_graph_matrix(adj_m, input_feature)\n",
        "        outs = self.dropout_feature(outs)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        outs_conv1d_5 = self.conv1d_5(outs)\n",
        "        outs_conv1d_5_relu = F.relu(outs_conv1d_5)\n",
        "        outs_conv1d_5_relu_drought = self.dropout_feature(outs_conv1d_5_relu)\n",
        "        outs_conv1d_6 = self.conv1d_6(outs_conv1d_5_relu_drought)\n",
        "        outs_conv1d_6 = F.relu(outs_conv1d_6)\n",
        "        outs_conv1d_6_permute = outs_conv1d_6.permute(0, 2, 1)\n",
        "        outs_conv1d_6_permute_unbind = torch.unbind(outs_conv1d_6_permute, dim=1)\n",
        "        outs_conv1d_6_batch_norm = self.bn(outs_conv1d_6_permute_unbind[0])\n",
        "        return outs_conv1d_6_batch_norm\n",
        "\n",
        "    def C_SGEL4(self, Normed_adj, input_feature):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.conv1d_spatial_graph_matrix(adj_m, input_feature)\n",
        "        outs = self.dropout_feature(outs)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        outs_conv1d_7 = self.conv1d_7(outs)\n",
        "        outs_conv1d_7_relu = F.relu(outs_conv1d_7)\n",
        "        outs_conv1d_7_relu_drought = self.dropout_feature(outs_conv1d_7_relu)\n",
        "        outs_conv1d_8 = self.conv1d_8(outs_conv1d_7_relu_drought)\n",
        "        outs_conv1d_8 = F.relu(outs_conv1d_8)\n",
        "        outs_conv1d_8_permute = outs_conv1d_8.permute(0, 2, 1)\n",
        "        outs_conv1d_8_permute_unbind = torch.unbind(outs_conv1d_8_permute, dim=1)\n",
        "        outs_conv1d_8_batch_norm = self.bn(outs_conv1d_8_permute_unbind[0])\n",
        "        return outs_conv1d_8_batch_norm\n",
        "\n",
        "    def C_SGEL5(self, Normed_adj, input_feature):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.conv1d_spatial_graph_matrix(adj_m, input_feature)\n",
        "        outs = self.dropout_feature(outs)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        outs_conv1d_9 = self.conv1d_9(outs)\n",
        "        outs_conv1d_9_relu = F.relu(outs_conv1d_9)\n",
        "        outs_conv1d_9_relu_drought = self.dropout_feature(outs_conv1d_9_relu)\n",
        "        outs_conv1d_10 = self.conv1d_10(outs_conv1d_9_relu_drought)\n",
        "        outs_conv1d_10 = F.relu(outs_conv1d_10)\n",
        "        outs_conv1d_10_permute = outs_conv1d_10.permute(0, 2, 1)\n",
        "        outs_conv1d_10_permute_unbind = torch.unbind(outs_conv1d_10_permute, dim=1)\n",
        "        outs_conv1d_10_batch_norm = self.bn(outs_conv1d_10_permute_unbind[0])\n",
        "        return outs_conv1d_10_batch_norm\n",
        "\n",
        "    def C_SGEL6(self, Normed_adj, input_feature):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.conv1d_spatial_graph_matrix(adj_m, input_feature)\n",
        "        outs = self.dropout_feature(outs)\n",
        "        outs = outs.permute(0, 2, 1)\n",
        "        outs_conv1d_11 = self.conv1d_11(outs)\n",
        "        outs_conv1d_11_relu = F.relu(outs_conv1d_11)\n",
        "        outs_conv1d_11_relu_drought = self.dropout_feature(outs_conv1d_11_relu)\n",
        "\n",
        "        outs_conv1d_12 = self.conv1d_12(outs_conv1d_11_relu_drought)\n",
        "\n",
        "        outs_conv1d_12 = F.relu(outs_conv1d_12)\n",
        "        outs_conv1d_12_permute = outs_conv1d_12.permute(0, 2, 1)\n",
        "        outs_conv1d_12_permute_unbind = torch.unbind(outs_conv1d_12_permute, dim=1)\n",
        "        outs_conv1d_12_batch_norm = self.bn(outs_conv1d_12_permute_unbind[0])\n",
        "        return outs_conv1d_12_batch_norm\n",
        "\n",
        "    def simple_conv2(self, Normed_adj, Features):\n",
        "        adj_m = self.dropout_adj(Normed_adj)\n",
        "        outs = self.dropout_feature(Features)\n",
        "        outs = self.layer2(outs)\n",
        "        outs = torch.matmul(adj_m, outs)\n",
        "        return outs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        Features, Normed_adj, Fringer = list(inputs[0]), list(inputs[1]), list(inputs[2])\n",
        "\n",
        "        axis = list(map(lambda x: len(x), Features))\n",
        "\n",
        "        Features = torch.cat(Features)\n",
        "        Normed_adj = self.pad(Normed_adj, 0)\n",
        "\n",
        "        Fringer = list(Fringer)\n",
        "        for i in range(len(Fringer)):\n",
        "            Fringer[i] = torch.unsqueeze(Fringer[i], 0)\n",
        "        Fringer = torch.cat(Fringer, 0)\n",
        "\n",
        "        # Graph embedding layer\n",
        "        outs1 = self.simple_conv1(Normed_adj, Features)\n",
        "\n",
        "        # Layer 1 Convolution Spatial Graph Embedding layer\n",
        "        cur_outs1 = self.C_SGEL1(Normed_adj, outs1)\n",
        "        # Skip connection\n",
        "        outs2 = torch.cat((outs1, cur_outs1), 1)\n",
        "\n",
        "        # Layer 2 Convolution Spatial Graph Embedding layer\n",
        "        cur_outs2 = self.C_SGEL2(Normed_adj, outs2)\n",
        "        # Skip connection\n",
        "        outs3 = torch.cat((outs2, cur_outs2), 1)\n",
        "\n",
        "        # Graph-gather layer\n",
        "        outs = self.simple_conv2(Normed_adj, outs3)\n",
        "        y_molecules = self.sum_axis(outs, axis)\n",
        "\n",
        "        # Deep neural network for molecular fingerprint\n",
        "        Fringer = self.DNN(Fringer)\n",
        "\n",
        "        # Concatenate molecule and fingerprint\n",
        "        y_molecules = torch.cat((y_molecules, Fringer), 1)\n",
        "\n",
        "        # Prediction of Molecular Properties by Fully Connected Layer\n",
        "        z_molecules = self.predict_property(y_molecules)\n",
        "\n",
        "        return z_molecules\n",
        "\n",
        "    def __call__(self, data_batch, std, mean, train=True):\n",
        "\n",
        "        inputs, t_interaction = data_batch[:-1], torch.squeeze(data_batch[-1])\n",
        "        z_interaction = self.forward(inputs)\n",
        "\n",
        "        if train:\n",
        "            t_interaction = torch.unsqueeze(t_interaction, 1)\n",
        "            loss = F.mse_loss(z_interaction, t_interaction)\n",
        "            return loss\n",
        "\n",
        "        else:\n",
        "            t_interaction = torch.unsqueeze(t_interaction, 1)\n",
        "            loss = F.mse_loss(z_interaction, t_interaction)\n",
        "            z = z_interaction.to('cpu').data.numpy()\n",
        "            t = t_interaction.to('cpu').data.numpy()\n",
        "            z, t = std * z + mean, std * t + mean\n",
        "            return loss, z, t"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1uatnUG9plv"
      },
      "source": [
        "class Trainer(object):\n",
        "    \n",
        "    def __init__(self, model, std, mean):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        loss_total = 0\n",
        "        num = 0\n",
        "        for data in train_loader:\n",
        "            num += 1\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.model(data, std=self.std, mean=self.mean, train=True)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            loss_total += loss.to('cpu').data.numpy()\n",
        "        loss_mean = loss_total/num\n",
        "        return loss_mean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ReQFobK9tki"
      },
      "source": [
        "class T(object):\n",
        "    \n",
        "    def __init__(self, model, std, mean):\n",
        "        self.model = model\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def test(self, test_loader):\n",
        "\n",
        "        loss_total = 0\n",
        "        all_p = []\n",
        "        all_t = []\n",
        "\n",
        "        num = 0\n",
        "        for data in test_loader:\n",
        "            num += 1\n",
        "            loss, predicted, true = self.model(data, std=self.std, mean=self.mean, train=False)\n",
        "\n",
        "            for i in predicted:\n",
        "                all_p.append(float(i))\n",
        "            for i in true:\n",
        "                all_t.append(float(i))\n",
        "            loss_total += loss.to('cpu').data.numpy()\n",
        "\n",
        "        RMSE = rms_score(all_t, all_p)\n",
        "        loss_mean = loss_total / num\n",
        "        return loss_mean, RMSE, all_p, all_t"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzoTWEtf9w5P"
      },
      "source": [
        "def metric(RMSE_k_test):\n",
        "    RMSE_mean_test = np.mean(np.array(RMSE_k_test))\n",
        "    RMSE_std_test = np.std(np.array(RMSE_k_test))\n",
        "\n",
        "    return RMSE_mean_test, RMSE_std_test"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXDG0xZb99XV"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "std = 3.8448222046029543\n",
        "mean = -3.8030062305295975\n",
        "iteration = 33\n",
        "window = 5\n",
        "layer_cnn = 3\n",
        "batch = 8\n",
        "k = 4\n",
        "ch_num = 4\n",
        "decay_interval = 10\n",
        "lr = 5e-4\n",
        "lr_decay = 0.5\n",
        "lr, lr_decay = map(float, [lr, lr_decay])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLxcc3Qo9_2U",
        "outputId": "233bfef4-758f-4013-bb38-272bb03b06ed"
      },
      "source": [
        "setting = 'FreeSolv--' \\\n",
        "          '--batch' + str(batch) + \\\n",
        "          '--k' + str(k) + \\\n",
        "          '--lr-' + str(lr) + \\\n",
        "          '--iteration-' + str(iteration)+\\\n",
        "            '--ch_num-' + str(4*ch_num)+\\\n",
        "            '--decay_interval-' + str(decay_interval)\n",
        "\n",
        "print(setting)\n",
        "print('batch:', batch)\n",
        "print('k:', k)\n",
        "print('ch_num:', str(4*ch_num))\n",
        "print('decay_interval:', decay_interval)\n",
        "print('lr:', lr)\n",
        "print('lr_decay:', lr_decay)\n",
        "print('iteration:', iteration)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FreeSolv----batch8--k4--lr-0.0005--iteration-33--ch_num-16--decay_interval-10\n",
            "batch: 8\n",
            "k: 4\n",
            "ch_num: 16\n",
            "decay_interval: 10\n",
            "lr: 0.0005\n",
            "lr_decay: 0.5\n",
            "iteration: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkKygW7--GSW",
        "outputId": "ea381404-89ad-4603-c877-e0da9aa50d26"
      },
      "source": [
        "import sys\n",
        "\n",
        "print('Current python interpreter path：')\n",
        "print(sys.executable)\n",
        "print('Epoch Time(sec) Loss_train Loss_dev Loss_test RMSE_train RMSE_dev RMSE_test')\n",
        "\n",
        "RMSE_k_valid = []\n",
        "RMSE_k_test = []\n",
        "\n",
        "seed_list = [256,512,1024]\n",
        "\n",
        "train_dataset = mydataset('train_data')\n",
        "valid_dataset = mydataset('valid_data')\n",
        "test_dataset = mydataset('test_data')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True, drop_last=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current python interpreter path：\n",
            "/usr/bin/python3\n",
            "Epoch Time(sec) Loss_train Loss_dev Loss_test RMSE_train RMSE_dev RMSE_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyCd-gpt-LuA",
        "outputId": "ed4d16d3-60f7-484b-8d0a-d0281d89b970"
      },
      "source": [
        "for seed in seed_list:\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    model = C_SGEN().to(device)\n",
        "\n",
        "    trainer = Trainer(model.train(), std, mean)\n",
        "    tester = T(model.eval(), std, mean)\n",
        "\n",
        "    Loss_train = []\n",
        "    Loss_valid = []\n",
        "    Loss_test = []\n",
        "\n",
        "    for epoch in range(1, (iteration + 1)):\n",
        "\n",
        "        if epoch  % decay_interval == 0:\n",
        "            trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
        "        start = timeit.default_timer()\n",
        "\n",
        "        train_loss = trainer.train(train_loader)\n",
        "        valid_loss, RMSE_valid, predicted_valid, true_valid = tester.test(valid_loader)\n",
        "        test_loss, RMSE_test, predicted_test, true_test = tester.test(test_loader)\n",
        "\n",
        "        Loss_train.append(train_loss)\n",
        "        Loss_valid.append(valid_loss)\n",
        "        Loss_test.append(test_loss)\n",
        "\n",
        "        end = timeit.default_timer()\n",
        "        time = end - start\n",
        "\n",
        "        print(\n",
        "            'epoch:%d-train loss: %.3f,valid loss: %.3f,test loss: %.3f, valid rmse: %.3f, test rmse: %.3f, time: %.3f' %\n",
        "            (epoch, train_loss, valid_loss, test_loss, RMSE_valid, RMSE_test, time))\n",
        "\n",
        "        if epoch == iteration:\n",
        "\n",
        "            RMSE_k_valid.append(RMSE_valid)\n",
        "            RMSE_k_test.append(RMSE_test)\n",
        "\n",
        "    print('RMSE_k_valid', RMSE_k_valid)\n",
        "    print('RMSE_k_test', RMSE_k_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:1-train loss: 0.398,valid loss: 0.252,test loss: 0.165, valid rmse: 1.929, test rmse: 1.563, time: 0.633\n",
            "epoch:2-train loss: 0.175,valid loss: 0.181,test loss: 0.171, valid rmse: 1.635, test rmse: 1.591, time: 0.489\n",
            "epoch:3-train loss: 0.120,valid loss: 0.105,test loss: 0.092, valid rmse: 1.248, test rmse: 1.166, time: 0.480\n",
            "epoch:4-train loss: 0.052,valid loss: 0.145,test loss: 0.087, valid rmse: 1.464, test rmse: 1.132, time: 0.497\n",
            "epoch:5-train loss: 0.062,valid loss: 0.108,test loss: 0.057, valid rmse: 1.261, test rmse: 0.919, time: 0.492\n",
            "epoch:6-train loss: 0.044,valid loss: 0.128,test loss: 0.075, valid rmse: 1.374, test rmse: 1.056, time: 0.472\n",
            "epoch:7-train loss: 0.026,valid loss: 0.113,test loss: 0.071, valid rmse: 1.295, test rmse: 1.022, time: 0.484\n",
            "epoch:8-train loss: 0.036,valid loss: 0.116,test loss: 0.092, valid rmse: 1.309, test rmse: 1.167, time: 0.484\n",
            "epoch:9-train loss: 0.078,valid loss: 0.152,test loss: 0.094, valid rmse: 1.498, test rmse: 1.177, time: 0.468\n",
            "epoch:10-train loss: 0.038,valid loss: 0.121,test loss: 0.066, valid rmse: 1.335, test rmse: 0.990, time: 0.456\n",
            "epoch:11-train loss: 0.017,valid loss: 0.097,test loss: 0.046, valid rmse: 1.197, test rmse: 0.828, time: 0.491\n",
            "epoch:12-train loss: 0.010,valid loss: 0.089,test loss: 0.059, valid rmse: 1.147, test rmse: 0.936, time: 0.502\n",
            "epoch:13-train loss: 0.009,valid loss: 0.110,test loss: 0.058, valid rmse: 1.276, test rmse: 0.927, time: 0.478\n",
            "epoch:14-train loss: 0.008,valid loss: 0.106,test loss: 0.051, valid rmse: 1.252, test rmse: 0.864, time: 0.486\n",
            "epoch:15-train loss: 0.007,valid loss: 0.105,test loss: 0.056, valid rmse: 1.247, test rmse: 0.911, time: 0.522\n",
            "epoch:16-train loss: 0.007,valid loss: 0.097,test loss: 0.052, valid rmse: 1.199, test rmse: 0.878, time: 0.501\n",
            "epoch:17-train loss: 0.006,valid loss: 0.104,test loss: 0.060, valid rmse: 1.242, test rmse: 0.940, time: 0.473\n",
            "epoch:18-train loss: 0.004,valid loss: 0.100,test loss: 0.055, valid rmse: 1.215, test rmse: 0.899, time: 0.487\n",
            "epoch:19-train loss: 0.003,valid loss: 0.099,test loss: 0.050, valid rmse: 1.210, test rmse: 0.857, time: 0.499\n",
            "epoch:20-train loss: 0.003,valid loss: 0.099,test loss: 0.050, valid rmse: 1.207, test rmse: 0.858, time: 0.494\n",
            "epoch:21-train loss: 0.002,valid loss: 0.097,test loss: 0.051, valid rmse: 1.200, test rmse: 0.870, time: 0.495\n",
            "epoch:22-train loss: 0.002,valid loss: 0.098,test loss: 0.051, valid rmse: 1.202, test rmse: 0.866, time: 0.482\n",
            "epoch:23-train loss: 0.002,valid loss: 0.098,test loss: 0.051, valid rmse: 1.205, test rmse: 0.865, time: 0.490\n",
            "epoch:24-train loss: 0.002,valid loss: 0.098,test loss: 0.050, valid rmse: 1.205, test rmse: 0.863, time: 0.486\n",
            "epoch:25-train loss: 0.001,valid loss: 0.098,test loss: 0.052, valid rmse: 1.206, test rmse: 0.873, time: 0.483\n",
            "epoch:26-train loss: 0.001,valid loss: 0.099,test loss: 0.046, valid rmse: 1.208, test rmse: 0.822, time: 0.506\n",
            "epoch:27-train loss: 0.002,valid loss: 0.100,test loss: 0.049, valid rmse: 1.216, test rmse: 0.851, time: 0.482\n",
            "epoch:28-train loss: 0.002,valid loss: 0.102,test loss: 0.051, valid rmse: 1.225, test rmse: 0.871, time: 0.482\n",
            "epoch:29-train loss: 0.002,valid loss: 0.100,test loss: 0.047, valid rmse: 1.213, test rmse: 0.836, time: 0.496\n",
            "epoch:30-train loss: 0.001,valid loss: 0.098,test loss: 0.045, valid rmse: 1.202, test rmse: 0.819, time: 0.488\n",
            "epoch:31-train loss: 0.001,valid loss: 0.102,test loss: 0.049, valid rmse: 1.227, test rmse: 0.855, time: 0.477\n",
            "epoch:32-train loss: 0.001,valid loss: 0.103,test loss: 0.050, valid rmse: 1.232, test rmse: 0.856, time: 0.485\n",
            "epoch:33-train loss: 0.001,valid loss: 0.098,test loss: 0.046, valid rmse: 1.206, test rmse: 0.828, time: 0.480\n",
            "RMSE_k_valid [1.2060235414989682]\n",
            "RMSE_k_test [0.8279231492694524]\n",
            "epoch:1-train loss: 0.360,valid loss: 0.176,test loss: 0.141, valid rmse: 1.612, test rmse: 1.445, time: 0.491\n",
            "epoch:2-train loss: 0.121,valid loss: 0.167,test loss: 0.108, valid rmse: 1.569, test rmse: 1.263, time: 0.487\n",
            "epoch:3-train loss: 0.083,valid loss: 0.153,test loss: 0.087, valid rmse: 1.505, test rmse: 1.132, time: 0.469\n",
            "epoch:4-train loss: 0.131,valid loss: 0.124,test loss: 0.092, valid rmse: 1.355, test rmse: 1.164, time: 0.470\n",
            "epoch:5-train loss: 0.047,valid loss: 0.134,test loss: 0.064, valid rmse: 1.408, test rmse: 0.972, time: 0.474\n",
            "epoch:6-train loss: 0.035,valid loss: 0.157,test loss: 0.053, valid rmse: 1.522, test rmse: 0.882, time: 0.471\n",
            "epoch:7-train loss: 0.022,valid loss: 0.119,test loss: 0.060, valid rmse: 1.325, test rmse: 0.943, time: 0.469\n",
            "epoch:8-train loss: 0.017,valid loss: 0.101,test loss: 0.067, valid rmse: 1.224, test rmse: 0.992, time: 0.461\n",
            "epoch:9-train loss: 0.020,valid loss: 0.113,test loss: 0.055, valid rmse: 1.294, test rmse: 0.899, time: 0.467\n",
            "epoch:10-train loss: 0.010,valid loss: 0.100,test loss: 0.052, valid rmse: 1.219, test rmse: 0.875, time: 0.489\n",
            "epoch:11-train loss: 0.007,valid loss: 0.104,test loss: 0.052, valid rmse: 1.238, test rmse: 0.874, time: 0.481\n",
            "epoch:12-train loss: 0.006,valid loss: 0.096,test loss: 0.047, valid rmse: 1.193, test rmse: 0.833, time: 0.487\n",
            "epoch:13-train loss: 0.005,valid loss: 0.097,test loss: 0.055, valid rmse: 1.195, test rmse: 0.898, time: 0.500\n",
            "epoch:14-train loss: 0.004,valid loss: 0.104,test loss: 0.048, valid rmse: 1.242, test rmse: 0.838, time: 0.477\n",
            "epoch:15-train loss: 0.005,valid loss: 0.097,test loss: 0.047, valid rmse: 1.195, test rmse: 0.837, time: 0.488\n",
            "epoch:16-train loss: 0.004,valid loss: 0.105,test loss: 0.053, valid rmse: 1.244, test rmse: 0.882, time: 0.506\n",
            "epoch:17-train loss: 0.004,valid loss: 0.101,test loss: 0.052, valid rmse: 1.219, test rmse: 0.880, time: 0.478\n",
            "epoch:18-train loss: 0.003,valid loss: 0.096,test loss: 0.054, valid rmse: 1.194, test rmse: 0.892, time: 0.497\n",
            "epoch:19-train loss: 0.003,valid loss: 0.108,test loss: 0.051, valid rmse: 1.264, test rmse: 0.866, time: 0.489\n",
            "epoch:20-train loss: 0.003,valid loss: 0.099,test loss: 0.053, valid rmse: 1.211, test rmse: 0.887, time: 0.480\n",
            "epoch:21-train loss: 0.002,valid loss: 0.096,test loss: 0.052, valid rmse: 1.191, test rmse: 0.873, time: 0.489\n",
            "epoch:22-train loss: 0.002,valid loss: 0.099,test loss: 0.047, valid rmse: 1.210, test rmse: 0.831, time: 0.491\n",
            "epoch:23-train loss: 0.001,valid loss: 0.096,test loss: 0.049, valid rmse: 1.190, test rmse: 0.847, time: 0.493\n",
            "epoch:24-train loss: 0.001,valid loss: 0.095,test loss: 0.049, valid rmse: 1.188, test rmse: 0.853, time: 0.485\n",
            "epoch:25-train loss: 0.001,valid loss: 0.095,test loss: 0.047, valid rmse: 1.183, test rmse: 0.836, time: 0.460\n",
            "epoch:26-train loss: 0.001,valid loss: 0.095,test loss: 0.045, valid rmse: 1.184, test rmse: 0.816, time: 0.493\n",
            "epoch:27-train loss: 0.001,valid loss: 0.096,test loss: 0.048, valid rmse: 1.191, test rmse: 0.840, time: 0.490\n",
            "epoch:28-train loss: 0.001,valid loss: 0.095,test loss: 0.048, valid rmse: 1.188, test rmse: 0.844, time: 0.496\n",
            "epoch:29-train loss: 0.001,valid loss: 0.097,test loss: 0.052, valid rmse: 1.195, test rmse: 0.875, time: 0.480\n",
            "epoch:30-train loss: 0.001,valid loss: 0.096,test loss: 0.049, valid rmse: 1.191, test rmse: 0.849, time: 0.492\n",
            "epoch:31-train loss: 0.001,valid loss: 0.096,test loss: 0.046, valid rmse: 1.189, test rmse: 0.823, time: 0.488\n",
            "epoch:32-train loss: 0.001,valid loss: 0.096,test loss: 0.047, valid rmse: 1.191, test rmse: 0.837, time: 0.503\n",
            "epoch:33-train loss: 0.001,valid loss: 0.096,test loss: 0.049, valid rmse: 1.194, test rmse: 0.848, time: 0.487\n",
            "RMSE_k_valid [1.2060235414989682, 1.194299562008171]\n",
            "RMSE_k_test [0.8279231492694524, 0.8482180047078691]\n",
            "epoch:1-train loss: 0.357,valid loss: 0.166,test loss: 0.124, valid rmse: 1.567, test rmse: 1.352, time: 0.489\n",
            "epoch:2-train loss: 0.129,valid loss: 0.217,test loss: 0.119, valid rmse: 1.792, test rmse: 1.327, time: 0.489\n",
            "epoch:3-train loss: 0.073,valid loss: 0.203,test loss: 0.145, valid rmse: 1.731, test rmse: 1.464, time: 0.472\n",
            "epoch:4-train loss: 0.047,valid loss: 0.112,test loss: 0.068, valid rmse: 1.289, test rmse: 1.004, time: 0.491\n",
            "epoch:5-train loss: 0.042,valid loss: 0.136,test loss: 0.069, valid rmse: 1.416, test rmse: 1.013, time: 0.480\n",
            "epoch:6-train loss: 0.041,valid loss: 0.127,test loss: 0.058, valid rmse: 1.372, test rmse: 0.925, time: 0.491\n",
            "epoch:7-train loss: 0.065,valid loss: 0.121,test loss: 0.069, valid rmse: 1.336, test rmse: 1.010, time: 0.491\n",
            "epoch:8-train loss: 0.055,valid loss: 0.137,test loss: 0.097, valid rmse: 1.424, test rmse: 1.196, time: 0.489\n",
            "epoch:9-train loss: 0.036,valid loss: 0.112,test loss: 0.079, valid rmse: 1.285, test rmse: 1.079, time: 0.484\n",
            "epoch:10-train loss: 0.019,valid loss: 0.113,test loss: 0.059, valid rmse: 1.290, test rmse: 0.935, time: 0.528\n",
            "epoch:11-train loss: 0.009,valid loss: 0.108,test loss: 0.046, valid rmse: 1.265, test rmse: 0.822, time: 0.504\n",
            "epoch:12-train loss: 0.009,valid loss: 0.110,test loss: 0.056, valid rmse: 1.278, test rmse: 0.908, time: 0.490\n",
            "epoch:13-train loss: 0.008,valid loss: 0.101,test loss: 0.052, valid rmse: 1.221, test rmse: 0.874, time: 0.484\n",
            "epoch:14-train loss: 0.005,valid loss: 0.097,test loss: 0.050, valid rmse: 1.198, test rmse: 0.859, time: 0.492\n",
            "epoch:15-train loss: 0.005,valid loss: 0.109,test loss: 0.064, valid rmse: 1.268, test rmse: 0.970, time: 0.497\n",
            "epoch:16-train loss: 0.004,valid loss: 0.101,test loss: 0.050, valid rmse: 1.220, test rmse: 0.863, time: 0.517\n",
            "epoch:17-train loss: 0.003,valid loss: 0.110,test loss: 0.047, valid rmse: 1.275, test rmse: 0.835, time: 0.531\n",
            "epoch:18-train loss: 0.005,valid loss: 0.098,test loss: 0.039, valid rmse: 1.202, test rmse: 0.762, time: 0.537\n",
            "epoch:19-train loss: 0.005,valid loss: 0.115,test loss: 0.057, valid rmse: 1.303, test rmse: 0.914, time: 0.522\n",
            "epoch:20-train loss: 0.003,valid loss: 0.101,test loss: 0.052, valid rmse: 1.219, test rmse: 0.873, time: 0.524\n",
            "epoch:21-train loss: 0.002,valid loss: 0.099,test loss: 0.051, valid rmse: 1.207, test rmse: 0.872, time: 0.513\n",
            "epoch:22-train loss: 0.002,valid loss: 0.102,test loss: 0.052, valid rmse: 1.225, test rmse: 0.876, time: 0.531\n",
            "epoch:23-train loss: 0.002,valid loss: 0.102,test loss: 0.048, valid rmse: 1.229, test rmse: 0.844, time: 0.541\n",
            "epoch:24-train loss: 0.002,valid loss: 0.102,test loss: 0.050, valid rmse: 1.228, test rmse: 0.856, time: 0.498\n",
            "epoch:25-train loss: 0.002,valid loss: 0.103,test loss: 0.050, valid rmse: 1.237, test rmse: 0.861, time: 0.516\n",
            "epoch:26-train loss: 0.001,valid loss: 0.101,test loss: 0.049, valid rmse: 1.223, test rmse: 0.848, time: 0.501\n",
            "epoch:27-train loss: 0.001,valid loss: 0.102,test loss: 0.050, valid rmse: 1.227, test rmse: 0.863, time: 0.505\n",
            "epoch:28-train loss: 0.001,valid loss: 0.101,test loss: 0.051, valid rmse: 1.222, test rmse: 0.869, time: 0.513\n",
            "epoch:29-train loss: 0.001,valid loss: 0.100,test loss: 0.051, valid rmse: 1.214, test rmse: 0.867, time: 0.513\n",
            "epoch:30-train loss: 0.001,valid loss: 0.099,test loss: 0.049, valid rmse: 1.212, test rmse: 0.854, time: 0.508\n",
            "epoch:31-train loss: 0.001,valid loss: 0.100,test loss: 0.050, valid rmse: 1.219, test rmse: 0.855, time: 0.490\n",
            "epoch:32-train loss: 0.001,valid loss: 0.101,test loss: 0.049, valid rmse: 1.220, test rmse: 0.852, time: 0.479\n",
            "epoch:33-train loss: 0.001,valid loss: 0.099,test loss: 0.051, valid rmse: 1.210, test rmse: 0.865, time: 0.475\n",
            "RMSE_k_valid [1.2060235414989682, 1.194299562008171, 1.210078970012746]\n",
            "RMSE_k_test [0.8279231492694524, 0.8482180047078691, 0.8651104761336323]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH-8irM3-Tbl"
      },
      "source": [
        "RMSE_mean_valid, RMSE_std_valid = metric(RMSE_k_valid)\n",
        "RMSE_mean_test, RMSE_std_test = metric(RMSE_k_test)\n",
        "\n",
        "print('result:, RMSE:%.3f, RMSE_std:%.3f' % (RMSE_mean_valid, RMSE_std_valid))\n",
        "print('result:, RMSE:%.3f, RMSE_std:%.3f' % (RMSE_mean_test, RMSE_std_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKc-5M8N-gIj"
      },
      "source": [
        "### Example: pyg_train\n",
        "\n",
        "Training a PyG model directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kupD4B-d-i26"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch_geometric.transforms import AddSelfLoops\n",
        "from utils import GAT, AGNN, SGC, ARMA"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tuGtQen-0-w"
      },
      "source": [
        "def rms_score(y_true, y_pred):\n",
        "    \"\"\"Computes RMS error.\"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def load_tensor(file_name, dtype):\n",
        "    return [dtype(d).to(device) for d in np.load(file_name + '.npy')]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKj3sqEkAiMR"
      },
      "source": [
        "class TestDataset(InMemoryDataset):\n",
        "    \n",
        "    def __init__(self, data_list):\n",
        "        super(TestDataset, self).__init__('/tmp/TestDataset')\n",
        "        self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "    def _download(self):\n",
        "        pass\n",
        "\n",
        "    def _process(self):\n",
        "        pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiYeAQj4Aop5"
      },
      "source": [
        "def load_dataset(dataset):\n",
        "\n",
        "    with open('./FreeSolv/decrease/' + dataset +'/full_feature','rb') as node_features:\n",
        "        x_train = pickle.load(node_features)\n",
        "    with open('./FreeSolv/decrease/' + dataset +'/edge','rb') as f:\n",
        "        edge_index_train = pickle.load(f)\n",
        "    y_train = load_tensor('./FreeSolv/decrease/' + dataset +'/Interactions', torch.FloatTensor)\n",
        "\n",
        "    d = []\n",
        "    for i in range(len(y_train)):\n",
        "        data = Data(x=x_train[i], edge_index=edge_index_train[i], y=y_train[i])\n",
        "        data = AddSelfLoops()(data)\n",
        "        data.atom_num = x_train[i].shape[0]\n",
        "        d.append(data)\n",
        "    set = TestDataset(d)\n",
        "    return set"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok29nStNAre3"
      },
      "source": [
        "# FreeSolv\n",
        "std = 3.8448222046029543\n",
        "mean = -3.8030062305295975"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q-_2D-JAuMW"
      },
      "source": [
        "class Trainer(object):\n",
        "    \n",
        "    def __init__(self, model, std, mean):\n",
        "        self.model = model\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def train(self, train_loader):\n",
        "\n",
        "        loss_total = 0\n",
        "        num = 0\n",
        "        for data in train_loader:\n",
        "            num += 1\n",
        "            data = data.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.model(data, std=self.std, mean=self.mean, train=True)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            loss_total += loss.to('cpu').data.numpy()\n",
        "\n",
        "        loss_mean = loss_total / num\n",
        "        return loss_mean"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH4FVYvVAyCQ"
      },
      "source": [
        "class tester(object):\n",
        "    \n",
        "    def __init__(self, model, std, mean):\n",
        "        self.model = model\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def test(self, test_loader):\n",
        "\n",
        "        loss_total = 0\n",
        "        all_p = []\n",
        "        all_t = []\n",
        "        num = 0\n",
        "        for data in test_loader:\n",
        "            num += 1\n",
        "            data = data.to(device)\n",
        "            loss, predicted, true = self.model(data, std=self.std, mean=self.mean, train=False)\n",
        "\n",
        "            for i in predicted:\n",
        "                all_p.append(float(i))\n",
        "            for i in true:\n",
        "                all_t.append(float(i))\n",
        "            loss_total += loss.to('cpu').data.numpy()\n",
        "\n",
        "        RMSE = rms_score(all_t,all_p)\n",
        "        loss_mean = loss_total / num\n",
        "        return loss_mean, RMSE"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx0HixYqA1Nu"
      },
      "source": [
        "def metric(RMSE_k_test):\n",
        "    RMSE_mean_test = np.mean(np.array(RMSE_k_test))\n",
        "    RMSE_std_test = np.std(np.array(RMSE_k_test))\n",
        "\n",
        "    return RMSE_mean_test, RMSE_std_test"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3lx5S0IA3py",
        "outputId": "3b941a21-aa08-431c-804c-9277720eee80"
      },
      "source": [
        "batch = 8\n",
        "iteration = 50\n",
        "lr = 0.01\n",
        "device = torch.device('cuda')\n",
        "decay_interval = 10\n",
        "lr_decay = 0.5\n",
        "\n",
        "print('decay_interval:', decay_interval)\n",
        "print('lr:', lr)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decay_interval: 10\n",
            "lr: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlbGBaXXA65i"
      },
      "source": [
        "train_dataset = load_dataset('train_data')\n",
        "valid_dataset = load_dataset('valid_data')\n",
        "test_dataset = load_dataset('test_data')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_RKccavA-dS"
      },
      "source": [
        "seed_list = [256, 512, 1024]\n",
        "\n",
        "RMSE_k_train = []\n",
        "R2_k_train = []\n",
        "\n",
        "RMSE_k_valid = []\n",
        "R2_k_valid = []\n",
        "\n",
        "RMSE_k_test = []\n",
        "R2_k_test = []"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQKw15lGBB3c",
        "outputId": "0c7ce62b-86db-4e3f-a387-adad10177e59"
      },
      "source": [
        "for seed in seed_list:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    model = ARMA().to(device)\n",
        "    trainer = Trainer(model.train(), std, mean)\n",
        "    Tester = tester(model.eval(), std, mean)\n",
        "\n",
        "    for epoch in range(1, (iteration + 1)):\n",
        "        if epoch  % decay_interval == 0:\n",
        "            trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
        "\n",
        "        start = timeit.default_timer()\n",
        "        train_loss = trainer.train(train_loader)\n",
        "        valid_loss, RMSE_valid = Tester.test(valid_loader)\n",
        "        test_loss, RMSE_test = Tester.test(test_loader)\n",
        "        end = timeit.default_timer()\n",
        "        time = end - start\n",
        "\n",
        "        print(\n",
        "            'ARMA-epoch:%d,---train loss: %.3f,valid loss: %.3f,test loss: %.3f, valid rmse: %.3f, test rmse: %.3f, time: %.3f' %\n",
        "            (epoch, train_loss, valid_loss, test_loss, RMSE_valid, RMSE_test, time))\n",
        "\n",
        "        if epoch == iteration:\n",
        "            RMSE_k_valid.append(RMSE_valid)\n",
        "            RMSE_k_test.append(RMSE_test)\n",
        "\n",
        "    print('RMSE_k_valid', RMSE_k_valid)\n",
        "    print('RMSE_k_test', RMSE_k_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARMA-epoch:1,---train loss: 3.035,valid loss: 0.748,test loss: 0.665, valid rmse: 3.325, test rmse: 3.112, time: 0.638\n",
            "ARMA-epoch:2,---train loss: 0.674,valid loss: 0.581,test loss: 0.387, valid rmse: 2.931, test rmse: 2.516, time: 0.501\n",
            "ARMA-epoch:3,---train loss: 0.424,valid loss: 0.389,test loss: 0.312, valid rmse: 2.398, test rmse: 1.936, time: 0.514\n",
            "ARMA-epoch:4,---train loss: 0.363,valid loss: 0.399,test loss: 0.216, valid rmse: 2.428, test rmse: 1.881, time: 0.507\n",
            "ARMA-epoch:5,---train loss: 0.267,valid loss: 0.239,test loss: 0.109, valid rmse: 1.878, test rmse: 1.302, time: 0.507\n",
            "ARMA-epoch:6,---train loss: 0.198,valid loss: 0.290,test loss: 0.216, valid rmse: 2.069, test rmse: 1.608, time: 0.500\n",
            "ARMA-epoch:7,---train loss: 0.228,valid loss: 0.260,test loss: 0.127, valid rmse: 1.960, test rmse: 1.368, time: 0.503\n",
            "ARMA-epoch:8,---train loss: 0.242,valid loss: 0.427,test loss: 0.319, valid rmse: 2.512, test rmse: 2.164, time: 0.516\n",
            "ARMA-epoch:9,---train loss: 0.193,valid loss: 0.222,test loss: 0.147, valid rmse: 1.813, test rmse: 1.477, time: 0.508\n",
            "ARMA-epoch:10,---train loss: 0.157,valid loss: 0.232,test loss: 0.285, valid rmse: 1.853, test rmse: 1.401, time: 0.505\n",
            "ARMA-epoch:11,---train loss: 0.124,valid loss: 0.178,test loss: 0.081, valid rmse: 1.620, test rmse: 1.093, time: 0.497\n",
            "ARMA-epoch:12,---train loss: 0.113,valid loss: 0.221,test loss: 0.150, valid rmse: 1.809, test rmse: 1.551, time: 0.511\n",
            "ARMA-epoch:13,---train loss: 0.119,valid loss: 0.171,test loss: 0.083, valid rmse: 1.591, test rmse: 1.162, time: 0.506\n",
            "ARMA-epoch:14,---train loss: 0.108,valid loss: 0.185,test loss: 0.126, valid rmse: 1.654, test rmse: 1.372, time: 0.503\n",
            "ARMA-epoch:15,---train loss: 0.135,valid loss: 0.178,test loss: 0.096, valid rmse: 1.621, test rmse: 1.251, time: 0.513\n",
            "ARMA-epoch:16,---train loss: 0.117,valid loss: 0.168,test loss: 0.093, valid rmse: 1.576, test rmse: 1.222, time: 0.503\n",
            "ARMA-epoch:17,---train loss: 0.125,valid loss: 0.157,test loss: 0.088, valid rmse: 1.522, test rmse: 1.087, time: 0.507\n",
            "ARMA-epoch:18,---train loss: 0.106,valid loss: 0.168,test loss: 0.083, valid rmse: 1.576, test rmse: 1.166, time: 0.499\n",
            "ARMA-epoch:19,---train loss: 0.137,valid loss: 0.165,test loss: 0.085, valid rmse: 1.564, test rmse: 1.117, time: 0.511\n",
            "ARMA-epoch:20,---train loss: 0.089,valid loss: 0.142,test loss: 0.080, valid rmse: 1.447, test rmse: 1.140, time: 0.494\n",
            "ARMA-epoch:21,---train loss: 0.085,valid loss: 0.144,test loss: 0.068, valid rmse: 1.457, test rmse: 1.050, time: 0.508\n",
            "ARMA-epoch:22,---train loss: 0.080,valid loss: 0.150,test loss: 0.123, valid rmse: 1.487, test rmse: 1.165, time: 0.495\n",
            "ARMA-epoch:23,---train loss: 0.079,valid loss: 0.149,test loss: 0.082, valid rmse: 1.483, test rmse: 1.079, time: 0.517\n",
            "ARMA-epoch:24,---train loss: 0.095,valid loss: 0.167,test loss: 0.104, valid rmse: 1.573, test rmse: 1.301, time: 0.522\n",
            "ARMA-epoch:25,---train loss: 0.076,valid loss: 0.141,test loss: 0.091, valid rmse: 1.446, test rmse: 1.072, time: 0.509\n",
            "ARMA-epoch:26,---train loss: 0.078,valid loss: 0.151,test loss: 0.080, valid rmse: 1.492, test rmse: 1.104, time: 0.503\n",
            "ARMA-epoch:27,---train loss: 0.079,valid loss: 0.158,test loss: 0.096, valid rmse: 1.530, test rmse: 1.112, time: 0.501\n",
            "ARMA-epoch:28,---train loss: 0.084,valid loss: 0.138,test loss: 0.066, valid rmse: 1.428, test rmse: 1.032, time: 0.477\n",
            "ARMA-epoch:29,---train loss: 0.083,valid loss: 0.148,test loss: 0.071, valid rmse: 1.480, test rmse: 1.060, time: 0.498\n",
            "ARMA-epoch:30,---train loss: 0.077,valid loss: 0.146,test loss: 0.100, valid rmse: 1.468, test rmse: 1.218, time: 0.512\n",
            "ARMA-epoch:31,---train loss: 0.074,valid loss: 0.136,test loss: 0.066, valid rmse: 1.419, test rmse: 1.036, time: 0.509\n",
            "ARMA-epoch:32,---train loss: 0.068,valid loss: 0.136,test loss: 0.112, valid rmse: 1.416, test rmse: 1.091, time: 0.497\n",
            "ARMA-epoch:33,---train loss: 0.070,valid loss: 0.133,test loss: 0.077, valid rmse: 1.403, test rmse: 1.079, time: 0.511\n",
            "ARMA-epoch:34,---train loss: 0.069,valid loss: 0.130,test loss: 0.067, valid rmse: 1.387, test rmse: 1.040, time: 0.508\n",
            "ARMA-epoch:35,---train loss: 0.064,valid loss: 0.131,test loss: 0.087, valid rmse: 1.392, test rmse: 1.064, time: 0.510\n",
            "ARMA-epoch:36,---train loss: 0.065,valid loss: 0.139,test loss: 0.089, valid rmse: 1.434, test rmse: 1.146, time: 0.513\n",
            "ARMA-epoch:37,---train loss: 0.072,valid loss: 0.137,test loss: 0.082, valid rmse: 1.424, test rmse: 1.140, time: 0.504\n",
            "ARMA-epoch:38,---train loss: 0.071,valid loss: 0.138,test loss: 0.078, valid rmse: 1.427, test rmse: 1.077, time: 0.498\n",
            "ARMA-epoch:39,---train loss: 0.065,valid loss: 0.150,test loss: 0.099, valid rmse: 1.489, test rmse: 1.168, time: 0.513\n",
            "ARMA-epoch:40,---train loss: 0.063,valid loss: 0.136,test loss: 0.095, valid rmse: 1.417, test rmse: 1.070, time: 0.489\n",
            "ARMA-epoch:41,---train loss: 0.062,valid loss: 0.140,test loss: 0.082, valid rmse: 1.440, test rmse: 1.152, time: 0.492\n",
            "ARMA-epoch:42,---train loss: 0.067,valid loss: 0.135,test loss: 0.079, valid rmse: 1.414, test rmse: 1.025, time: 0.489\n",
            "ARMA-epoch:43,---train loss: 0.062,valid loss: 0.134,test loss: 0.068, valid rmse: 1.408, test rmse: 1.042, time: 0.492\n",
            "ARMA-epoch:44,---train loss: 0.063,valid loss: 0.135,test loss: 0.095, valid rmse: 1.415, test rmse: 1.066, time: 0.500\n",
            "ARMA-epoch:45,---train loss: 0.062,valid loss: 0.128,test loss: 0.133, valid rmse: 1.377, test rmse: 1.006, time: 0.484\n",
            "ARMA-epoch:46,---train loss: 0.060,valid loss: 0.132,test loss: 0.065, valid rmse: 1.394, test rmse: 1.026, time: 0.497\n",
            "ARMA-epoch:47,---train loss: 0.061,valid loss: 0.131,test loss: 0.070, valid rmse: 1.392, test rmse: 1.065, time: 0.512\n",
            "ARMA-epoch:48,---train loss: 0.059,valid loss: 0.138,test loss: 0.084, valid rmse: 1.427, test rmse: 1.116, time: 0.496\n",
            "ARMA-epoch:49,---train loss: 0.061,valid loss: 0.140,test loss: 0.086, valid rmse: 1.439, test rmse: 1.151, time: 0.515\n",
            "ARMA-epoch:50,---train loss: 0.056,valid loss: 0.131,test loss: 0.081, valid rmse: 1.390, test rmse: 1.044, time: 0.492\n",
            "RMSE_k_valid [1.3901561923980623]\n",
            "RMSE_k_test [1.0444605201320527]\n",
            "ARMA-epoch:1,---train loss: 1.463,valid loss: 0.648,test loss: 0.648, valid rmse: 3.094, test rmse: 2.901, time: 0.494\n",
            "ARMA-epoch:2,---train loss: 0.431,valid loss: 0.379,test loss: 0.232, valid rmse: 2.366, test rmse: 1.934, time: 0.498\n",
            "ARMA-epoch:3,---train loss: 0.338,valid loss: 0.351,test loss: 0.220, valid rmse: 2.278, test rmse: 1.628, time: 0.503\n",
            "ARMA-epoch:4,---train loss: 0.336,valid loss: 0.342,test loss: 0.168, valid rmse: 2.248, test rmse: 1.544, time: 0.487\n",
            "ARMA-epoch:5,---train loss: 0.286,valid loss: 0.285,test loss: 0.133, valid rmse: 2.052, test rmse: 1.472, time: 0.502\n",
            "ARMA-epoch:6,---train loss: 0.202,valid loss: 0.249,test loss: 0.142, valid rmse: 1.920, test rmse: 1.350, time: 0.510\n",
            "ARMA-epoch:7,---train loss: 0.192,valid loss: 0.260,test loss: 0.143, valid rmse: 1.961, test rmse: 1.473, time: 0.514\n",
            "ARMA-epoch:8,---train loss: 0.185,valid loss: 0.342,test loss: 0.244, valid rmse: 2.249, test rmse: 1.952, time: 0.515\n",
            "ARMA-epoch:9,---train loss: 0.220,valid loss: 0.282,test loss: 0.220, valid rmse: 2.042, test rmse: 1.893, time: 0.511\n",
            "ARMA-epoch:10,---train loss: 0.150,valid loss: 0.167,test loss: 0.080, valid rmse: 1.569, test rmse: 1.146, time: 0.519\n",
            "ARMA-epoch:11,---train loss: 0.118,valid loss: 0.209,test loss: 0.140, valid rmse: 1.759, test rmse: 1.491, time: 0.526\n",
            "ARMA-epoch:12,---train loss: 0.110,valid loss: 0.166,test loss: 0.076, valid rmse: 1.565, test rmse: 1.099, time: 0.507\n",
            "ARMA-epoch:13,---train loss: 0.116,valid loss: 0.177,test loss: 0.099, valid rmse: 1.619, test rmse: 1.270, time: 0.501\n",
            "ARMA-epoch:14,---train loss: 0.102,valid loss: 0.168,test loss: 0.099, valid rmse: 1.577, test rmse: 1.190, time: 0.507\n",
            "ARMA-epoch:15,---train loss: 0.101,valid loss: 0.209,test loss: 0.152, valid rmse: 1.759, test rmse: 1.529, time: 0.526\n",
            "ARMA-epoch:16,---train loss: 0.130,valid loss: 0.225,test loss: 0.122, valid rmse: 1.824, test rmse: 1.278, time: 0.513\n",
            "ARMA-epoch:17,---train loss: 0.109,valid loss: 0.162,test loss: 0.068, valid rmse: 1.546, test rmse: 1.045, time: 0.518\n",
            "ARMA-epoch:18,---train loss: 0.121,valid loss: 0.169,test loss: 0.069, valid rmse: 1.580, test rmse: 1.059, time: 0.519\n",
            "ARMA-epoch:19,---train loss: 0.107,valid loss: 0.155,test loss: 0.076, valid rmse: 1.515, test rmse: 1.034, time: 0.537\n",
            "ARMA-epoch:20,---train loss: 0.114,valid loss: 0.152,test loss: 0.104, valid rmse: 1.500, test rmse: 1.134, time: 0.506\n",
            "ARMA-epoch:21,---train loss: 0.084,valid loss: 0.141,test loss: 0.063, valid rmse: 1.445, test rmse: 1.009, time: 0.521\n",
            "ARMA-epoch:22,---train loss: 0.085,valid loss: 0.144,test loss: 0.079, valid rmse: 1.459, test rmse: 1.019, time: 0.517\n",
            "ARMA-epoch:23,---train loss: 0.082,valid loss: 0.145,test loss: 0.062, valid rmse: 1.462, test rmse: 1.005, time: 0.544\n",
            "ARMA-epoch:24,---train loss: 0.076,valid loss: 0.141,test loss: 0.067, valid rmse: 1.446, test rmse: 0.965, time: 0.540\n",
            "ARMA-epoch:25,---train loss: 0.081,valid loss: 0.148,test loss: 0.067, valid rmse: 1.477, test rmse: 1.040, time: 0.519\n",
            "ARMA-epoch:26,---train loss: 0.082,valid loss: 0.144,test loss: 0.064, valid rmse: 1.460, test rmse: 1.021, time: 0.526\n",
            "ARMA-epoch:27,---train loss: 0.079,valid loss: 0.162,test loss: 0.092, valid rmse: 1.548, test rmse: 1.222, time: 0.536\n",
            "ARMA-epoch:28,---train loss: 0.080,valid loss: 0.150,test loss: 0.067, valid rmse: 1.487, test rmse: 1.036, time: 0.511\n",
            "ARMA-epoch:29,---train loss: 0.084,valid loss: 0.133,test loss: 0.063, valid rmse: 1.400, test rmse: 0.996, time: 0.512\n",
            "ARMA-epoch:30,---train loss: 0.070,valid loss: 0.142,test loss: 0.103, valid rmse: 1.450, test rmse: 1.161, time: 0.508\n",
            "ARMA-epoch:31,---train loss: 0.073,valid loss: 0.133,test loss: 0.060, valid rmse: 1.405, test rmse: 0.991, time: 0.516\n",
            "ARMA-epoch:32,---train loss: 0.070,valid loss: 0.131,test loss: 0.056, valid rmse: 1.394, test rmse: 0.951, time: 0.498\n",
            "ARMA-epoch:33,---train loss: 0.080,valid loss: 0.146,test loss: 0.066, valid rmse: 1.471, test rmse: 1.001, time: 0.506\n",
            "ARMA-epoch:34,---train loss: 0.069,valid loss: 0.136,test loss: 0.071, valid rmse: 1.420, test rmse: 1.078, time: 0.496\n",
            "ARMA-epoch:35,---train loss: 0.066,valid loss: 0.134,test loss: 0.065, valid rmse: 1.410, test rmse: 1.009, time: 0.495\n",
            "ARMA-epoch:36,---train loss: 0.064,valid loss: 0.139,test loss: 0.059, valid rmse: 1.431, test rmse: 0.962, time: 0.505\n",
            "ARMA-epoch:37,---train loss: 0.066,valid loss: 0.133,test loss: 0.061, valid rmse: 1.402, test rmse: 0.984, time: 0.510\n",
            "ARMA-epoch:38,---train loss: 0.063,valid loss: 0.134,test loss: 0.075, valid rmse: 1.410, test rmse: 1.050, time: 0.503\n",
            "ARMA-epoch:39,---train loss: 0.066,valid loss: 0.144,test loss: 0.070, valid rmse: 1.457, test rmse: 1.050, time: 0.532\n",
            "ARMA-epoch:40,---train loss: 0.062,valid loss: 0.131,test loss: 0.136, valid rmse: 1.390, test rmse: 0.936, time: 0.537\n",
            "ARMA-epoch:41,---train loss: 0.060,valid loss: 0.130,test loss: 0.066, valid rmse: 1.387, test rmse: 0.959, time: 0.509\n",
            "ARMA-epoch:42,---train loss: 0.058,valid loss: 0.127,test loss: 0.056, valid rmse: 1.370, test rmse: 0.946, time: 0.509\n",
            "ARMA-epoch:43,---train loss: 0.059,valid loss: 0.128,test loss: 0.058, valid rmse: 1.377, test rmse: 0.975, time: 0.514\n",
            "ARMA-epoch:44,---train loss: 0.061,valid loss: 0.128,test loss: 0.065, valid rmse: 1.375, test rmse: 1.008, time: 0.492\n",
            "ARMA-epoch:45,---train loss: 0.058,valid loss: 0.130,test loss: 0.066, valid rmse: 1.385, test rmse: 1.041, time: 0.503\n",
            "ARMA-epoch:46,---train loss: 0.057,valid loss: 0.127,test loss: 0.079, valid rmse: 1.371, test rmse: 1.027, time: 0.504\n",
            "ARMA-epoch:47,---train loss: 0.057,valid loss: 0.125,test loss: 0.061, valid rmse: 1.360, test rmse: 0.953, time: 0.506\n",
            "ARMA-epoch:48,---train loss: 0.058,valid loss: 0.124,test loss: 0.055, valid rmse: 1.355, test rmse: 0.935, time: 0.510\n",
            "ARMA-epoch:49,---train loss: 0.056,valid loss: 0.124,test loss: 0.067, valid rmse: 1.355, test rmse: 0.926, time: 0.499\n",
            "ARMA-epoch:50,---train loss: 0.055,valid loss: 0.123,test loss: 0.066, valid rmse: 1.350, test rmse: 0.932, time: 0.508\n",
            "RMSE_k_valid [1.3901561923980623, 1.3495005425107156]\n",
            "RMSE_k_test [1.0444605201320527, 0.9321377503051926]\n",
            "ARMA-epoch:1,---train loss: 2.221,valid loss: 0.710,test loss: 0.669, valid rmse: 3.241, test rmse: 3.238, time: 0.507\n",
            "ARMA-epoch:2,---train loss: 0.565,valid loss: 0.562,test loss: 0.384, valid rmse: 2.883, test rmse: 2.453, time: 0.507\n",
            "ARMA-epoch:3,---train loss: 0.415,valid loss: 0.359,test loss: 0.255, valid rmse: 2.305, test rmse: 1.958, time: 0.500\n",
            "ARMA-epoch:4,---train loss: 0.329,valid loss: 0.371,test loss: 0.220, valid rmse: 2.341, test rmse: 1.859, time: 0.501\n",
            "ARMA-epoch:5,---train loss: 0.310,valid loss: 0.368,test loss: 0.281, valid rmse: 2.332, test rmse: 2.127, time: 0.509\n",
            "ARMA-epoch:6,---train loss: 0.707,valid loss: 1.052,test loss: 1.054, valid rmse: 3.944, test rmse: 4.045, time: 0.509\n",
            "ARMA-epoch:7,---train loss: 0.303,valid loss: 0.291,test loss: 0.139, valid rmse: 2.074, test rmse: 1.504, time: 0.502\n",
            "ARMA-epoch:8,---train loss: 0.212,valid loss: 0.988,test loss: 1.028, valid rmse: 3.822, test rmse: 3.788, time: 0.510\n",
            "ARMA-epoch:9,---train loss: 0.232,valid loss: 0.285,test loss: 0.111, valid rmse: 2.051, test rmse: 1.339, time: 0.495\n",
            "ARMA-epoch:10,---train loss: 0.165,valid loss: 0.287,test loss: 0.139, valid rmse: 2.058, test rmse: 1.504, time: 0.497\n",
            "ARMA-epoch:11,---train loss: 0.171,valid loss: 0.263,test loss: 0.178, valid rmse: 1.973, test rmse: 1.538, time: 0.487\n",
            "ARMA-epoch:12,---train loss: 0.167,valid loss: 0.270,test loss: 0.212, valid rmse: 1.996, test rmse: 1.630, time: 0.500\n",
            "ARMA-epoch:13,---train loss: 0.152,valid loss: 0.230,test loss: 0.093, valid rmse: 1.843, test rmse: 1.231, time: 0.508\n",
            "ARMA-epoch:14,---train loss: 0.121,valid loss: 0.251,test loss: 0.098, valid rmse: 1.928, test rmse: 1.236, time: 0.496\n",
            "ARMA-epoch:15,---train loss: 0.147,valid loss: 0.252,test loss: 0.137, valid rmse: 1.930, test rmse: 1.218, time: 0.490\n",
            "ARMA-epoch:16,---train loss: 0.118,valid loss: 0.244,test loss: 0.162, valid rmse: 1.898, test rmse: 1.509, time: 0.509\n",
            "ARMA-epoch:17,---train loss: 0.134,valid loss: 0.239,test loss: 0.126, valid rmse: 1.878, test rmse: 1.433, time: 0.498\n",
            "ARMA-epoch:18,---train loss: 0.145,valid loss: 0.196,test loss: 0.068, valid rmse: 1.702, test rmse: 1.043, time: 0.510\n",
            "ARMA-epoch:19,---train loss: 0.134,valid loss: 0.218,test loss: 0.104, valid rmse: 1.795, test rmse: 1.297, time: 0.506\n",
            "ARMA-epoch:20,---train loss: 0.103,valid loss: 0.213,test loss: 0.078, valid rmse: 1.774, test rmse: 1.071, time: 0.514\n",
            "ARMA-epoch:21,---train loss: 0.103,valid loss: 0.218,test loss: 0.118, valid rmse: 1.796, test rmse: 1.241, time: 0.502\n",
            "ARMA-epoch:22,---train loss: 0.090,valid loss: 0.194,test loss: 0.082, valid rmse: 1.692, test rmse: 1.125, time: 0.513\n",
            "ARMA-epoch:23,---train loss: 0.091,valid loss: 0.211,test loss: 0.162, valid rmse: 1.768, test rmse: 1.274, time: 0.512\n",
            "ARMA-epoch:24,---train loss: 0.107,valid loss: 0.204,test loss: 0.093, valid rmse: 1.737, test rmse: 1.223, time: 0.512\n",
            "ARMA-epoch:25,---train loss: 0.101,valid loss: 0.192,test loss: 0.076, valid rmse: 1.684, test rmse: 1.099, time: 0.504\n",
            "ARMA-epoch:26,---train loss: 0.085,valid loss: 0.203,test loss: 0.215, valid rmse: 1.731, test rmse: 1.176, time: 0.505\n",
            "ARMA-epoch:27,---train loss: 0.087,valid loss: 0.198,test loss: 0.088, valid rmse: 1.710, test rmse: 1.126, time: 0.517\n",
            "ARMA-epoch:28,---train loss: 0.090,valid loss: 0.204,test loss: 0.094, valid rmse: 1.736, test rmse: 1.206, time: 0.507\n",
            "ARMA-epoch:29,---train loss: 0.088,valid loss: 0.200,test loss: 0.074, valid rmse: 1.718, test rmse: 1.080, time: 0.521\n",
            "ARMA-epoch:30,---train loss: 0.078,valid loss: 0.194,test loss: 0.080, valid rmse: 1.692, test rmse: 1.099, time: 0.512\n",
            "ARMA-epoch:31,---train loss: 0.075,valid loss: 0.190,test loss: 0.082, valid rmse: 1.676, test rmse: 1.097, time: 0.496\n",
            "ARMA-epoch:32,---train loss: 0.075,valid loss: 0.186,test loss: 0.112, valid rmse: 1.659, test rmse: 1.090, time: 0.503\n",
            "ARMA-epoch:33,---train loss: 0.077,valid loss: 0.191,test loss: 0.081, valid rmse: 1.680, test rmse: 1.125, time: 0.521\n",
            "ARMA-epoch:34,---train loss: 0.074,valid loss: 0.201,test loss: 0.080, valid rmse: 1.723, test rmse: 1.107, time: 0.510\n",
            "ARMA-epoch:35,---train loss: 0.073,valid loss: 0.180,test loss: 0.072, valid rmse: 1.630, test rmse: 1.003, time: 0.500\n",
            "ARMA-epoch:36,---train loss: 0.074,valid loss: 0.191,test loss: 0.070, valid rmse: 1.680, test rmse: 1.068, time: 0.511\n",
            "ARMA-epoch:37,---train loss: 0.080,valid loss: 0.205,test loss: 0.089, valid rmse: 1.742, test rmse: 1.116, time: 0.504\n",
            "ARMA-epoch:38,---train loss: 0.076,valid loss: 0.190,test loss: 0.074, valid rmse: 1.675, test rmse: 1.083, time: 0.504\n",
            "ARMA-epoch:39,---train loss: 0.069,valid loss: 0.183,test loss: 0.069, valid rmse: 1.645, test rmse: 1.056, time: 0.501\n",
            "ARMA-epoch:40,---train loss: 0.068,valid loss: 0.198,test loss: 0.084, valid rmse: 1.710, test rmse: 1.167, time: 0.501\n",
            "ARMA-epoch:41,---train loss: 0.067,valid loss: 0.184,test loss: 0.061, valid rmse: 1.651, test rmse: 0.994, time: 0.507\n",
            "ARMA-epoch:42,---train loss: 0.066,valid loss: 0.181,test loss: 0.064, valid rmse: 1.636, test rmse: 1.024, time: 0.508\n",
            "ARMA-epoch:43,---train loss: 0.067,valid loss: 0.186,test loss: 0.071, valid rmse: 1.660, test rmse: 1.076, time: 0.503\n",
            "ARMA-epoch:44,---train loss: 0.067,valid loss: 0.192,test loss: 0.079, valid rmse: 1.683, test rmse: 1.058, time: 0.511\n",
            "ARMA-epoch:45,---train loss: 0.065,valid loss: 0.188,test loss: 0.075, valid rmse: 1.667, test rmse: 1.063, time: 0.509\n",
            "ARMA-epoch:46,---train loss: 0.066,valid loss: 0.185,test loss: 0.060, valid rmse: 1.654, test rmse: 0.985, time: 0.491\n",
            "ARMA-epoch:47,---train loss: 0.065,valid loss: 0.190,test loss: 0.086, valid rmse: 1.675, test rmse: 1.094, time: 0.507\n",
            "ARMA-epoch:48,---train loss: 0.064,valid loss: 0.187,test loss: 0.070, valid rmse: 1.664, test rmse: 1.070, time: 0.508\n",
            "ARMA-epoch:49,---train loss: 0.064,valid loss: 0.185,test loss: 0.062, valid rmse: 1.656, test rmse: 0.990, time: 0.494\n",
            "ARMA-epoch:50,---train loss: 0.062,valid loss: 0.184,test loss: 0.076, valid rmse: 1.647, test rmse: 1.062, time: 0.504\n",
            "RMSE_k_valid [1.3901561923980623, 1.3495005425107156, 1.6474101308796016]\n",
            "RMSE_k_test [1.0444605201320527, 0.9321377503051926, 1.0623148702312388]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7gKrUnZBExD",
        "outputId": "03bb2f79-2d10-4910-d866-48244cfa01f7"
      },
      "source": [
        "RMSE_mean_valid, RMSE_std_valid = metric(RMSE_k_valid)\n",
        "RMSE_mean_test, RMSE_std_test = metric(RMSE_k_test)\n",
        "\n",
        "print('result:, valid_RMSE:%.3f, valid_RMSE_std:%.3f' % (RMSE_mean_valid, RMSE_std_valid))\n",
        "print('result:, test_RMSE:%.3f, test_RMSE_std:%.3f' % (RMSE_mean_test, RMSE_std_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result:, valid_RMSE:1.462, valid_RMSE_std:0.132\n",
            "result:, test_RMSE:1.013, test_RMSE_std:0.058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_gBDx49DXZU"
      },
      "source": [
        "### Example: load_FreeSolv\n",
        "\n",
        "Load data from DeepChem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq4jI_aRDc9_"
      },
      "source": [
        "import numpy as np\n",
        "import deepchem as dc\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from utils import preprocess_adj\n",
        "from rdkit import Chem\n",
        "\n",
        "import pickle\n",
        "\n",
        "from numpy import flip"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJyJiMgFDpYO"
      },
      "source": [
        "def create_adjacency(mol):\n",
        "    adjacency = Chem.GetAdjacencyMatrix(mol)\n",
        "    return np.array(adjacency,dtype=float)\n",
        "\n",
        "def save_feature(dir, Features, Normed_adj, Interactions, smiles, edge, full_feature, dataset=None):\n",
        "    dir_input = (dir + dataset + '/')\n",
        "    os.makedirs(dir_input, exist_ok=True)\n",
        "    np.save(dir_input + 'Features', Features)\n",
        "    np.save(dir_input + 'Normed_adj', Normed_adj)\n",
        "    np.save(dir_input + 'Interactions', Interactions)\n",
        "    np.save(dir_input + 'smiles', smiles)\n",
        "\n",
        "    with open(dir_input + 'edge', 'wb') as f:\n",
        "        pickle.dump(edge, f)\n",
        "\n",
        "    with open(dir_input + 'full_feature', 'wb') as a:\n",
        "        pickle.dump(full_feature, a)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_0M5JwyDsiK",
        "outputId": "72b11d52-da09-496a-c30d-138db964b2a5"
      },
      "source": [
        "# Only for debug!\n",
        "np.random.seed(123)\n",
        "\n",
        "# Load Delaney dataset\n",
        "delaney_tasks, delaney_datasets, transformers = dc.molnet.load_sampl(featurizer='GraphConv', split='random')\n",
        "train_dataset, valid_dataset, test_dataset = delaney_datasets\n",
        "\n",
        "maxNumAtoms = 16"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'split' is deprecated.  Use 'splitter' instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU1RO3IWDvtc"
      },
      "source": [
        "def fix_input(feature_array, iAdjTmp):\n",
        "    \"Fix number of input molecular atoms\"\n",
        "\n",
        "    iFeature = np.zeros((maxNumAtoms, 75))\n",
        "    if len(feature_array) <= maxNumAtoms:\n",
        "        iFeature[0:len(feature_array), 0:75] = feature_array\n",
        "    else:\n",
        "        iFeature = feature_array[0:maxNumAtoms]\n",
        "\n",
        "    adjacency = np.zeros((maxNumAtoms, maxNumAtoms))\n",
        "\n",
        "    if len(feature_array) <= maxNumAtoms:\n",
        "        adjacency[0:len(feature_array), 0:len(feature_array)] = iAdjTmp\n",
        "    else:\n",
        "        adjacency = iAdjTmp[0:maxNumAtoms, 0:maxNumAtoms]\n",
        "\n",
        "    return iFeature, adjacency"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWwes1GjDyMk"
      },
      "source": [
        "def get_feature(dataset):\n",
        "\n",
        "    Features_decrease, adj_decrease, edge_decrease, full_feature_decrease = [], [], [], []\n",
        "    Interactions, smiles = [], []\n",
        "\n",
        "    for x, label, w, smile in dataset.itersamples():\n",
        "\n",
        "        # The smile is used to extract molecular fingerprints\n",
        "        smiles.append(smile)\n",
        "\n",
        "        interaction = label\n",
        "        Interactions.append(interaction)\n",
        "\n",
        "        mol = Chem.MolFromSmiles(smile)\n",
        "\n",
        "        if not mol:\n",
        "            raise ValueError(\"Could not parse SMILES string:\", smile)\n",
        "\n",
        "        # increased order\n",
        "        feature_increase = x.get_atom_features()\n",
        "        iAdjTmp_increase = create_adjacency(mol)\n",
        "\n",
        "        # decreased order\n",
        "        # Turn the data upside down\n",
        "        feature_decrease = flip(feature_increase, 0)\n",
        "        iAdjTmp_decrease = flip(iAdjTmp_increase, 0)\n",
        "\n",
        "        # Obtaining fixed-size molecular input data\n",
        "        iFeature_decrease, adjacency_decrease = fix_input(feature_decrease, iAdjTmp_decrease)\n",
        "\n",
        "        Features_decrease.append(np.array(iFeature_decrease))\n",
        "        normed_adj_decrease = preprocess_adj(adjacency_decrease)\n",
        "        adj_decrease.append(normed_adj_decrease)\n",
        "\n",
        "        #Transforms data into PyTorch Geometrics specific data format.\n",
        "        index = np.array(np.where(iAdjTmp_decrease == 1))\n",
        "        edge_index = torch.from_numpy(index).long()\n",
        "        edge_decrease.append(edge_index)\n",
        "\n",
        "        feature = torch.from_numpy(feature_decrease.copy()).float()\n",
        "        full_feature_decrease.append(feature)\n",
        "\n",
        "    return  Features_decrease, adj_decrease, edge_decrease, full_feature_decrease, Interactions, smiles"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjVe5czMD2T2"
      },
      "source": [
        "Features_decrease1, adj_decrease1, edge_decrease1, full_feature_decrease1, Interactions1, smiles1 = get_feature(train_dataset)\n",
        "Features_decrease2, adj_decrease2, edge_decrease2, full_feature_decrease2, Interactions2, smiles2 = get_feature(valid_dataset)\n",
        "Features_decrease3, adj_decrease3, edge_decrease3, full_feature_decrease3, Interactions3, smiles3 = get_feature(test_dataset)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cD6jNjfEGeT"
      },
      "source": [
        "dir = './FreeSolv/decrease/'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9yC4uEzD7MC"
      },
      "source": [
        "save_feature(dir, Features_decrease1, adj_decrease1, Interactions1, smiles1, edge_decrease1, full_feature_decrease1, dataset='train_data')\n",
        "save_feature(dir, Features_decrease2, adj_decrease1, Interactions2, smiles2, edge_decrease2, full_feature_decrease2, dataset='valid_data')\n",
        "save_feature(dir, Features_decrease3, adj_decrease1, Interactions3, smiles3, edge_decrease3, full_feature_decrease3, dataset='test_data')"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}