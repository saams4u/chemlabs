{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gpd3ty8vr2o"
   },
   "source": [
    "## C-SGEN: Molecule Property Prediction Based on Spatial Graph Embedding\n",
    "\n",
    "ABSTRACT: Accurate prediction of molecular properties is\n",
    "important for new compound design, which is a crucial step in\n",
    "drug discovery. In this paper, molecular graph data is utilized\n",
    "for property prediction based on graph convolution neural\n",
    "networks. In addition, a convolution spatial graph embedding\n",
    "layer (C-SGEL) is introduced to retain the spatial connection\n",
    "information on molecules. And, multiple C-SGELs are stacked\n",
    "to construct a convolution spatial graph embedding network\n",
    "(C-SGEN) for end-to-end representation learning. In order to\n",
    "enhance the robustness of the network, molecular fingerprints\n",
    "are also combined with C-SGEN to build a composite model for predicting molecular properties. Our comparative experiments\n",
    "have shown that our method is accurate and achieves the best results on some open benchmark datasets.\n",
    "\n",
    "Link to paper: https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.9b00410?rand=oin4mnup\n",
    "\n",
    "Credit: https://github.com/wxfsd/C-SGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMaze21avdHK",
    "outputId": "f62ad795-bd4f-497a-982a-ca308a874fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'C-SGEN'...\n",
      "remote: Enumerating objects: 32, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 32 (delta 10), reused 32 (delta 10), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (32/32), done.\n",
      "/Users/saams4u/chemlabs/playground/C-SGEN\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository and cd into directory\n",
    "!git clone https://github.com/wxfsd/C-SGEN.git\n",
    "%cd C-SGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEWmXSvSvsSB"
   },
   "outputs": [],
   "source": [
    "# Install dependencies / requirements\n",
    "!pip install theano==1.0.3 numpy==1.16.4 scipy==1.3.0\n",
    "!pip install sklearn==0.0 deepchem torch==1.4.0 torchvision==0.5.0 torchtext==0.5.0\n",
    "\n",
    "!pip install torch-geometric \\\n",
    "  torch-sparse==latest+cu101 \\\n",
    "  torch-scatter==latest+cu101 \\\n",
    "  torch-cluster==latest+cu101 \\\n",
    "  torch-spline-conv==latest+cu101 \\\n",
    "  -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "\n",
    "# Install RDKit \n",
    "!pip install rdkit-pypi==2021.3.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_gTduj9w4ih"
   },
   "source": [
    "### Datasets\n",
    "`Feature.npy`, `Normed_adj.npy`, `fingerprint_stand.npy` and `Interactions.npy` are molecular features, adjacency matrices, molecular fingerprints and corresponding target values in the data, respectively.Input for C-SGEN Model\n",
    "\n",
    "full_feature, edge and `Interactions.npy` are molecular features, adjacency matrices and corresponding target values of `pytorch_geometric` specific data format in the data, respectively.\n",
    "\n",
    "\n",
    "#### Model Hyper-Parameters\n",
    "```\n",
    "  --epochs                      INT     Number of epochs.                              Default is 33.\n",
    "  --batch-size                  INT     Number fo molecules per batch.                 Default is 8.\n",
    "  --C-SGEL-layers               INT     Number of C-SGELs.                             Default is 2.\n",
    "  --ch_num                      INT     Number of neurons in Graph embedding layer.    Default is 16.\n",
    "  --k                           INT     Number of filters in conv1d.                   Default is 4.\n",
    "  --lr_decay                    FLOAT   Weight decay / 10 epochs.                      Defatuls is 0.5.\n",
    "  --learning-rate               FLOAT   Adam learning rate.                            Default is 5e-4.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlIlJAVrxLdF"
   },
   "source": [
    "### Examples\n",
    "\n",
    "The following commands learn a model and save the predictions. Training C-SGEN model on the default dataset,the data is ready to be saved in a folder. You can execute the above model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Du0eAQg1ws8I",
    "outputId": "032c84b2-8213-4ac7-a57e-665547f979f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreeSolv----batch8--k4--lr-0.0005--iteration-33--ch_num-16--decay_interval-10\n",
      "batch: 8\n",
      "k: 4\n",
      "ch_num: 16\n",
      "decay_interval: 10\n",
      "lr: 0.0005\n",
      "lr_decay: 0.5\n",
      "iteration: 33\n",
      "Current python interpreter pathï¼š\n",
      "/usr/bin/python3\n",
      "Epoch Time(sec) Loss_train Loss_dev Loss_test RMSE_train RMSE_dev RMSE_test\n",
      "epoch:1-train loss: 0.365,valid loss: 0.238,test loss: 0.165, valid rmse: 1.874, test rmse: 1.564, time: 0.505\n",
      "epoch:2-train loss: 0.132,valid loss: 0.140,test loss: 0.093, valid rmse: 1.438, test rmse: 1.173, time: 0.508\n",
      "epoch:3-train loss: 0.081,valid loss: 0.138,test loss: 0.059, valid rmse: 1.429, test rmse: 0.935, time: 0.503\n",
      "epoch:4-train loss: 0.072,valid loss: 0.126,test loss: 0.091, valid rmse: 1.364, test rmse: 1.162, time: 0.499\n",
      "epoch:5-train loss: 0.059,valid loss: 0.108,test loss: 0.062, valid rmse: 1.261, test rmse: 0.957, time: 0.477\n",
      "epoch:6-train loss: 0.041,valid loss: 0.227,test loss: 0.149, valid rmse: 1.833, test rmse: 1.483, time: 0.482\n",
      "epoch:7-train loss: 0.033,valid loss: 0.108,test loss: 0.049, valid rmse: 1.265, test rmse: 0.855, time: 0.478\n",
      "epoch:8-train loss: 0.029,valid loss: 0.108,test loss: 0.058, valid rmse: 1.262, test rmse: 0.924, time: 0.475\n",
      "epoch:9-train loss: 0.026,valid loss: 0.102,test loss: 0.053, valid rmse: 1.226, test rmse: 0.888, time: 0.484\n",
      "epoch:10-train loss: 0.012,valid loss: 0.089,test loss: 0.049, valid rmse: 1.146, test rmse: 0.855, time: 0.478\n",
      "epoch:11-train loss: 0.008,valid loss: 0.088,test loss: 0.056, valid rmse: 1.139, test rmse: 0.907, time: 0.479\n",
      "epoch:12-train loss: 0.007,valid loss: 0.098,test loss: 0.041, valid rmse: 1.203, test rmse: 0.779, time: 0.475\n",
      "epoch:13-train loss: 0.005,valid loss: 0.102,test loss: 0.049, valid rmse: 1.227, test rmse: 0.850, time: 0.475\n",
      "epoch:14-train loss: 0.005,valid loss: 0.095,test loss: 0.045, valid rmse: 1.188, test rmse: 0.819, time: 0.473\n",
      "epoch:15-train loss: 0.006,valid loss: 0.093,test loss: 0.039, valid rmse: 1.171, test rmse: 0.755, time: 0.471\n",
      "epoch:16-train loss: 0.004,valid loss: 0.091,test loss: 0.039, valid rmse: 1.162, test rmse: 0.760, time: 0.473\n",
      "epoch:17-train loss: 0.004,valid loss: 0.095,test loss: 0.037, valid rmse: 1.184, test rmse: 0.735, time: 0.478\n",
      "epoch:18-train loss: 0.004,valid loss: 0.090,test loss: 0.041, valid rmse: 1.155, test rmse: 0.780, time: 0.469\n",
      "epoch:19-train loss: 0.004,valid loss: 0.091,test loss: 0.050, valid rmse: 1.162, test rmse: 0.858, time: 0.481\n",
      "epoch:20-train loss: 0.003,valid loss: 0.098,test loss: 0.043, valid rmse: 1.205, test rmse: 0.793, time: 0.475\n",
      "epoch:21-train loss: 0.002,valid loss: 0.095,test loss: 0.042, valid rmse: 1.188, test rmse: 0.792, time: 0.469\n",
      "epoch:22-train loss: 0.001,valid loss: 0.093,test loss: 0.040, valid rmse: 1.172, test rmse: 0.768, time: 0.473\n",
      "epoch:23-train loss: 0.001,valid loss: 0.094,test loss: 0.040, valid rmse: 1.178, test rmse: 0.773, time: 0.469\n",
      "epoch:24-train loss: 0.001,valid loss: 0.093,test loss: 0.040, valid rmse: 1.171, test rmse: 0.772, time: 0.480\n",
      "epoch:25-train loss: 0.001,valid loss: 0.089,test loss: 0.042, valid rmse: 1.146, test rmse: 0.792, time: 0.511\n",
      "epoch:26-train loss: 0.001,valid loss: 0.093,test loss: 0.042, valid rmse: 1.171, test rmse: 0.786, time: 0.490\n",
      "epoch:27-train loss: 0.001,valid loss: 0.094,test loss: 0.043, valid rmse: 1.181, test rmse: 0.799, time: 0.479\n",
      "epoch:28-train loss: 0.001,valid loss: 0.093,test loss: 0.036, valid rmse: 1.174, test rmse: 0.725, time: 0.472\n",
      "epoch:29-train loss: 0.001,valid loss: 0.093,test loss: 0.040, valid rmse: 1.173, test rmse: 0.772, time: 0.477\n",
      "epoch:30-train loss: 0.001,valid loss: 0.093,test loss: 0.041, valid rmse: 1.171, test rmse: 0.777, time: 0.479\n",
      "epoch:31-train loss: 0.001,valid loss: 0.093,test loss: 0.041, valid rmse: 1.170, test rmse: 0.780, time: 0.470\n",
      "epoch:32-train loss: 0.001,valid loss: 0.093,test loss: 0.040, valid rmse: 1.175, test rmse: 0.767, time: 0.473\n",
      "epoch:33-train loss: 0.001,valid loss: 0.093,test loss: 0.039, valid rmse: 1.170, test rmse: 0.758, time: 0.485\n",
      "RMSE_k_valid [1.1695796020139184]\n",
      "RMSE_k_test [0.7583424957567303]\n",
      "epoch:1-train loss: 0.358,valid loss: 0.203,test loss: 0.212, valid rmse: 1.732, test rmse: 1.770, time: 0.479\n",
      "epoch:2-train loss: 0.142,valid loss: 0.191,test loss: 0.125, valid rmse: 1.681, test rmse: 1.359, time: 0.477\n",
      "epoch:3-train loss: 0.074,valid loss: 0.121,test loss: 0.074, valid rmse: 1.338, test rmse: 1.048, time: 0.476\n",
      "epoch:4-train loss: 0.049,valid loss: 0.146,test loss: 0.070, valid rmse: 1.471, test rmse: 1.016, time: 0.479\n",
      "epoch:5-train loss: 0.045,valid loss: 0.140,test loss: 0.062, valid rmse: 1.438, test rmse: 0.954, time: 0.480\n",
      "epoch:6-train loss: 0.035,valid loss: 0.118,test loss: 0.098, valid rmse: 1.319, test rmse: 1.206, time: 0.473\n",
      "epoch:7-train loss: 0.031,valid loss: 0.147,test loss: 0.071, valid rmse: 1.472, test rmse: 1.025, time: 0.481\n",
      "epoch:8-train loss: 0.030,valid loss: 0.103,test loss: 0.054, valid rmse: 1.236, test rmse: 0.897, time: 0.477\n",
      "epoch:9-train loss: 0.024,valid loss: 0.114,test loss: 0.077, valid rmse: 1.297, test rmse: 1.067, time: 0.478\n",
      "epoch:10-train loss: 0.020,valid loss: 0.100,test loss: 0.056, valid rmse: 1.219, test rmse: 0.911, time: 0.473\n",
      "epoch:11-train loss: 0.009,valid loss: 0.097,test loss: 0.050, valid rmse: 1.196, test rmse: 0.862, time: 0.470\n",
      "epoch:12-train loss: 0.006,valid loss: 0.107,test loss: 0.045, valid rmse: 1.257, test rmse: 0.817, time: 0.477\n",
      "epoch:13-train loss: 0.006,valid loss: 0.098,test loss: 0.047, valid rmse: 1.205, test rmse: 0.834, time: 0.485\n",
      "epoch:14-train loss: 0.006,valid loss: 0.099,test loss: 0.045, valid rmse: 1.211, test rmse: 0.814, time: 0.478\n",
      "epoch:15-train loss: 0.004,valid loss: 0.100,test loss: 0.051, valid rmse: 1.217, test rmse: 0.869, time: 0.478\n",
      "epoch:16-train loss: 0.005,valid loss: 0.106,test loss: 0.041, valid rmse: 1.249, test rmse: 0.783, time: 0.486\n",
      "epoch:17-train loss: 0.005,valid loss: 0.106,test loss: 0.044, valid rmse: 1.254, test rmse: 0.810, time: 0.474\n",
      "epoch:18-train loss: 0.008,valid loss: 0.109,test loss: 0.042, valid rmse: 1.268, test rmse: 0.793, time: 0.479\n",
      "epoch:19-train loss: 0.007,valid loss: 0.100,test loss: 0.045, valid rmse: 1.214, test rmse: 0.814, time: 0.482\n",
      "epoch:20-train loss: 0.005,valid loss: 0.106,test loss: 0.052, valid rmse: 1.251, test rmse: 0.875, time: 0.470\n",
      "epoch:21-train loss: 0.002,valid loss: 0.103,test loss: 0.038, valid rmse: 1.235, test rmse: 0.751, time: 0.479\n",
      "epoch:22-train loss: 0.002,valid loss: 0.101,test loss: 0.046, valid rmse: 1.223, test rmse: 0.823, time: 0.495\n",
      "epoch:23-train loss: 0.001,valid loss: 0.101,test loss: 0.046, valid rmse: 1.222, test rmse: 0.821, time: 0.497\n",
      "epoch:24-train loss: 0.001,valid loss: 0.103,test loss: 0.045, valid rmse: 1.232, test rmse: 0.819, time: 0.496\n",
      "epoch:25-train loss: 0.001,valid loss: 0.101,test loss: 0.044, valid rmse: 1.225, test rmse: 0.802, time: 0.485\n",
      "epoch:26-train loss: 0.001,valid loss: 0.103,test loss: 0.044, valid rmse: 1.235, test rmse: 0.803, time: 0.481\n",
      "epoch:27-train loss: 0.001,valid loss: 0.100,test loss: 0.045, valid rmse: 1.219, test rmse: 0.813, time: 0.482\n",
      "epoch:28-train loss: 0.001,valid loss: 0.102,test loss: 0.044, valid rmse: 1.229, test rmse: 0.810, time: 0.483\n",
      "epoch:29-train loss: 0.001,valid loss: 0.099,test loss: 0.046, valid rmse: 1.208, test rmse: 0.827, time: 0.475\n",
      "epoch:30-train loss: 0.001,valid loss: 0.100,test loss: 0.044, valid rmse: 1.218, test rmse: 0.807, time: 0.468\n",
      "epoch:31-train loss: 0.001,valid loss: 0.102,test loss: 0.044, valid rmse: 1.230, test rmse: 0.807, time: 0.467\n",
      "epoch:32-train loss: 0.001,valid loss: 0.101,test loss: 0.044, valid rmse: 1.220, test rmse: 0.808, time: 0.472\n",
      "epoch:33-train loss: 0.001,valid loss: 0.103,test loss: 0.044, valid rmse: 1.237, test rmse: 0.811, time: 0.478\n",
      "RMSE_k_valid [1.1695796020139184, 1.236914551199594]\n",
      "RMSE_k_test [0.7583424957567303, 0.8105052306929426]\n",
      "epoch:1-train loss: 0.362,valid loss: 0.235,test loss: 0.184, valid rmse: 1.864, test rmse: 1.651, time: 0.489\n",
      "epoch:2-train loss: 0.121,valid loss: 0.165,test loss: 0.100, valid rmse: 1.560, test rmse: 1.215, time: 0.479\n",
      "epoch:3-train loss: 0.075,valid loss: 0.108,test loss: 0.071, valid rmse: 1.261, test rmse: 1.021, time: 0.476\n",
      "epoch:4-train loss: 0.050,valid loss: 0.138,test loss: 0.056, valid rmse: 1.426, test rmse: 0.906, time: 0.480\n",
      "epoch:5-train loss: 0.049,valid loss: 0.089,test loss: 0.047, valid rmse: 1.148, test rmse: 0.830, time: 0.483\n",
      "epoch:6-train loss: 0.033,valid loss: 0.117,test loss: 0.047, valid rmse: 1.316, test rmse: 0.838, time: 0.476\n",
      "epoch:7-train loss: 0.025,valid loss: 0.110,test loss: 0.045, valid rmse: 1.276, test rmse: 0.812, time: 0.475\n",
      "epoch:8-train loss: 0.030,valid loss: 0.128,test loss: 0.053, valid rmse: 1.373, test rmse: 0.882, time: 0.477\n",
      "epoch:9-train loss: 0.042,valid loss: 0.113,test loss: 0.048, valid rmse: 1.293, test rmse: 0.842, time: 0.476\n",
      "epoch:10-train loss: 0.027,valid loss: 0.091,test loss: 0.043, valid rmse: 1.159, test rmse: 0.799, time: 0.474\n",
      "epoch:11-train loss: 0.010,valid loss: 0.099,test loss: 0.042, valid rmse: 1.210, test rmse: 0.786, time: 0.482\n",
      "epoch:12-train loss: 0.008,valid loss: 0.101,test loss: 0.050, valid rmse: 1.220, test rmse: 0.863, time: 0.479\n",
      "epoch:13-train loss: 0.006,valid loss: 0.093,test loss: 0.043, valid rmse: 1.172, test rmse: 0.796, time: 0.481\n",
      "epoch:14-train loss: 0.006,valid loss: 0.086,test loss: 0.044, valid rmse: 1.125, test rmse: 0.809, time: 0.483\n",
      "epoch:15-train loss: 0.005,valid loss: 0.102,test loss: 0.044, valid rmse: 1.227, test rmse: 0.806, time: 0.479\n",
      "epoch:16-train loss: 0.004,valid loss: 0.099,test loss: 0.037, valid rmse: 1.209, test rmse: 0.744, time: 0.481\n",
      "epoch:17-train loss: 0.005,valid loss: 0.103,test loss: 0.043, valid rmse: 1.234, test rmse: 0.801, time: 0.475\n",
      "epoch:18-train loss: 0.005,valid loss: 0.084,test loss: 0.039, valid rmse: 1.114, test rmse: 0.760, time: 0.486\n",
      "epoch:19-train loss: 0.006,valid loss: 0.091,test loss: 0.043, valid rmse: 1.162, test rmse: 0.793, time: 0.492\n",
      "epoch:20-train loss: 0.005,valid loss: 0.096,test loss: 0.040, valid rmse: 1.189, test rmse: 0.768, time: 0.518\n",
      "epoch:21-train loss: 0.002,valid loss: 0.096,test loss: 0.041, valid rmse: 1.194, test rmse: 0.782, time: 0.509\n",
      "epoch:22-train loss: 0.002,valid loss: 0.090,test loss: 0.040, valid rmse: 1.153, test rmse: 0.768, time: 0.512\n",
      "epoch:23-train loss: 0.002,valid loss: 0.092,test loss: 0.039, valid rmse: 1.167, test rmse: 0.761, time: 0.499\n",
      "epoch:24-train loss: 0.001,valid loss: 0.092,test loss: 0.041, valid rmse: 1.165, test rmse: 0.780, time: 0.502\n",
      "epoch:25-train loss: 0.001,valid loss: 0.095,test loss: 0.042, valid rmse: 1.184, test rmse: 0.787, time: 0.491\n",
      "epoch:26-train loss: 0.001,valid loss: 0.093,test loss: 0.040, valid rmse: 1.176, test rmse: 0.772, time: 0.486\n",
      "epoch:27-train loss: 0.001,valid loss: 0.091,test loss: 0.043, valid rmse: 1.157, test rmse: 0.794, time: 0.483\n",
      "epoch:28-train loss: 0.001,valid loss: 0.095,test loss: 0.042, valid rmse: 1.184, test rmse: 0.789, time: 0.482\n",
      "epoch:29-train loss: 0.001,valid loss: 0.093,test loss: 0.041, valid rmse: 1.173, test rmse: 0.783, time: 0.506\n",
      "epoch:30-train loss: 0.001,valid loss: 0.091,test loss: 0.041, valid rmse: 1.157, test rmse: 0.781, time: 0.479\n",
      "epoch:31-train loss: 0.001,valid loss: 0.092,test loss: 0.041, valid rmse: 1.168, test rmse: 0.780, time: 0.482\n",
      "epoch:32-train loss: 0.001,valid loss: 0.091,test loss: 0.041, valid rmse: 1.162, test rmse: 0.779, time: 0.485\n",
      "epoch:33-train loss: 0.001,valid loss: 0.091,test loss: 0.041, valid rmse: 1.161, test rmse: 0.774, time: 0.478\n",
      "RMSE_k_valid [1.1695796020139184, 1.236914551199594, 1.1612906722484622]\n",
      "RMSE_k_test [0.7583424957567303, 0.8105052306929426, 0.7742337291554489]\n",
      "result:, RMSE:1.189, RMSE_std:0.034\n",
      "result:, RMSE:0.781, RMSE_std:0.022\n"
     ]
    }
   ],
   "source": [
    "!python C-SGEN_trian.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqq6W2fR0CuV"
   },
   "source": [
    "Training a PyG model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhV9k8Dh0DIH",
    "outputId": "d4973d4d-4380-4733-9f3a-da008a592a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay_interval: 10\n",
      "lr: 0.01\n",
      "ARMA-epoch:1,---train loss: 3.035,valid loss: 0.748,test loss: 0.665, valid rmse: 3.325, test rmse: 3.112, time: 0.653\n",
      "ARMA-epoch:2,---train loss: 0.674,valid loss: 0.581,test loss: 0.387, valid rmse: 2.931, test rmse: 2.516, time: 0.441\n",
      "ARMA-epoch:3,---train loss: 0.424,valid loss: 0.389,test loss: 0.312, valid rmse: 2.398, test rmse: 1.936, time: 0.448\n",
      "ARMA-epoch:4,---train loss: 0.363,valid loss: 0.399,test loss: 0.216, valid rmse: 2.428, test rmse: 1.881, time: 0.439\n",
      "ARMA-epoch:5,---train loss: 0.267,valid loss: 0.239,test loss: 0.109, valid rmse: 1.878, test rmse: 1.302, time: 0.441\n",
      "ARMA-epoch:6,---train loss: 0.198,valid loss: 0.290,test loss: 0.216, valid rmse: 2.069, test rmse: 1.608, time: 0.454\n",
      "ARMA-epoch:7,---train loss: 0.228,valid loss: 0.260,test loss: 0.127, valid rmse: 1.960, test rmse: 1.368, time: 0.444\n",
      "ARMA-epoch:8,---train loss: 0.242,valid loss: 0.427,test loss: 0.319, valid rmse: 2.512, test rmse: 2.164, time: 0.453\n",
      "ARMA-epoch:9,---train loss: 0.193,valid loss: 0.222,test loss: 0.147, valid rmse: 1.813, test rmse: 1.477, time: 0.436\n",
      "ARMA-epoch:10,---train loss: 0.157,valid loss: 0.232,test loss: 0.285, valid rmse: 1.853, test rmse: 1.401, time: 0.440\n",
      "ARMA-epoch:11,---train loss: 0.124,valid loss: 0.178,test loss: 0.081, valid rmse: 1.620, test rmse: 1.093, time: 0.440\n",
      "ARMA-epoch:12,---train loss: 0.113,valid loss: 0.221,test loss: 0.150, valid rmse: 1.809, test rmse: 1.551, time: 0.436\n",
      "ARMA-epoch:13,---train loss: 0.119,valid loss: 0.171,test loss: 0.083, valid rmse: 1.591, test rmse: 1.162, time: 0.444\n",
      "ARMA-epoch:14,---train loss: 0.108,valid loss: 0.185,test loss: 0.126, valid rmse: 1.654, test rmse: 1.372, time: 0.446\n",
      "ARMA-epoch:15,---train loss: 0.135,valid loss: 0.178,test loss: 0.096, valid rmse: 1.621, test rmse: 1.251, time: 0.439\n",
      "ARMA-epoch:16,---train loss: 0.117,valid loss: 0.168,test loss: 0.093, valid rmse: 1.576, test rmse: 1.222, time: 0.442\n",
      "ARMA-epoch:17,---train loss: 0.125,valid loss: 0.157,test loss: 0.088, valid rmse: 1.522, test rmse: 1.087, time: 0.433\n",
      "ARMA-epoch:18,---train loss: 0.106,valid loss: 0.168,test loss: 0.083, valid rmse: 1.576, test rmse: 1.166, time: 0.437\n",
      "ARMA-epoch:19,---train loss: 0.137,valid loss: 0.165,test loss: 0.085, valid rmse: 1.564, test rmse: 1.117, time: 0.439\n",
      "ARMA-epoch:20,---train loss: 0.089,valid loss: 0.142,test loss: 0.080, valid rmse: 1.447, test rmse: 1.140, time: 0.445\n",
      "ARMA-epoch:21,---train loss: 0.085,valid loss: 0.144,test loss: 0.068, valid rmse: 1.457, test rmse: 1.050, time: 0.447\n",
      "ARMA-epoch:22,---train loss: 0.080,valid loss: 0.150,test loss: 0.123, valid rmse: 1.487, test rmse: 1.165, time: 0.445\n",
      "ARMA-epoch:23,---train loss: 0.079,valid loss: 0.149,test loss: 0.082, valid rmse: 1.483, test rmse: 1.079, time: 0.445\n",
      "ARMA-epoch:24,---train loss: 0.095,valid loss: 0.167,test loss: 0.104, valid rmse: 1.573, test rmse: 1.301, time: 0.444\n",
      "ARMA-epoch:25,---train loss: 0.076,valid loss: 0.142,test loss: 0.091, valid rmse: 1.446, test rmse: 1.072, time: 0.442\n",
      "ARMA-epoch:26,---train loss: 0.078,valid loss: 0.151,test loss: 0.080, valid rmse: 1.492, test rmse: 1.104, time: 0.440\n",
      "ARMA-epoch:27,---train loss: 0.079,valid loss: 0.158,test loss: 0.096, valid rmse: 1.530, test rmse: 1.111, time: 0.441\n",
      "ARMA-epoch:28,---train loss: 0.084,valid loss: 0.138,test loss: 0.066, valid rmse: 1.428, test rmse: 1.032, time: 0.444\n",
      "ARMA-epoch:29,---train loss: 0.083,valid loss: 0.147,test loss: 0.071, valid rmse: 1.476, test rmse: 1.062, time: 0.443\n",
      "ARMA-epoch:30,---train loss: 0.077,valid loss: 0.146,test loss: 0.100, valid rmse: 1.467, test rmse: 1.217, time: 0.445\n",
      "ARMA-epoch:31,---train loss: 0.074,valid loss: 0.137,test loss: 0.066, valid rmse: 1.421, test rmse: 1.037, time: 0.458\n",
      "ARMA-epoch:32,---train loss: 0.068,valid loss: 0.135,test loss: 0.111, valid rmse: 1.415, test rmse: 1.089, time: 0.445\n",
      "ARMA-epoch:33,---train loss: 0.070,valid loss: 0.133,test loss: 0.077, valid rmse: 1.404, test rmse: 1.081, time: 0.451\n",
      "ARMA-epoch:34,---train loss: 0.069,valid loss: 0.131,test loss: 0.066, valid rmse: 1.390, test rmse: 1.038, time: 0.449\n",
      "ARMA-epoch:35,---train loss: 0.064,valid loss: 0.132,test loss: 0.087, valid rmse: 1.394, test rmse: 1.066, time: 0.445\n",
      "ARMA-epoch:36,---train loss: 0.065,valid loss: 0.139,test loss: 0.089, valid rmse: 1.435, test rmse: 1.147, time: 0.442\n",
      "ARMA-epoch:37,---train loss: 0.072,valid loss: 0.137,test loss: 0.082, valid rmse: 1.425, test rmse: 1.140, time: 0.443\n",
      "ARMA-epoch:38,---train loss: 0.071,valid loss: 0.139,test loss: 0.079, valid rmse: 1.431, test rmse: 1.080, time: 0.446\n",
      "ARMA-epoch:39,---train loss: 0.065,valid loss: 0.151,test loss: 0.097, valid rmse: 1.492, test rmse: 1.167, time: 0.444\n",
      "ARMA-epoch:40,---train loss: 0.063,valid loss: 0.136,test loss: 0.096, valid rmse: 1.420, test rmse: 1.072, time: 0.453\n",
      "ARMA-epoch:41,---train loss: 0.062,valid loss: 0.141,test loss: 0.082, valid rmse: 1.442, test rmse: 1.153, time: 0.444\n",
      "ARMA-epoch:42,---train loss: 0.066,valid loss: 0.136,test loss: 0.080, valid rmse: 1.416, test rmse: 1.025, time: 0.455\n",
      "ARMA-epoch:43,---train loss: 0.062,valid loss: 0.135,test loss: 0.068, valid rmse: 1.412, test rmse: 1.046, time: 0.446\n",
      "ARMA-epoch:44,---train loss: 0.063,valid loss: 0.136,test loss: 0.095, valid rmse: 1.418, test rmse: 1.069, time: 0.444\n",
      "ARMA-epoch:45,---train loss: 0.062,valid loss: 0.129,test loss: 0.134, valid rmse: 1.381, test rmse: 1.009, time: 0.438\n",
      "ARMA-epoch:46,---train loss: 0.060,valid loss: 0.132,test loss: 0.066, valid rmse: 1.397, test rmse: 1.029, time: 0.435\n",
      "ARMA-epoch:47,---train loss: 0.061,valid loss: 0.132,test loss: 0.070, valid rmse: 1.396, test rmse: 1.067, time: 0.444\n",
      "ARMA-epoch:48,---train loss: 0.059,valid loss: 0.138,test loss: 0.083, valid rmse: 1.429, test rmse: 1.115, time: 0.439\n",
      "ARMA-epoch:49,---train loss: 0.061,valid loss: 0.140,test loss: 0.087, valid rmse: 1.439, test rmse: 1.152, time: 0.444\n",
      "ARMA-epoch:50,---train loss: 0.056,valid loss: 0.131,test loss: 0.080, valid rmse: 1.391, test rmse: 1.043, time: 0.444\n",
      "RMSE_k_valid [1.390656949342825]\n",
      "RMSE_k_test [1.0428266754510749]\n",
      "ARMA-epoch:1,---train loss: 1.463,valid loss: 0.648,test loss: 0.648, valid rmse: 3.094, test rmse: 2.901, time: 0.442\n",
      "ARMA-epoch:2,---train loss: 0.431,valid loss: 0.379,test loss: 0.232, valid rmse: 2.366, test rmse: 1.934, time: 0.439\n",
      "ARMA-epoch:3,---train loss: 0.338,valid loss: 0.351,test loss: 0.220, valid rmse: 2.278, test rmse: 1.628, time: 0.441\n",
      "ARMA-epoch:4,---train loss: 0.336,valid loss: 0.342,test loss: 0.168, valid rmse: 2.248, test rmse: 1.544, time: 0.449\n",
      "ARMA-epoch:5,---train loss: 0.286,valid loss: 0.285,test loss: 0.133, valid rmse: 2.052, test rmse: 1.472, time: 0.440\n",
      "ARMA-epoch:6,---train loss: 0.202,valid loss: 0.249,test loss: 0.142, valid rmse: 1.920, test rmse: 1.350, time: 0.463\n",
      "ARMA-epoch:7,---train loss: 0.192,valid loss: 0.260,test loss: 0.143, valid rmse: 1.961, test rmse: 1.473, time: 0.468\n",
      "ARMA-epoch:8,---train loss: 0.185,valid loss: 0.342,test loss: 0.244, valid rmse: 2.249, test rmse: 1.952, time: 0.465\n",
      "ARMA-epoch:9,---train loss: 0.220,valid loss: 0.282,test loss: 0.220, valid rmse: 2.042, test rmse: 1.893, time: 0.443\n",
      "ARMA-epoch:10,---train loss: 0.150,valid loss: 0.167,test loss: 0.080, valid rmse: 1.569, test rmse: 1.146, time: 0.434\n",
      "ARMA-epoch:11,---train loss: 0.118,valid loss: 0.209,test loss: 0.140, valid rmse: 1.759, test rmse: 1.491, time: 0.450\n",
      "ARMA-epoch:12,---train loss: 0.110,valid loss: 0.166,test loss: 0.076, valid rmse: 1.565, test rmse: 1.099, time: 0.448\n",
      "ARMA-epoch:13,---train loss: 0.116,valid loss: 0.177,test loss: 0.099, valid rmse: 1.619, test rmse: 1.270, time: 0.442\n",
      "ARMA-epoch:14,---train loss: 0.102,valid loss: 0.168,test loss: 0.099, valid rmse: 1.577, test rmse: 1.190, time: 0.444\n",
      "ARMA-epoch:15,---train loss: 0.101,valid loss: 0.209,test loss: 0.152, valid rmse: 1.759, test rmse: 1.529, time: 0.442\n",
      "ARMA-epoch:16,---train loss: 0.130,valid loss: 0.225,test loss: 0.122, valid rmse: 1.824, test rmse: 1.278, time: 0.448\n",
      "ARMA-epoch:17,---train loss: 0.109,valid loss: 0.162,test loss: 0.068, valid rmse: 1.546, test rmse: 1.045, time: 0.446\n",
      "ARMA-epoch:18,---train loss: 0.121,valid loss: 0.169,test loss: 0.069, valid rmse: 1.579, test rmse: 1.059, time: 0.441\n",
      "ARMA-epoch:19,---train loss: 0.107,valid loss: 0.154,test loss: 0.075, valid rmse: 1.510, test rmse: 1.029, time: 0.444\n",
      "ARMA-epoch:20,---train loss: 0.113,valid loss: 0.152,test loss: 0.104, valid rmse: 1.499, test rmse: 1.136, time: 0.447\n",
      "ARMA-epoch:21,---train loss: 0.084,valid loss: 0.141,test loss: 0.063, valid rmse: 1.446, test rmse: 1.010, time: 0.443\n",
      "ARMA-epoch:22,---train loss: 0.085,valid loss: 0.143,test loss: 0.079, valid rmse: 1.456, test rmse: 1.017, time: 0.444\n",
      "ARMA-epoch:23,---train loss: 0.082,valid loss: 0.144,test loss: 0.062, valid rmse: 1.459, test rmse: 1.007, time: 0.444\n",
      "ARMA-epoch:24,---train loss: 0.076,valid loss: 0.141,test loss: 0.068, valid rmse: 1.443, test rmse: 0.975, time: 0.443\n",
      "ARMA-epoch:25,---train loss: 0.081,valid loss: 0.148,test loss: 0.069, valid rmse: 1.478, test rmse: 1.052, time: 0.440\n",
      "ARMA-epoch:26,---train loss: 0.083,valid loss: 0.143,test loss: 0.064, valid rmse: 1.454, test rmse: 1.021, time: 0.446\n",
      "ARMA-epoch:27,---train loss: 0.079,valid loss: 0.162,test loss: 0.094, valid rmse: 1.545, test rmse: 1.232, time: 0.447\n",
      "ARMA-epoch:28,---train loss: 0.080,valid loss: 0.150,test loss: 0.067, valid rmse: 1.488, test rmse: 1.034, time: 0.448\n",
      "ARMA-epoch:29,---train loss: 0.085,valid loss: 0.132,test loss: 0.064, valid rmse: 1.395, test rmse: 1.002, time: 0.439\n",
      "ARMA-epoch:30,---train loss: 0.070,valid loss: 0.140,test loss: 0.104, valid rmse: 1.441, test rmse: 1.166, time: 0.441\n",
      "ARMA-epoch:31,---train loss: 0.073,valid loss: 0.133,test loss: 0.060, valid rmse: 1.400, test rmse: 0.987, time: 0.446\n",
      "ARMA-epoch:32,---train loss: 0.071,valid loss: 0.131,test loss: 0.055, valid rmse: 1.392, test rmse: 0.943, time: 0.450\n",
      "ARMA-epoch:33,---train loss: 0.079,valid loss: 0.145,test loss: 0.065, valid rmse: 1.464, test rmse: 0.995, time: 0.450\n",
      "ARMA-epoch:34,---train loss: 0.069,valid loss: 0.135,test loss: 0.071, valid rmse: 1.412, test rmse: 1.074, time: 0.450\n",
      "ARMA-epoch:35,---train loss: 0.066,valid loss: 0.134,test loss: 0.065, valid rmse: 1.408, test rmse: 1.008, time: 0.449\n",
      "ARMA-epoch:36,---train loss: 0.064,valid loss: 0.138,test loss: 0.059, valid rmse: 1.430, test rmse: 0.962, time: 0.444\n",
      "ARMA-epoch:37,---train loss: 0.066,valid loss: 0.134,test loss: 0.061, valid rmse: 1.405, test rmse: 0.987, time: 0.443\n",
      "ARMA-epoch:38,---train loss: 0.063,valid loss: 0.134,test loss: 0.075, valid rmse: 1.410, test rmse: 1.058, time: 0.439\n",
      "ARMA-epoch:39,---train loss: 0.065,valid loss: 0.144,test loss: 0.071, valid rmse: 1.457, test rmse: 1.056, time: 0.445\n",
      "ARMA-epoch:40,---train loss: 0.062,valid loss: 0.131,test loss: 0.140, valid rmse: 1.393, test rmse: 0.940, time: 0.443\n",
      "ARMA-epoch:41,---train loss: 0.060,valid loss: 0.131,test loss: 0.066, valid rmse: 1.393, test rmse: 0.967, time: 0.431\n",
      "ARMA-epoch:42,---train loss: 0.058,valid loss: 0.128,test loss: 0.057, valid rmse: 1.376, test rmse: 0.955, time: 0.449\n",
      "ARMA-epoch:43,---train loss: 0.059,valid loss: 0.130,test loss: 0.059, valid rmse: 1.386, test rmse: 0.984, time: 0.481\n",
      "ARMA-epoch:44,---train loss: 0.060,valid loss: 0.129,test loss: 0.065, valid rmse: 1.383, test rmse: 1.010, time: 0.468\n",
      "ARMA-epoch:45,---train loss: 0.058,valid loss: 0.132,test loss: 0.066, valid rmse: 1.395, test rmse: 1.043, time: 0.464\n",
      "ARMA-epoch:46,---train loss: 0.057,valid loss: 0.129,test loss: 0.080, valid rmse: 1.381, test rmse: 1.033, time: 0.468\n",
      "ARMA-epoch:47,---train loss: 0.057,valid loss: 0.127,test loss: 0.061, valid rmse: 1.369, test rmse: 0.955, time: 0.451\n",
      "ARMA-epoch:48,---train loss: 0.058,valid loss: 0.126,test loss: 0.056, valid rmse: 1.366, test rmse: 0.937, time: 0.456\n",
      "ARMA-epoch:49,---train loss: 0.056,valid loss: 0.126,test loss: 0.067, valid rmse: 1.365, test rmse: 0.930, time: 0.437\n",
      "ARMA-epoch:50,---train loss: 0.055,valid loss: 0.125,test loss: 0.067, valid rmse: 1.358, test rmse: 0.935, time: 0.448\n",
      "RMSE_k_valid [1.390656949342825, 1.3584418547688566]\n",
      "RMSE_k_test [1.0428266754510749, 0.935364235641485]\n",
      "ARMA-epoch:1,---train loss: 2.221,valid loss: 0.710,test loss: 0.669, valid rmse: 3.241, test rmse: 3.238, time: 0.445\n",
      "ARMA-epoch:2,---train loss: 0.565,valid loss: 0.562,test loss: 0.384, valid rmse: 2.883, test rmse: 2.453, time: 0.437\n",
      "ARMA-epoch:3,---train loss: 0.415,valid loss: 0.359,test loss: 0.255, valid rmse: 2.305, test rmse: 1.958, time: 0.438\n",
      "ARMA-epoch:4,---train loss: 0.329,valid loss: 0.371,test loss: 0.220, valid rmse: 2.341, test rmse: 1.859, time: 0.464\n",
      "ARMA-epoch:5,---train loss: 0.310,valid loss: 0.368,test loss: 0.281, valid rmse: 2.332, test rmse: 2.127, time: 0.463\n",
      "ARMA-epoch:6,---train loss: 0.707,valid loss: 1.052,test loss: 1.054, valid rmse: 3.944, test rmse: 4.045, time: 0.455\n",
      "ARMA-epoch:7,---train loss: 0.303,valid loss: 0.291,test loss: 0.139, valid rmse: 2.074, test rmse: 1.504, time: 0.444\n",
      "ARMA-epoch:8,---train loss: 0.212,valid loss: 0.988,test loss: 1.028, valid rmse: 3.822, test rmse: 3.788, time: 0.442\n",
      "ARMA-epoch:9,---train loss: 0.232,valid loss: 0.285,test loss: 0.111, valid rmse: 2.051, test rmse: 1.339, time: 0.434\n",
      "ARMA-epoch:10,---train loss: 0.165,valid loss: 0.287,test loss: 0.139, valid rmse: 2.058, test rmse: 1.504, time: 0.441\n",
      "ARMA-epoch:11,---train loss: 0.171,valid loss: 0.263,test loss: 0.178, valid rmse: 1.973, test rmse: 1.538, time: 0.446\n",
      "ARMA-epoch:12,---train loss: 0.167,valid loss: 0.270,test loss: 0.212, valid rmse: 1.996, test rmse: 1.630, time: 0.442\n",
      "ARMA-epoch:13,---train loss: 0.152,valid loss: 0.230,test loss: 0.093, valid rmse: 1.843, test rmse: 1.231, time: 0.442\n",
      "ARMA-epoch:14,---train loss: 0.121,valid loss: 0.251,test loss: 0.098, valid rmse: 1.928, test rmse: 1.236, time: 0.445\n",
      "ARMA-epoch:15,---train loss: 0.147,valid loss: 0.252,test loss: 0.137, valid rmse: 1.930, test rmse: 1.218, time: 0.443\n",
      "ARMA-epoch:16,---train loss: 0.118,valid loss: 0.244,test loss: 0.162, valid rmse: 1.898, test rmse: 1.509, time: 0.436\n",
      "ARMA-epoch:17,---train loss: 0.134,valid loss: 0.239,test loss: 0.126, valid rmse: 1.878, test rmse: 1.433, time: 0.438\n",
      "ARMA-epoch:18,---train loss: 0.145,valid loss: 0.196,test loss: 0.068, valid rmse: 1.702, test rmse: 1.043, time: 0.441\n",
      "ARMA-epoch:19,---train loss: 0.134,valid loss: 0.218,test loss: 0.104, valid rmse: 1.795, test rmse: 1.297, time: 0.445\n",
      "ARMA-epoch:20,---train loss: 0.103,valid loss: 0.213,test loss: 0.078, valid rmse: 1.774, test rmse: 1.071, time: 0.444\n",
      "ARMA-epoch:21,---train loss: 0.103,valid loss: 0.218,test loss: 0.118, valid rmse: 1.796, test rmse: 1.241, time: 0.443\n",
      "ARMA-epoch:22,---train loss: 0.090,valid loss: 0.194,test loss: 0.082, valid rmse: 1.692, test rmse: 1.125, time: 0.441\n",
      "ARMA-epoch:23,---train loss: 0.091,valid loss: 0.211,test loss: 0.162, valid rmse: 1.768, test rmse: 1.274, time: 0.448\n",
      "ARMA-epoch:24,---train loss: 0.107,valid loss: 0.204,test loss: 0.093, valid rmse: 1.737, test rmse: 1.223, time: 0.436\n",
      "ARMA-epoch:25,---train loss: 0.101,valid loss: 0.192,test loss: 0.076, valid rmse: 1.684, test rmse: 1.099, time: 0.447\n",
      "ARMA-epoch:26,---train loss: 0.085,valid loss: 0.203,test loss: 0.215, valid rmse: 1.731, test rmse: 1.176, time: 0.443\n",
      "ARMA-epoch:27,---train loss: 0.087,valid loss: 0.198,test loss: 0.088, valid rmse: 1.710, test rmse: 1.126, time: 0.441\n",
      "ARMA-epoch:28,---train loss: 0.090,valid loss: 0.204,test loss: 0.094, valid rmse: 1.736, test rmse: 1.206, time: 0.459\n",
      "ARMA-epoch:29,---train loss: 0.088,valid loss: 0.200,test loss: 0.074, valid rmse: 1.718, test rmse: 1.080, time: 0.473\n",
      "ARMA-epoch:30,---train loss: 0.078,valid loss: 0.194,test loss: 0.080, valid rmse: 1.692, test rmse: 1.099, time: 0.469\n",
      "ARMA-epoch:31,---train loss: 0.075,valid loss: 0.190,test loss: 0.082, valid rmse: 1.676, test rmse: 1.097, time: 0.455\n",
      "ARMA-epoch:32,---train loss: 0.075,valid loss: 0.186,test loss: 0.112, valid rmse: 1.659, test rmse: 1.090, time: 0.449\n",
      "ARMA-epoch:33,---train loss: 0.077,valid loss: 0.191,test loss: 0.081, valid rmse: 1.680, test rmse: 1.125, time: 0.448\n",
      "ARMA-epoch:34,---train loss: 0.074,valid loss: 0.201,test loss: 0.080, valid rmse: 1.723, test rmse: 1.107, time: 0.436\n",
      "ARMA-epoch:35,---train loss: 0.073,valid loss: 0.180,test loss: 0.072, valid rmse: 1.630, test rmse: 1.003, time: 0.440\n",
      "ARMA-epoch:36,---train loss: 0.074,valid loss: 0.191,test loss: 0.070, valid rmse: 1.680, test rmse: 1.068, time: 0.436\n",
      "ARMA-epoch:37,---train loss: 0.080,valid loss: 0.205,test loss: 0.089, valid rmse: 1.742, test rmse: 1.116, time: 0.437\n",
      "ARMA-epoch:38,---train loss: 0.076,valid loss: 0.190,test loss: 0.074, valid rmse: 1.675, test rmse: 1.083, time: 0.434\n",
      "ARMA-epoch:39,---train loss: 0.069,valid loss: 0.183,test loss: 0.069, valid rmse: 1.645, test rmse: 1.056, time: 0.436\n",
      "ARMA-epoch:40,---train loss: 0.068,valid loss: 0.198,test loss: 0.084, valid rmse: 1.710, test rmse: 1.167, time: 0.430\n",
      "ARMA-epoch:41,---train loss: 0.067,valid loss: 0.184,test loss: 0.061, valid rmse: 1.651, test rmse: 0.994, time: 0.435\n",
      "ARMA-epoch:42,---train loss: 0.066,valid loss: 0.181,test loss: 0.064, valid rmse: 1.636, test rmse: 1.024, time: 0.432\n",
      "ARMA-epoch:43,---train loss: 0.067,valid loss: 0.186,test loss: 0.071, valid rmse: 1.660, test rmse: 1.076, time: 0.436\n",
      "ARMA-epoch:44,---train loss: 0.067,valid loss: 0.192,test loss: 0.079, valid rmse: 1.683, test rmse: 1.058, time: 0.439\n",
      "ARMA-epoch:45,---train loss: 0.065,valid loss: 0.188,test loss: 0.075, valid rmse: 1.667, test rmse: 1.063, time: 0.438\n",
      "ARMA-epoch:46,---train loss: 0.066,valid loss: 0.185,test loss: 0.060, valid rmse: 1.654, test rmse: 0.985, time: 0.443\n",
      "ARMA-epoch:47,---train loss: 0.065,valid loss: 0.190,test loss: 0.086, valid rmse: 1.675, test rmse: 1.094, time: 0.434\n",
      "ARMA-epoch:48,---train loss: 0.064,valid loss: 0.187,test loss: 0.070, valid rmse: 1.664, test rmse: 1.070, time: 0.438\n",
      "ARMA-epoch:49,---train loss: 0.064,valid loss: 0.185,test loss: 0.062, valid rmse: 1.656, test rmse: 0.990, time: 0.443\n",
      "ARMA-epoch:50,---train loss: 0.062,valid loss: 0.184,test loss: 0.076, valid rmse: 1.647, test rmse: 1.062, time: 0.446\n",
      "RMSE_k_valid [1.390656949342825, 1.3584418547688566, 1.6474102307158278]\n",
      "RMSE_k_test [1.0428266754510749, 0.935364235641485, 1.0623145343808702]\n",
      "result:, valid_RMSE:1.466, valid_RMSE_std:0.129\n",
      "result:, test_RMSE:1.014, test_RMSE_std:0.056\n"
     ]
    }
   ],
   "source": [
    "!python pyg_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWG3q7iR0Fuy"
   },
   "source": [
    "Load data from DeepChem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n91yq_K10GCi",
    "outputId": "562cce0c-ba22-4500-cb0b-e07a4c64bcef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-02 22:36:28.141690: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "'split' is deprecated.  Use 'splitter' instead.\n"
     ]
    }
   ],
   "source": [
    "!python load_FreeSolv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWVtRu0735vF"
   },
   "source": [
    "This work was inspired by Large-scale learnable graph convolutional networks [(LGCN)](https://github.com/divelab/lgcn)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "C-SGEN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
