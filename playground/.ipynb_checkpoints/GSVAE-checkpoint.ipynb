{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeZOV2C7FqkH"
   },
   "source": [
    "## Physics-Constrained Predictive Molecular Latent Space Discovery with Graph Scattering Variational Autoencoder\n",
    "\n",
    "ABSTRACT: Recent advances in artificial intelligence have propelled the development of innovative\n",
    "computational materials modeling and design techniques. Generative deep learning models have been used for molecular representation, discovery and design. In this work, we\n",
    "assess the predictive capabilities of a molecular generative model developed based on variational inference and graph theory in the small data regime. Physical constraints that encourage energetically stable molecules are proposed. The encoding network is based on the\n",
    "scattering transform with adaptive spectral filters to allow for better generalization of the\n",
    "model. The decoding network is a one-shot graph generative model that conditions atom\n",
    "types on molecular topology. A Bayesian formalism is considered to capture uncertainties\n",
    "in the predictive estimates of molecular properties. The modelâ€™s performance is evaluated\n",
    "by generating molecules with desired target properties.\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/2009.13878v2.pdf\n",
    "\n",
    "Credit: https://github.com/zabaras/GSVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQAy_fiGB8qs",
    "outputId": "8df6c62c-b183-46be-8ba9-a20122752924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/GSVAE\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository and cd into directory\n",
    "!git clone https://github.com/zabaras/GSVAE.git\n",
    "%cd GSVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQkeftRBFrDu"
   },
   "outputs": [],
   "source": [
    "# Install dependencies / requirements\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install RDKit\n",
    "!pip install rdkit-pypi==2021.3.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85Z34IXpGf6V"
   },
   "source": [
    "### Data\n",
    "\n",
    "Data samples are generated through `data_gen.py`, which also performs classic bootstrapping. The script accepts the following arguments:\n",
    "\n",
    "```bash\n",
    "optional arguments:\n",
    "  --data_size           Total size of the training + test dataset (default: 100000)\n",
    "  --N                   Size of the training set. This only affects the bootstrapping (default: 600)\n",
    "  --n_samples           Number of the bootstrap samples (default: 1, no bootstrap)\n",
    "```\n",
    "\n",
    "Note that the Bayesian bootstrapping is done in the main code. To generate data, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrjWxvSxGJ-F",
    "outputId": "3006eebf-46f3-4855-8aa6-0be11e9ccae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/GSVAE/data\n",
      "Extracting QM9 dataset, it takes time...\n",
      "Downloading from https://ndownloader.figshare.com/files/3195389...\n",
      "100% 133885/133885 [00:06<00:00, 19479.81it/s]\n",
      "100% 100000/100000 [02:05<00:00, 799.28it/s]\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "%cd GSVAE/data\n",
    "!python data_gen.py\n",
    "%cd GSVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgiUjMXAGpz3"
   },
   "source": [
    "### Run\n",
    "\n",
    "#### Training\n",
    "\n",
    "The model is trained using `main.py`. This code accepts the following arguments:\n",
    "\n",
    "```bash\n",
    "optional arguments:\n",
    "  --epochs              number of epochs to train (default: 1900)\n",
    "  --batch_number        number of batches per epoch (default: 25)\n",
    "  --gpu_mode            accelerate the script using GPU (default: 1)\n",
    "  --z_dim               latent space dimensionality (default: 30)\n",
    "  --seed                random seed (default: 1400)\n",
    "  --loadtrainedmodel    path to trained model\n",
    "  --mu_reg_1            regularization parameter for ghost nodes and valence constraint (default: 0)\n",
    "  --mu_reg_2            regularization parameter for connectivity constraint (default: 0)\n",
    "  --mu_reg_3            regularization parameter for 3-member cycle constraint (default: 0)\n",
    "  --mu_reg_4            regularization parameter for cycle with triple bond constraint (default: 0)\n",
    "  --N_vis               number of test data for visualization (default: 3000)\n",
    "  --log_interval        number of epochs between visualizations (default: 200)\n",
    "  --mol_vis             visualize samples molecules (default: 0)\n",
    "  --n_samples           number of generated samples from molecular space (default: 10000)\n",
    "  --wlt_scales          number of wavelet scales (default: 12)\n",
    "  --scat_layers         number of scattering layers (default: 4)\n",
    "  --database            name of the training database (default: 'QM9')\n",
    "  --datafile            name and location of the training file in data folder (default: 'QM9_0.data')\n",
    "  --BB_samples          index for Bayesian bootstrap sample (default: 0)\n",
    "  --N                   number of training data (default: 600)\n",
    "  --res                 path for storing the results (default: 'results/')\n",
    "  --y_id                index for target property in the conditional design (default: None, unconditional design)\n",
    "  --y_target            target property value in the conditional design (default: None, unconditional design)\n",
    "```\n",
    "\n",
    "After generating the data, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMMYzTQfGl6l",
    "outputId": "1a7b768d-6cb9-49f4-a4bb-7e4818fce856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count() 1\n",
      "get_device_name Tesla P100-PCIE-16GB\n",
      "Train Epoch: 1\tLoss: 1.989043\n",
      "Train Epoch: 2\tLoss: 1.236556\n",
      "Train Epoch: 3\tLoss: 1.109881\n",
      "Train Epoch: 4\tLoss: 1.063782\n",
      "Train Epoch: 5\tLoss: 1.038642\n",
      "Train Epoch: 6\tLoss: 1.017001\n",
      "Train Epoch: 7\tLoss: 1.005502\n",
      "Train Epoch: 8\tLoss: 0.990827\n",
      "Train Epoch: 9\tLoss: 0.981253\n",
      "Train Epoch: 10\tLoss: 0.976275\n",
      "Train Epoch: 11\tLoss: 0.972006\n",
      "Train Epoch: 12\tLoss: 0.969025\n",
      "Train Epoch: 13\tLoss: 0.966821\n",
      "Train Epoch: 14\tLoss: 0.962601\n",
      "Train Epoch: 15\tLoss: 0.959713\n",
      "Train Epoch: 16\tLoss: 0.954832\n",
      "Train Epoch: 17\tLoss: 0.953984\n",
      "Train Epoch: 18\tLoss: 0.955744\n",
      "Train Epoch: 19\tLoss: 0.950577\n",
      "Train Epoch: 20\tLoss: 0.950971\n",
      "Train Epoch: 21\tLoss: 0.953846\n",
      "Train Epoch: 22\tLoss: 0.959475\n",
      "Train Epoch: 23\tLoss: 0.962950\n",
      "Train Epoch: 24\tLoss: 0.960788\n",
      "Train Epoch: 25\tLoss: 0.956558\n",
      "Train Epoch: 26\tLoss: 0.953923\n",
      "Train Epoch: 27\tLoss: 0.950419\n",
      "Train Epoch: 28\tLoss: 0.947926\n",
      "Train Epoch: 29\tLoss: 0.952807\n",
      "Train Epoch: 30\tLoss: 0.947883\n",
      "Train Epoch: 31\tLoss: 0.946758\n",
      "Train Epoch: 32\tLoss: 0.940439\n",
      "Train Epoch: 33\tLoss: 0.942255\n",
      "Train Epoch: 34\tLoss: 0.937230\n",
      "Train Epoch: 35\tLoss: 0.935679\n",
      "Train Epoch: 36\tLoss: 0.929789\n",
      "Train Epoch: 37\tLoss: 0.931046\n",
      "Train Epoch: 38\tLoss: 0.922349\n",
      "Train Epoch: 39\tLoss: 0.927216\n",
      "Train Epoch: 40\tLoss: 0.926424\n",
      "Train Epoch: 41\tLoss: 0.923611\n",
      "Train Epoch: 42\tLoss: 0.922136\n",
      "Train Epoch: 43\tLoss: 0.922237\n",
      "Train Epoch: 44\tLoss: 0.918601\n",
      "Train Epoch: 45\tLoss: 0.914547\n",
      "Train Epoch: 46\tLoss: 0.916848\n",
      "Train Epoch: 47\tLoss: 0.914617\n",
      "Train Epoch: 48\tLoss: 0.914880\n",
      "Train Epoch: 49\tLoss: 0.907851\n",
      "Train Epoch: 50\tLoss: 0.909033\n",
      "Train Epoch: 51\tLoss: 0.906034\n",
      "Train Epoch: 52\tLoss: 0.902622\n",
      "Train Epoch: 53\tLoss: 0.902619\n",
      "Train Epoch: 54\tLoss: 0.907416\n",
      "Train Epoch: 55\tLoss: 0.908431\n",
      "Train Epoch: 56\tLoss: 0.906236\n",
      "Train Epoch: 57\tLoss: 0.901317\n",
      "Train Epoch: 58\tLoss: 0.901126\n",
      "Train Epoch: 59\tLoss: 0.891536\n",
      "Train Epoch: 60\tLoss: 0.888742\n",
      "Train Epoch: 61\tLoss: 0.885311\n",
      "Train Epoch: 62\tLoss: 0.889656\n",
      "Train Epoch: 63\tLoss: 0.885604\n",
      "Train Epoch: 64\tLoss: 0.883982\n",
      "Train Epoch: 65\tLoss: 0.877470\n",
      "Train Epoch: 66\tLoss: 0.879204\n",
      "Train Epoch: 67\tLoss: 0.876495\n",
      "Train Epoch: 68\tLoss: 0.881723\n",
      "Train Epoch: 69\tLoss: 0.876254\n",
      "Train Epoch: 70\tLoss: 0.877102\n",
      "Train Epoch: 71\tLoss: 0.875389\n",
      "Train Epoch: 72\tLoss: 0.867925\n",
      "Train Epoch: 73\tLoss: 0.864102\n",
      "Train Epoch: 74\tLoss: 0.859878\n",
      "Train Epoch: 75\tLoss: 0.863725\n",
      "Train Epoch: 76\tLoss: 0.854130\n",
      "Train Epoch: 77\tLoss: 0.855613\n",
      "Train Epoch: 78\tLoss: 0.853426\n",
      "Train Epoch: 79\tLoss: 0.854539\n",
      "Train Epoch: 80\tLoss: 0.857782\n",
      "Train Epoch: 81\tLoss: 0.863280\n",
      "Train Epoch: 82\tLoss: 0.858121\n",
      "Train Epoch: 83\tLoss: 0.855598\n",
      "Train Epoch: 84\tLoss: 0.844408\n",
      "Train Epoch: 85\tLoss: 0.842207\n",
      "Train Epoch: 86\tLoss: 0.837772\n",
      "Train Epoch: 87\tLoss: 0.834084\n",
      "Train Epoch: 88\tLoss: 0.826247\n",
      "Train Epoch: 89\tLoss: 0.826526\n",
      "Train Epoch: 90\tLoss: 0.824759\n",
      "Train Epoch: 91\tLoss: 0.822943\n",
      "Train Epoch: 92\tLoss: 0.828276\n",
      "Train Epoch: 93\tLoss: 0.825317\n",
      "Train Epoch: 94\tLoss: 0.824062\n",
      "Train Epoch: 95\tLoss: 0.824411\n",
      "Train Epoch: 96\tLoss: 0.819833\n",
      "Train Epoch: 97\tLoss: 0.822187\n",
      "Train Epoch: 98\tLoss: 0.815908\n",
      "Train Epoch: 99\tLoss: 0.810462\n",
      "Train Epoch: 100\tLoss: 0.815358\n",
      "Train Epoch: 101\tLoss: 0.808224\n",
      "Train Epoch: 102\tLoss: 0.809965\n",
      "Train Epoch: 103\tLoss: 0.811235\n",
      "Train Epoch: 104\tLoss: 0.809667\n",
      "Train Epoch: 105\tLoss: 0.803335\n",
      "Train Epoch: 106\tLoss: 0.806304\n",
      "Train Epoch: 107\tLoss: 0.805116\n",
      "Train Epoch: 108\tLoss: 0.800115\n",
      "Train Epoch: 109\tLoss: 0.801273\n",
      "Train Epoch: 110\tLoss: 0.797455\n",
      "Train Epoch: 111\tLoss: 0.798828\n",
      "Train Epoch: 112\tLoss: 0.796857\n",
      "Train Epoch: 113\tLoss: 0.797843\n",
      "Train Epoch: 114\tLoss: 0.793678\n",
      "Train Epoch: 115\tLoss: 0.787343\n",
      "Train Epoch: 116\tLoss: 0.783922\n",
      "Train Epoch: 117\tLoss: 0.779740\n",
      "Train Epoch: 118\tLoss: 0.784334\n",
      "Train Epoch: 119\tLoss: 0.778183\n",
      "Train Epoch: 120\tLoss: 0.781431\n",
      "Train Epoch: 121\tLoss: 0.781044\n",
      "Train Epoch: 122\tLoss: 0.776861\n",
      "Train Epoch: 123\tLoss: 0.769372\n",
      "Train Epoch: 124\tLoss: 0.770047\n",
      "Train Epoch: 125\tLoss: 0.767102\n",
      "Train Epoch: 126\tLoss: 0.774286\n",
      "Train Epoch: 127\tLoss: 0.762955\n",
      "Train Epoch: 128\tLoss: 0.763374\n",
      "Train Epoch: 129\tLoss: 0.761967\n",
      "Train Epoch: 130\tLoss: 0.764488\n",
      "Train Epoch: 131\tLoss: 0.765307\n",
      "Train Epoch: 132\tLoss: 0.757534\n",
      "Train Epoch: 133\tLoss: 0.756744\n",
      "Train Epoch: 134\tLoss: 0.753673\n",
      "Train Epoch: 135\tLoss: 0.744435\n",
      "Train Epoch: 136\tLoss: 0.739791\n",
      "Train Epoch: 137\tLoss: 0.746938\n",
      "Train Epoch: 138\tLoss: 0.748225\n",
      "Train Epoch: 139\tLoss: 0.752542\n",
      "Train Epoch: 140\tLoss: 0.745379\n",
      "Train Epoch: 141\tLoss: 0.744992\n",
      "Train Epoch: 142\tLoss: 0.743238\n",
      "Train Epoch: 143\tLoss: 0.736904\n",
      "Train Epoch: 144\tLoss: 0.727063\n",
      "Train Epoch: 145\tLoss: 0.731160\n",
      "Train Epoch: 146\tLoss: 0.727289\n",
      "Train Epoch: 147\tLoss: 0.730933\n",
      "Train Epoch: 148\tLoss: 0.725642\n",
      "Train Epoch: 149\tLoss: 0.728891\n",
      "Train Epoch: 150\tLoss: 0.724439\n",
      "Train Epoch: 151\tLoss: 0.730707\n",
      "Train Epoch: 152\tLoss: 0.723265\n",
      "Train Epoch: 153\tLoss: 0.721199\n",
      "Train Epoch: 154\tLoss: 0.720096\n",
      "Train Epoch: 155\tLoss: 0.716889\n",
      "Train Epoch: 156\tLoss: 0.720684\n",
      "Train Epoch: 157\tLoss: 0.713854\n",
      "Train Epoch: 158\tLoss: 0.715518\n",
      "Train Epoch: 159\tLoss: 0.717337\n",
      "Train Epoch: 160\tLoss: 0.723150\n",
      "Train Epoch: 161\tLoss: 0.717675\n",
      "Train Epoch: 162\tLoss: 0.715461\n",
      "Train Epoch: 163\tLoss: 0.714677\n",
      "Train Epoch: 164\tLoss: 0.709657\n",
      "Train Epoch: 165\tLoss: 0.706326\n",
      "Train Epoch: 166\tLoss: 0.703180\n",
      "Train Epoch: 167\tLoss: 0.704333\n",
      "Train Epoch: 168\tLoss: 0.703458\n",
      "Train Epoch: 169\tLoss: 0.704358\n",
      "Train Epoch: 170\tLoss: 0.703283\n",
      "Train Epoch: 171\tLoss: 0.697991\n",
      "Train Epoch: 172\tLoss: 0.694590\n",
      "Train Epoch: 173\tLoss: 0.695566\n",
      "Train Epoch: 174\tLoss: 0.692885\n",
      "Train Epoch: 175\tLoss: 0.696147\n",
      "Train Epoch: 176\tLoss: 0.690756\n",
      "Train Epoch: 177\tLoss: 0.696892\n",
      "Train Epoch: 178\tLoss: 0.684208\n",
      "Train Epoch: 179\tLoss: 0.684783\n",
      "Train Epoch: 180\tLoss: 0.684976\n",
      "Train Epoch: 181\tLoss: 0.675389\n",
      "Train Epoch: 182\tLoss: 0.671636\n",
      "Train Epoch: 183\tLoss: 0.677465\n",
      "Train Epoch: 184\tLoss: 0.670447\n",
      "Train Epoch: 185\tLoss: 0.670408\n",
      "Train Epoch: 186\tLoss: 0.669397\n",
      "Train Epoch: 187\tLoss: 0.664153\n",
      "Train Epoch: 188\tLoss: 0.667285\n",
      "Train Epoch: 189\tLoss: 0.661795\n",
      "Train Epoch: 190\tLoss: 0.665233\n",
      "Train Epoch: 191\tLoss: 0.667605\n",
      "Train Epoch: 192\tLoss: 0.661249\n",
      "Train Epoch: 193\tLoss: 0.657490\n",
      "Train Epoch: 194\tLoss: 0.657692\n",
      "Train Epoch: 195\tLoss: 0.658382\n",
      "Train Epoch: 196\tLoss: 0.646885\n",
      "Train Epoch: 197\tLoss: 0.650940\n",
      "Train Epoch: 198\tLoss: 0.647958\n",
      "Train Epoch: 199\tLoss: 0.646683\n",
      "Train Epoch: 200\tLoss: 0.645316\n",
      "Train Epoch: 201\tLoss: 0.643244\n",
      "Train Epoch: 202\tLoss: 0.636207\n",
      "Train Epoch: 203\tLoss: 0.643520\n",
      "Train Epoch: 204\tLoss: 0.641330\n",
      "Train Epoch: 205\tLoss: 0.641521\n",
      "Train Epoch: 206\tLoss: 0.642450\n",
      "Train Epoch: 207\tLoss: 0.648476\n",
      "Train Epoch: 208\tLoss: 0.640339\n",
      "Train Epoch: 209\tLoss: 0.634298\n",
      "Train Epoch: 210\tLoss: 0.636478\n",
      "Train Epoch: 211\tLoss: 0.637587\n",
      "Train Epoch: 212\tLoss: 0.633442\n",
      "Train Epoch: 213\tLoss: 0.635134\n",
      "Train Epoch: 214\tLoss: 0.631636\n",
      "Train Epoch: 215\tLoss: 0.625511\n",
      "Train Epoch: 216\tLoss: 0.625688\n",
      "Train Epoch: 217\tLoss: 0.629909\n",
      "Train Epoch: 218\tLoss: 0.630192\n",
      "Train Epoch: 219\tLoss: 0.627262\n",
      "Train Epoch: 220\tLoss: 0.627325\n",
      "Train Epoch: 221\tLoss: 0.625356\n",
      "Train Epoch: 222\tLoss: 0.622696\n",
      "Train Epoch: 223\tLoss: 0.622758\n",
      "Train Epoch: 224\tLoss: 0.622191\n",
      "Train Epoch: 225\tLoss: 0.620184\n",
      "Train Epoch: 226\tLoss: 0.623680\n",
      "Train Epoch: 227\tLoss: 0.615997\n",
      "Train Epoch: 228\tLoss: 0.620346\n",
      "Train Epoch: 229\tLoss: 0.616173\n",
      "Train Epoch: 230\tLoss: 0.612333\n",
      "Train Epoch: 231\tLoss: 0.613711\n",
      "Train Epoch: 232\tLoss: 0.610393\n",
      "Train Epoch: 233\tLoss: 0.612078\n",
      "Train Epoch: 234\tLoss: 0.604991\n",
      "Train Epoch: 235\tLoss: 0.604303\n",
      "Train Epoch: 236\tLoss: 0.604884\n",
      "Train Epoch: 237\tLoss: 0.603404\n",
      "Train Epoch: 238\tLoss: 0.604195\n",
      "Train Epoch: 239\tLoss: 0.596975\n",
      "Train Epoch: 240\tLoss: 0.596233\n",
      "Train Epoch: 241\tLoss: 0.595812\n",
      "Train Epoch: 242\tLoss: 0.598076\n",
      "Train Epoch: 243\tLoss: 0.598179\n",
      "Train Epoch: 244\tLoss: 0.597806\n",
      "Train Epoch: 245\tLoss: 0.599152\n",
      "Train Epoch: 246\tLoss: 0.595426\n",
      "Train Epoch: 247\tLoss: 0.593820\n",
      "Train Epoch: 248\tLoss: 0.598175\n",
      "Train Epoch: 249\tLoss: 0.587514\n",
      "Train Epoch: 250\tLoss: 0.589687\n",
      "Train Epoch: 251\tLoss: 0.589361\n",
      "Train Epoch: 252\tLoss: 0.586114\n",
      "Train Epoch: 253\tLoss: 0.592102\n",
      "Train Epoch: 254\tLoss: 0.586236\n",
      "Train Epoch: 255\tLoss: 0.589108\n",
      "Train Epoch: 256\tLoss: 0.582156\n",
      "Train Epoch: 257\tLoss: 0.588188\n",
      "Train Epoch: 258\tLoss: 0.590501\n",
      "Train Epoch: 259\tLoss: 0.582681\n",
      "Train Epoch: 260\tLoss: 0.588450\n",
      "Train Epoch: 261\tLoss: 0.581495\n",
      "Train Epoch: 262\tLoss: 0.576494\n",
      "Train Epoch: 263\tLoss: 0.578588\n",
      "Train Epoch: 264\tLoss: 0.580096\n",
      "Train Epoch: 265\tLoss: 0.583459\n",
      "Train Epoch: 266\tLoss: 0.581564\n",
      "Train Epoch: 267\tLoss: 0.575112\n",
      "Train Epoch: 268\tLoss: 0.570838\n",
      "Train Epoch: 269\tLoss: 0.578696\n",
      "Train Epoch: 270\tLoss: 0.573147\n",
      "Train Epoch: 271\tLoss: 0.569395\n",
      "Train Epoch: 272\tLoss: 0.570067\n",
      "Train Epoch: 273\tLoss: 0.575525\n",
      "Train Epoch: 274\tLoss: 0.573418\n",
      "Train Epoch: 275\tLoss: 0.573648\n",
      "Train Epoch: 276\tLoss: 0.577729\n",
      "Train Epoch: 277\tLoss: 0.570115\n",
      "Train Epoch: 278\tLoss: 0.565427\n",
      "Train Epoch: 279\tLoss: 0.561764\n",
      "Train Epoch: 280\tLoss: 0.565913\n",
      "Train Epoch: 281\tLoss: 0.568961\n",
      "Train Epoch: 282\tLoss: 0.570122\n",
      "Train Epoch: 283\tLoss: 0.566867\n",
      "Train Epoch: 284\tLoss: 0.565245\n",
      "Train Epoch: 285\tLoss: 0.565590\n",
      "Train Epoch: 286\tLoss: 0.562326\n",
      "Train Epoch: 287\tLoss: 0.556193\n",
      "Train Epoch: 288\tLoss: 0.569749\n",
      "Train Epoch: 289\tLoss: 0.567582\n",
      "Train Epoch: 290\tLoss: 0.559912\n",
      "Train Epoch: 291\tLoss: 0.559376\n",
      "Train Epoch: 292\tLoss: 0.554046\n",
      "Train Epoch: 293\tLoss: 0.559711\n",
      "Train Epoch: 294\tLoss: 0.562303\n",
      "Train Epoch: 295\tLoss: 0.558128\n",
      "Train Epoch: 296\tLoss: 0.556525\n",
      "Train Epoch: 297\tLoss: 0.559352\n",
      "Train Epoch: 298\tLoss: 0.557197\n",
      "Train Epoch: 299\tLoss: 0.558460\n",
      "Train Epoch: 300\tLoss: 0.561116\n",
      "Train Epoch: 301\tLoss: 0.563029\n",
      "Train Epoch: 302\tLoss: 0.553711\n",
      "Train Epoch: 303\tLoss: 0.563839\n",
      "Train Epoch: 304\tLoss: 0.557974\n",
      "Train Epoch: 305\tLoss: 0.558354\n",
      "Train Epoch: 306\tLoss: 0.545989\n",
      "Train Epoch: 307\tLoss: 0.555013\n",
      "Train Epoch: 308\tLoss: 0.548851\n",
      "Train Epoch: 309\tLoss: 0.548093\n",
      "Train Epoch: 310\tLoss: 0.550338\n",
      "Train Epoch: 311\tLoss: 0.551321\n",
      "Train Epoch: 312\tLoss: 0.556682\n",
      "Train Epoch: 313\tLoss: 0.558598\n",
      "Train Epoch: 314\tLoss: 0.546243\n",
      "Train Epoch: 315\tLoss: 0.544927\n",
      "Train Epoch: 316\tLoss: 0.550151\n",
      "Train Epoch: 317\tLoss: 0.548091\n",
      "Train Epoch: 318\tLoss: 0.548143\n",
      "Train Epoch: 319\tLoss: 0.549001\n",
      "Train Epoch: 320\tLoss: 0.547936\n",
      "Train Epoch: 321\tLoss: 0.550416\n",
      "Train Epoch: 322\tLoss: 0.547387\n",
      "Train Epoch: 323\tLoss: 0.549916\n",
      "Train Epoch: 324\tLoss: 0.546808\n",
      "Train Epoch: 325\tLoss: 0.549300\n",
      "Train Epoch: 326\tLoss: 0.540806\n",
      "Train Epoch: 327\tLoss: 0.546903\n",
      "Train Epoch: 328\tLoss: 0.543018\n",
      "Train Epoch: 329\tLoss: 0.544470\n",
      "Train Epoch: 330\tLoss: 0.544480\n",
      "Train Epoch: 331\tLoss: 0.544420\n",
      "Train Epoch: 332\tLoss: 0.537182\n",
      "Train Epoch: 333\tLoss: 0.538242\n",
      "Train Epoch: 334\tLoss: 0.538956\n",
      "Train Epoch: 335\tLoss: 0.546550\n",
      "Train Epoch: 336\tLoss: 0.536945\n",
      "Train Epoch: 337\tLoss: 0.538756\n",
      "Train Epoch: 338\tLoss: 0.534299\n",
      "Train Epoch: 339\tLoss: 0.531329\n",
      "Train Epoch: 340\tLoss: 0.540795\n",
      "Train Epoch: 341\tLoss: 0.534385\n",
      "Train Epoch: 342\tLoss: 0.537659\n",
      "Train Epoch: 343\tLoss: 0.537055\n",
      "Train Epoch: 344\tLoss: 0.530256\n",
      "Train Epoch: 345\tLoss: 0.534831\n",
      "Train Epoch: 346\tLoss: 0.535633\n",
      "Train Epoch: 347\tLoss: 0.531408\n",
      "Train Epoch: 348\tLoss: 0.535905\n",
      "Train Epoch: 349\tLoss: 0.532559\n",
      "Train Epoch: 350\tLoss: 0.528838\n",
      "Train Epoch: 351\tLoss: 0.529933\n",
      "Train Epoch: 352\tLoss: 0.531157\n",
      "Train Epoch: 353\tLoss: 0.535799\n",
      "Train Epoch: 354\tLoss: 0.531418\n",
      "Train Epoch: 355\tLoss: 0.525848\n",
      "Train Epoch: 356\tLoss: 0.531700\n",
      "Train Epoch: 357\tLoss: 0.524507\n",
      "Train Epoch: 358\tLoss: 0.524054\n",
      "Train Epoch: 359\tLoss: 0.522997\n",
      "Train Epoch: 360\tLoss: 0.529733\n",
      "Train Epoch: 361\tLoss: 0.534374\n",
      "Train Epoch: 362\tLoss: 0.531323\n",
      "Train Epoch: 363\tLoss: 0.526152\n",
      "Train Epoch: 364\tLoss: 0.532368\n",
      "Train Epoch: 365\tLoss: 0.530954\n",
      "Train Epoch: 366\tLoss: 0.530996\n",
      "Train Epoch: 367\tLoss: 0.525404\n",
      "Train Epoch: 368\tLoss: 0.527550\n",
      "Train Epoch: 369\tLoss: 0.525478\n",
      "Train Epoch: 370\tLoss: 0.523736\n",
      "Train Epoch: 371\tLoss: 0.524534\n",
      "Train Epoch: 372\tLoss: 0.525332\n",
      "Train Epoch: 373\tLoss: 0.519919\n",
      "Train Epoch: 374\tLoss: 0.516714\n",
      "Train Epoch: 375\tLoss: 0.522401\n",
      "Train Epoch: 376\tLoss: 0.523580\n",
      "Train Epoch: 377\tLoss: 0.525687\n",
      "Train Epoch: 378\tLoss: 0.520721\n",
      "Train Epoch: 379\tLoss: 0.521618\n",
      "Train Epoch: 380\tLoss: 0.521580\n",
      "Train Epoch: 381\tLoss: 0.529755\n",
      "Train Epoch: 382\tLoss: 0.519462\n",
      "Train Epoch: 383\tLoss: 0.524439\n",
      "Train Epoch: 384\tLoss: 0.521275\n",
      "Train Epoch: 385\tLoss: 0.518699\n",
      "Train Epoch: 386\tLoss: 0.515323\n",
      "Train Epoch: 387\tLoss: 0.515421\n",
      "Train Epoch: 388\tLoss: 0.521208\n",
      "Train Epoch: 389\tLoss: 0.523437\n",
      "Train Epoch: 390\tLoss: 0.512090\n",
      "Train Epoch: 391\tLoss: 0.519325\n",
      "Train Epoch: 392\tLoss: 0.517292\n",
      "Train Epoch: 393\tLoss: 0.519231\n",
      "Train Epoch: 394\tLoss: 0.520787\n",
      "Train Epoch: 395\tLoss: 0.521663\n",
      "Train Epoch: 396\tLoss: 0.518034\n",
      "Train Epoch: 397\tLoss: 0.516696\n",
      "Train Epoch: 398\tLoss: 0.514194\n",
      "Train Epoch: 399\tLoss: 0.516248\n",
      "Train Epoch: 400\tLoss: 0.515529\n",
      "Train Epoch: 401\tLoss: 0.519060\n",
      "Train Epoch: 402\tLoss: 0.517578\n",
      "Train Epoch: 403\tLoss: 0.511539\n",
      "Train Epoch: 404\tLoss: 0.513582\n",
      "Train Epoch: 405\tLoss: 0.515929\n",
      "Train Epoch: 406\tLoss: 0.518704\n",
      "Train Epoch: 407\tLoss: 0.515926\n",
      "Train Epoch: 408\tLoss: 0.514262\n",
      "Train Epoch: 409\tLoss: 0.516085\n",
      "Train Epoch: 410\tLoss: 0.509564\n",
      "Train Epoch: 411\tLoss: 0.515130\n",
      "Train Epoch: 412\tLoss: 0.516578\n",
      "Train Epoch: 413\tLoss: 0.508278\n",
      "Train Epoch: 414\tLoss: 0.512658\n",
      "Train Epoch: 415\tLoss: 0.515707\n",
      "Train Epoch: 416\tLoss: 0.503142\n",
      "Train Epoch: 417\tLoss: 0.511164\n",
      "Train Epoch: 418\tLoss: 0.511460\n",
      "Train Epoch: 419\tLoss: 0.508774\n",
      "Train Epoch: 420\tLoss: 0.511281\n",
      "Train Epoch: 421\tLoss: 0.507821\n",
      "Train Epoch: 422\tLoss: 0.511874\n",
      "Train Epoch: 423\tLoss: 0.505816\n",
      "Train Epoch: 424\tLoss: 0.503291\n",
      "Train Epoch: 425\tLoss: 0.511617\n",
      "Train Epoch: 426\tLoss: 0.509960\n",
      "Train Epoch: 427\tLoss: 0.513369\n",
      "Train Epoch: 428\tLoss: 0.515306\n",
      "Train Epoch: 429\tLoss: 0.511572\n",
      "Train Epoch: 430\tLoss: 0.505937\n",
      "Train Epoch: 431\tLoss: 0.504277\n",
      "Train Epoch: 432\tLoss: 0.508128\n",
      "Train Epoch: 433\tLoss: 0.510313\n",
      "Train Epoch: 434\tLoss: 0.503244\n",
      "Train Epoch: 435\tLoss: 0.508323\n",
      "Train Epoch: 436\tLoss: 0.512192\n",
      "Train Epoch: 437\tLoss: 0.507197\n",
      "Train Epoch: 438\tLoss: 0.507397\n",
      "Train Epoch: 439\tLoss: 0.509828\n",
      "Train Epoch: 440\tLoss: 0.511939\n",
      "Train Epoch: 441\tLoss: 0.506444\n",
      "Train Epoch: 442\tLoss: 0.507428\n",
      "Train Epoch: 443\tLoss: 0.503864\n",
      "Train Epoch: 444\tLoss: 0.507583\n",
      "Train Epoch: 445\tLoss: 0.503806\n",
      "Train Epoch: 446\tLoss: 0.504044\n",
      "Train Epoch: 447\tLoss: 0.499700\n",
      "Train Epoch: 448\tLoss: 0.504456\n",
      "Train Epoch: 449\tLoss: 0.496580\n",
      "Train Epoch: 450\tLoss: 0.499951\n",
      "Train Epoch: 451\tLoss: 0.507564\n",
      "Train Epoch: 452\tLoss: 0.504412\n",
      "Train Epoch: 453\tLoss: 0.500489\n",
      "Train Epoch: 454\tLoss: 0.504350\n",
      "Train Epoch: 455\tLoss: 0.504120\n",
      "Train Epoch: 456\tLoss: 0.500465\n",
      "Train Epoch: 457\tLoss: 0.503979\n",
      "Train Epoch: 458\tLoss: 0.506521\n",
      "Train Epoch: 459\tLoss: 0.504953\n",
      "Train Epoch: 460\tLoss: 0.505993\n",
      "Train Epoch: 461\tLoss: 0.507177\n",
      "Train Epoch: 462\tLoss: 0.508193\n",
      "Train Epoch: 463\tLoss: 0.495241\n",
      "Train Epoch: 464\tLoss: 0.500197\n",
      "Train Epoch: 465\tLoss: 0.502394\n",
      "Train Epoch: 466\tLoss: 0.501065\n",
      "Train Epoch: 467\tLoss: 0.495482\n",
      "Train Epoch: 468\tLoss: 0.499061\n",
      "Train Epoch: 469\tLoss: 0.502698\n",
      "Train Epoch: 470\tLoss: 0.499842\n",
      "Train Epoch: 471\tLoss: 0.501827\n",
      "Train Epoch: 472\tLoss: 0.505647\n",
      "Train Epoch: 473\tLoss: 0.500156\n",
      "Train Epoch: 474\tLoss: 0.495858\n",
      "Train Epoch: 475\tLoss: 0.504243\n",
      "Train Epoch: 476\tLoss: 0.500656\n",
      "Train Epoch: 477\tLoss: 0.499489\n",
      "Train Epoch: 478\tLoss: 0.497114\n",
      "Train Epoch: 479\tLoss: 0.498650\n",
      "Train Epoch: 480\tLoss: 0.503230\n",
      "Train Epoch: 481\tLoss: 0.497379\n",
      "Train Epoch: 482\tLoss: 0.498175\n",
      "Train Epoch: 483\tLoss: 0.500980\n",
      "Train Epoch: 484\tLoss: 0.493372\n",
      "Train Epoch: 485\tLoss: 0.494539\n",
      "Train Epoch: 486\tLoss: 0.494013\n",
      "Train Epoch: 487\tLoss: 0.498979\n",
      "Train Epoch: 488\tLoss: 0.497148\n",
      "Train Epoch: 489\tLoss: 0.498588\n",
      "Train Epoch: 490\tLoss: 0.496709\n",
      "Train Epoch: 491\tLoss: 0.493908\n",
      "Train Epoch: 492\tLoss: 0.494774\n",
      "Train Epoch: 493\tLoss: 0.494905\n",
      "Train Epoch: 494\tLoss: 0.496558\n",
      "Train Epoch: 495\tLoss: 0.496165\n",
      "Train Epoch: 496\tLoss: 0.495882\n",
      "Train Epoch: 497\tLoss: 0.497802\n",
      "Train Epoch: 498\tLoss: 0.494760\n",
      "Train Epoch: 499\tLoss: 0.501579\n",
      "Train Epoch: 500\tLoss: 0.490316\n",
      "Train Epoch: 501\tLoss: 0.490903\n",
      "Train Epoch: 502\tLoss: 0.492479\n",
      "Train Epoch: 503\tLoss: 0.492903\n",
      "Train Epoch: 504\tLoss: 0.491307\n",
      "Train Epoch: 505\tLoss: 0.487197\n",
      "Train Epoch: 506\tLoss: 0.491956\n",
      "Train Epoch: 507\tLoss: 0.492604\n",
      "Train Epoch: 508\tLoss: 0.491443\n",
      "Train Epoch: 509\tLoss: 0.497637\n",
      "Train Epoch: 510\tLoss: 0.490801\n",
      "Train Epoch: 511\tLoss: 0.493462\n",
      "Train Epoch: 512\tLoss: 0.490782\n",
      "Train Epoch: 513\tLoss: 0.492902\n",
      "Train Epoch: 514\tLoss: 0.487172\n",
      "Train Epoch: 515\tLoss: 0.494758\n",
      "Train Epoch: 516\tLoss: 0.491938\n",
      "Train Epoch: 517\tLoss: 0.490870\n",
      "Train Epoch: 518\tLoss: 0.490739\n",
      "Train Epoch: 519\tLoss: 0.499001\n",
      "Train Epoch: 520\tLoss: 0.494734\n",
      "Train Epoch: 521\tLoss: 0.488158\n",
      "Train Epoch: 522\tLoss: 0.493870\n",
      "Train Epoch: 523\tLoss: 0.490303\n",
      "Train Epoch: 524\tLoss: 0.496110\n",
      "Train Epoch: 525\tLoss: 0.497999\n",
      "Train Epoch: 526\tLoss: 0.494103\n",
      "Train Epoch: 527\tLoss: 0.493264\n",
      "Train Epoch: 528\tLoss: 0.491148\n",
      "Train Epoch: 529\tLoss: 0.494770\n",
      "Train Epoch: 530\tLoss: 0.488898\n",
      "Train Epoch: 531\tLoss: 0.490919\n",
      "Train Epoch: 532\tLoss: 0.492952\n",
      "Train Epoch: 533\tLoss: 0.490700\n",
      "Train Epoch: 534\tLoss: 0.490199\n",
      "Train Epoch: 535\tLoss: 0.484638\n",
      "Train Epoch: 536\tLoss: 0.493154\n",
      "Train Epoch: 537\tLoss: 0.492545\n",
      "Train Epoch: 538\tLoss: 0.489806\n",
      "Train Epoch: 539\tLoss: 0.488617\n",
      "Train Epoch: 540\tLoss: 0.488309\n",
      "Train Epoch: 541\tLoss: 0.492079\n",
      "Train Epoch: 542\tLoss: 0.488762\n",
      "Train Epoch: 543\tLoss: 0.492231\n",
      "Train Epoch: 544\tLoss: 0.481551\n",
      "Train Epoch: 545\tLoss: 0.486184\n",
      "Train Epoch: 546\tLoss: 0.490782\n",
      "Train Epoch: 547\tLoss: 0.492877\n",
      "Train Epoch: 548\tLoss: 0.493098\n",
      "Train Epoch: 549\tLoss: 0.486414\n",
      "Train Epoch: 550\tLoss: 0.489565\n",
      "Train Epoch: 551\tLoss: 0.489461\n",
      "Train Epoch: 552\tLoss: 0.490483\n",
      "Train Epoch: 553\tLoss: 0.488283\n",
      "Train Epoch: 554\tLoss: 0.484891\n",
      "Train Epoch: 555\tLoss: 0.489092\n",
      "Train Epoch: 556\tLoss: 0.489237\n",
      "Train Epoch: 557\tLoss: 0.491489\n",
      "Train Epoch: 558\tLoss: 0.494059\n",
      "Train Epoch: 559\tLoss: 0.481838\n",
      "Train Epoch: 560\tLoss: 0.482737\n",
      "Train Epoch: 561\tLoss: 0.484678\n",
      "Train Epoch: 562\tLoss: 0.491703\n",
      "Train Epoch: 563\tLoss: 0.485975\n",
      "Train Epoch: 564\tLoss: 0.492091\n",
      "Train Epoch: 565\tLoss: 0.489542\n",
      "Train Epoch: 566\tLoss: 0.486131\n",
      "Train Epoch: 567\tLoss: 0.481544\n",
      "Train Epoch: 568\tLoss: 0.489308\n",
      "Train Epoch: 569\tLoss: 0.482597\n",
      "Train Epoch: 570\tLoss: 0.490403\n",
      "Train Epoch: 571\tLoss: 0.486860\n",
      "Train Epoch: 572\tLoss: 0.491175\n",
      "Train Epoch: 573\tLoss: 0.487351\n",
      "Train Epoch: 574\tLoss: 0.487273\n",
      "Train Epoch: 575\tLoss: 0.487171\n",
      "Train Epoch: 576\tLoss: 0.481415\n",
      "Train Epoch: 577\tLoss: 0.488819\n",
      "Train Epoch: 578\tLoss: 0.491623\n",
      "Train Epoch: 579\tLoss: 0.489826\n",
      "Train Epoch: 580\tLoss: 0.487513\n",
      "Train Epoch: 581\tLoss: 0.487400\n",
      "Train Epoch: 582\tLoss: 0.482200\n",
      "Train Epoch: 583\tLoss: 0.486680\n",
      "Train Epoch: 584\tLoss: 0.492240\n",
      "Train Epoch: 585\tLoss: 0.485757\n",
      "Train Epoch: 586\tLoss: 0.489058\n",
      "Train Epoch: 587\tLoss: 0.486088\n",
      "Train Epoch: 588\tLoss: 0.490108\n",
      "Train Epoch: 589\tLoss: 0.486069\n",
      "Train Epoch: 590\tLoss: 0.490099\n",
      "Train Epoch: 591\tLoss: 0.479387\n",
      "Train Epoch: 592\tLoss: 0.483795\n",
      "Train Epoch: 593\tLoss: 0.484172\n",
      "Train Epoch: 594\tLoss: 0.482546\n",
      "Train Epoch: 595\tLoss: 0.483581\n",
      "Train Epoch: 596\tLoss: 0.480187\n",
      "Train Epoch: 597\tLoss: 0.493314\n",
      "Train Epoch: 598\tLoss: 0.489345\n",
      "Train Epoch: 599\tLoss: 0.491352\n",
      "Train Epoch: 600\tLoss: 0.480188\n",
      "Train Epoch: 601\tLoss: 0.488519\n",
      "Train Epoch: 602\tLoss: 0.482946\n",
      "Train Epoch: 603\tLoss: 0.484404\n",
      "Train Epoch: 604\tLoss: 0.479414\n",
      "Train Epoch: 605\tLoss: 0.479937\n",
      "Train Epoch: 606\tLoss: 0.481935\n",
      "Train Epoch: 607\tLoss: 0.488431\n",
      "Train Epoch: 608\tLoss: 0.481619\n",
      "Train Epoch: 609\tLoss: 0.480350\n",
      "Train Epoch: 610\tLoss: 0.483000\n",
      "Train Epoch: 611\tLoss: 0.481988\n",
      "Train Epoch: 612\tLoss: 0.480663\n",
      "Train Epoch: 613\tLoss: 0.478092\n",
      "Train Epoch: 614\tLoss: 0.482712\n",
      "Train Epoch: 615\tLoss: 0.486717\n",
      "Train Epoch: 616\tLoss: 0.486519\n",
      "Train Epoch: 617\tLoss: 0.482119\n",
      "Train Epoch: 618\tLoss: 0.478761\n",
      "Train Epoch: 619\tLoss: 0.479085\n",
      "Train Epoch: 620\tLoss: 0.490474\n",
      "Train Epoch: 621\tLoss: 0.480938\n",
      "Train Epoch: 622\tLoss: 0.481834\n",
      "Train Epoch: 623\tLoss: 0.475140\n",
      "Train Epoch: 624\tLoss: 0.483331\n",
      "Train Epoch: 625\tLoss: 0.481403\n",
      "Train Epoch: 626\tLoss: 0.477720\n",
      "Train Epoch: 627\tLoss: 0.484225\n",
      "Train Epoch: 628\tLoss: 0.485376\n",
      "Train Epoch: 629\tLoss: 0.483356\n",
      "Train Epoch: 630\tLoss: 0.483420\n",
      "Train Epoch: 631\tLoss: 0.480716\n",
      "Train Epoch: 632\tLoss: 0.478187\n",
      "Train Epoch: 633\tLoss: 0.479823\n",
      "Train Epoch: 634\tLoss: 0.474752\n",
      "Train Epoch: 635\tLoss: 0.486114\n",
      "Train Epoch: 636\tLoss: 0.487828\n",
      "Train Epoch: 637\tLoss: 0.484709\n",
      "Train Epoch: 638\tLoss: 0.483461\n",
      "Train Epoch: 639\tLoss: 0.482377\n",
      "Train Epoch: 640\tLoss: 0.480525\n",
      "Train Epoch: 641\tLoss: 0.481370\n",
      "Train Epoch: 642\tLoss: 0.478109\n",
      "Train Epoch: 643\tLoss: 0.473708\n",
      "Train Epoch: 644\tLoss: 0.481709\n",
      "Train Epoch: 645\tLoss: 0.481799\n",
      "Train Epoch: 646\tLoss: 0.476592\n",
      "Train Epoch: 647\tLoss: 0.476498\n",
      "Train Epoch: 648\tLoss: 0.477089\n",
      "Train Epoch: 649\tLoss: 0.477427\n",
      "Train Epoch: 650\tLoss: 0.478965\n",
      "Train Epoch: 651\tLoss: 0.480574\n",
      "Train Epoch: 652\tLoss: 0.475069\n",
      "Train Epoch: 653\tLoss: 0.487565\n",
      "Train Epoch: 654\tLoss: 0.484467\n",
      "Train Epoch: 655\tLoss: 0.477661\n",
      "Train Epoch: 656\tLoss: 0.478215\n",
      "Train Epoch: 657\tLoss: 0.481099\n",
      "Train Epoch: 658\tLoss: 0.476102\n",
      "Train Epoch: 659\tLoss: 0.478757\n",
      "Train Epoch: 660\tLoss: 0.477661\n",
      "Train Epoch: 661\tLoss: 0.475645\n",
      "Train Epoch: 662\tLoss: 0.478248\n",
      "Train Epoch: 663\tLoss: 0.475949\n",
      "Train Epoch: 664\tLoss: 0.478906\n",
      "Train Epoch: 665\tLoss: 0.478694\n",
      "Train Epoch: 666\tLoss: 0.483354\n",
      "Train Epoch: 667\tLoss: 0.480430\n",
      "Train Epoch: 668\tLoss: 0.470258\n",
      "Train Epoch: 669\tLoss: 0.476789\n",
      "Train Epoch: 670\tLoss: 0.478546\n",
      "Train Epoch: 671\tLoss: 0.475524\n",
      "Train Epoch: 672\tLoss: 0.477037\n",
      "Train Epoch: 673\tLoss: 0.475329\n",
      "Train Epoch: 674\tLoss: 0.475616\n",
      "Train Epoch: 675\tLoss: 0.474999\n",
      "Train Epoch: 676\tLoss: 0.479578\n",
      "Train Epoch: 677\tLoss: 0.479772\n",
      "Train Epoch: 678\tLoss: 0.473494\n",
      "Train Epoch: 679\tLoss: 0.476269\n",
      "Train Epoch: 680\tLoss: 0.480355\n",
      "Train Epoch: 681\tLoss: 0.475554\n",
      "Train Epoch: 682\tLoss: 0.476759\n",
      "Train Epoch: 683\tLoss: 0.478620\n",
      "Train Epoch: 684\tLoss: 0.478109\n",
      "Train Epoch: 685\tLoss: 0.471041\n",
      "Train Epoch: 686\tLoss: 0.474513\n",
      "Train Epoch: 687\tLoss: 0.473388\n",
      "Train Epoch: 688\tLoss: 0.478255\n",
      "Train Epoch: 689\tLoss: 0.474607\n",
      "Train Epoch: 690\tLoss: 0.475968\n",
      "Train Epoch: 691\tLoss: 0.472448\n",
      "Train Epoch: 692\tLoss: 0.478276\n",
      "Train Epoch: 693\tLoss: 0.471776\n",
      "Train Epoch: 694\tLoss: 0.481642\n",
      "Train Epoch: 695\tLoss: 0.478707\n",
      "Train Epoch: 696\tLoss: 0.482788\n",
      "Train Epoch: 697\tLoss: 0.474285\n",
      "Train Epoch: 698\tLoss: 0.476510\n",
      "Train Epoch: 699\tLoss: 0.473522\n",
      "Train Epoch: 700\tLoss: 0.473290\n",
      "Train Epoch: 701\tLoss: 0.470083\n",
      "Train Epoch: 702\tLoss: 0.476503\n",
      "Train Epoch: 703\tLoss: 0.473442\n",
      "Train Epoch: 704\tLoss: 0.471779\n",
      "Train Epoch: 705\tLoss: 0.471804\n",
      "Train Epoch: 706\tLoss: 0.477095\n",
      "Train Epoch: 707\tLoss: 0.474970\n",
      "Train Epoch: 708\tLoss: 0.471631\n",
      "Train Epoch: 709\tLoss: 0.470501\n",
      "Train Epoch: 710\tLoss: 0.473276\n",
      "Train Epoch: 711\tLoss: 0.476878\n",
      "Train Epoch: 712\tLoss: 0.481926\n",
      "Train Epoch: 713\tLoss: 0.473489\n",
      "Train Epoch: 714\tLoss: 0.473550\n",
      "Train Epoch: 715\tLoss: 0.473160\n",
      "Train Epoch: 716\tLoss: 0.479118\n",
      "Train Epoch: 717\tLoss: 0.476296\n",
      "Train Epoch: 718\tLoss: 0.473032\n",
      "Train Epoch: 719\tLoss: 0.481336\n",
      "Train Epoch: 720\tLoss: 0.469424\n",
      "Train Epoch: 721\tLoss: 0.476732\n",
      "Train Epoch: 722\tLoss: 0.469546\n",
      "Train Epoch: 723\tLoss: 0.469901\n",
      "Train Epoch: 724\tLoss: 0.468705\n",
      "Train Epoch: 725\tLoss: 0.478051\n",
      "Train Epoch: 726\tLoss: 0.475495\n",
      "Train Epoch: 727\tLoss: 0.470911\n",
      "Train Epoch: 728\tLoss: 0.476747\n",
      "Train Epoch: 729\tLoss: 0.474580\n",
      "Train Epoch: 730\tLoss: 0.479805\n",
      "Train Epoch: 731\tLoss: 0.466885\n",
      "Train Epoch: 732\tLoss: 0.474775\n",
      "Train Epoch: 733\tLoss: 0.474383\n",
      "Train Epoch: 734\tLoss: 0.474025\n",
      "Train Epoch: 735\tLoss: 0.470720\n",
      "Train Epoch: 736\tLoss: 0.476174\n",
      "Train Epoch: 737\tLoss: 0.474579\n",
      "Train Epoch: 738\tLoss: 0.471017\n",
      "Train Epoch: 739\tLoss: 0.470938\n",
      "Train Epoch: 740\tLoss: 0.478044\n",
      "Train Epoch: 741\tLoss: 0.474724\n",
      "Train Epoch: 742\tLoss: 0.470112\n",
      "Train Epoch: 743\tLoss: 0.472917\n",
      "Train Epoch: 744\tLoss: 0.470674\n",
      "Train Epoch: 745\tLoss: 0.472579\n",
      "Train Epoch: 746\tLoss: 0.470798\n",
      "Train Epoch: 747\tLoss: 0.473899\n",
      "Train Epoch: 748\tLoss: 0.476306\n",
      "Train Epoch: 749\tLoss: 0.477989\n",
      "Train Epoch: 750\tLoss: 0.471163\n",
      "Train Epoch: 751\tLoss: 0.470552\n",
      "Train Epoch: 752\tLoss: 0.475126\n",
      "Train Epoch: 753\tLoss: 0.469777\n",
      "Train Epoch: 754\tLoss: 0.472987\n",
      "Train Epoch: 755\tLoss: 0.469659\n",
      "Train Epoch: 756\tLoss: 0.473183\n",
      "Train Epoch: 757\tLoss: 0.471293\n",
      "Train Epoch: 758\tLoss: 0.467700\n",
      "Train Epoch: 759\tLoss: 0.467641\n",
      "Train Epoch: 760\tLoss: 0.466622\n",
      "Train Epoch: 761\tLoss: 0.471454\n",
      "Train Epoch: 762\tLoss: 0.471558\n",
      "Train Epoch: 763\tLoss: 0.472389\n",
      "Train Epoch: 764\tLoss: 0.469239\n",
      "Train Epoch: 765\tLoss: 0.470977\n",
      "Train Epoch: 766\tLoss: 0.472574\n",
      "Train Epoch: 767\tLoss: 0.471693\n",
      "Train Epoch: 768\tLoss: 0.470918\n",
      "Train Epoch: 769\tLoss: 0.470322\n",
      "Train Epoch: 770\tLoss: 0.465680\n",
      "Train Epoch: 771\tLoss: 0.470620\n",
      "Train Epoch: 772\tLoss: 0.473187\n",
      "Train Epoch: 773\tLoss: 0.471802\n",
      "Train Epoch: 774\tLoss: 0.473303\n",
      "Train Epoch: 775\tLoss: 0.472697\n",
      "Train Epoch: 776\tLoss: 0.467099\n",
      "Train Epoch: 777\tLoss: 0.471407\n",
      "Train Epoch: 778\tLoss: 0.464953\n",
      "Train Epoch: 779\tLoss: 0.472924\n",
      "Train Epoch: 780\tLoss: 0.475358\n",
      "Train Epoch: 781\tLoss: 0.473366\n",
      "Train Epoch: 782\tLoss: 0.468777\n",
      "Train Epoch: 783\tLoss: 0.468168\n",
      "Train Epoch: 784\tLoss: 0.474387\n",
      "Train Epoch: 785\tLoss: 0.469236\n",
      "Train Epoch: 786\tLoss: 0.469866\n",
      "Train Epoch: 787\tLoss: 0.474340\n",
      "Train Epoch: 788\tLoss: 0.469405\n",
      "Train Epoch: 789\tLoss: 0.471449\n",
      "Train Epoch: 790\tLoss: 0.469295\n",
      "Train Epoch: 791\tLoss: 0.470165\n",
      "Train Epoch: 792\tLoss: 0.467257\n",
      "Train Epoch: 793\tLoss: 0.469775\n",
      "Train Epoch: 794\tLoss: 0.471711\n",
      "Train Epoch: 795\tLoss: 0.473535\n",
      "Train Epoch: 796\tLoss: 0.464166\n",
      "Train Epoch: 797\tLoss: 0.469666\n",
      "Train Epoch: 798\tLoss: 0.470097\n",
      "Train Epoch: 799\tLoss: 0.472189\n",
      "Train Epoch: 800\tLoss: 0.466173\n",
      "Train Epoch: 801\tLoss: 0.467985\n",
      "Train Epoch: 802\tLoss: 0.468785\n",
      "Train Epoch: 803\tLoss: 0.468219\n",
      "Train Epoch: 804\tLoss: 0.470218\n",
      "Train Epoch: 805\tLoss: 0.470567\n",
      "Train Epoch: 806\tLoss: 0.470113\n",
      "Train Epoch: 807\tLoss: 0.470694\n",
      "Train Epoch: 808\tLoss: 0.472609\n",
      "Train Epoch: 809\tLoss: 0.473181\n",
      "Train Epoch: 810\tLoss: 0.467936\n",
      "Train Epoch: 811\tLoss: 0.462010\n",
      "Train Epoch: 812\tLoss: 0.465783\n",
      "Train Epoch: 813\tLoss: 0.470364\n",
      "Train Epoch: 814\tLoss: 0.471032\n",
      "Train Epoch: 815\tLoss: 0.468597\n",
      "Train Epoch: 816\tLoss: 0.472786\n",
      "Train Epoch: 817\tLoss: 0.469569\n",
      "Train Epoch: 818\tLoss: 0.471214\n",
      "Train Epoch: 819\tLoss: 0.470576\n",
      "Train Epoch: 820\tLoss: 0.472215\n",
      "Train Epoch: 821\tLoss: 0.467674\n",
      "Train Epoch: 822\tLoss: 0.468569\n",
      "Train Epoch: 823\tLoss: 0.468776\n",
      "Train Epoch: 824\tLoss: 0.468227\n",
      "Train Epoch: 825\tLoss: 0.465191\n",
      "Train Epoch: 826\tLoss: 0.467858\n",
      "Train Epoch: 827\tLoss: 0.471161\n",
      "Train Epoch: 828\tLoss: 0.472072\n",
      "Train Epoch: 829\tLoss: 0.467927\n",
      "Train Epoch: 830\tLoss: 0.466441\n",
      "Train Epoch: 831\tLoss: 0.466234\n",
      "Train Epoch: 832\tLoss: 0.464683\n",
      "Train Epoch: 833\tLoss: 0.471169\n",
      "Train Epoch: 834\tLoss: 0.464151\n",
      "Train Epoch: 835\tLoss: 0.470908\n",
      "Train Epoch: 836\tLoss: 0.468047\n",
      "Train Epoch: 837\tLoss: 0.466392\n",
      "Train Epoch: 838\tLoss: 0.467351\n",
      "Train Epoch: 839\tLoss: 0.466493\n",
      "Train Epoch: 840\tLoss: 0.474507\n",
      "Train Epoch: 841\tLoss: 0.468709\n",
      "Train Epoch: 842\tLoss: 0.462887\n",
      "Train Epoch: 843\tLoss: 0.466942\n",
      "Train Epoch: 844\tLoss: 0.472271\n",
      "Train Epoch: 845\tLoss: 0.471483\n",
      "Train Epoch: 846\tLoss: 0.467014\n",
      "Train Epoch: 847\tLoss: 0.470339\n",
      "Train Epoch: 848\tLoss: 0.463313\n",
      "Train Epoch: 849\tLoss: 0.466286\n",
      "Train Epoch: 850\tLoss: 0.467505\n",
      "Train Epoch: 851\tLoss: 0.462444\n",
      "Train Epoch: 852\tLoss: 0.466356\n",
      "Train Epoch: 853\tLoss: 0.466018\n",
      "Train Epoch: 854\tLoss: 0.464148\n",
      "Train Epoch: 855\tLoss: 0.468427\n",
      "Train Epoch: 856\tLoss: 0.458466\n",
      "Train Epoch: 857\tLoss: 0.469211\n",
      "Train Epoch: 858\tLoss: 0.472132\n",
      "Train Epoch: 859\tLoss: 0.465676\n",
      "Train Epoch: 860\tLoss: 0.467077\n",
      "Train Epoch: 861\tLoss: 0.466884\n",
      "Train Epoch: 862\tLoss: 0.464532\n",
      "Train Epoch: 863\tLoss: 0.467659\n",
      "Train Epoch: 864\tLoss: 0.466640\n",
      "Train Epoch: 865\tLoss: 0.466139\n",
      "Train Epoch: 866\tLoss: 0.464212\n",
      "Train Epoch: 867\tLoss: 0.462810\n",
      "Train Epoch: 868\tLoss: 0.458686\n",
      "Train Epoch: 869\tLoss: 0.462122\n",
      "Train Epoch: 870\tLoss: 0.467814\n",
      "Train Epoch: 871\tLoss: 0.474065\n",
      "Train Epoch: 872\tLoss: 0.465948\n",
      "Train Epoch: 873\tLoss: 0.466934\n",
      "Train Epoch: 874\tLoss: 0.467950\n",
      "Train Epoch: 875\tLoss: 0.458022\n",
      "Train Epoch: 876\tLoss: 0.472109\n",
      "Train Epoch: 877\tLoss: 0.466732\n",
      "Train Epoch: 878\tLoss: 0.472005\n",
      "Train Epoch: 879\tLoss: 0.463395\n",
      "Train Epoch: 880\tLoss: 0.464144\n",
      "Train Epoch: 881\tLoss: 0.468267\n",
      "Train Epoch: 882\tLoss: 0.468694\n",
      "Train Epoch: 883\tLoss: 0.470097\n",
      "Train Epoch: 884\tLoss: 0.462912\n",
      "Train Epoch: 885\tLoss: 0.460173\n",
      "Train Epoch: 886\tLoss: 0.465525\n",
      "Train Epoch: 887\tLoss: 0.466068\n",
      "Train Epoch: 888\tLoss: 0.462542\n",
      "Train Epoch: 889\tLoss: 0.465173\n",
      "Train Epoch: 890\tLoss: 0.464654\n",
      "Train Epoch: 891\tLoss: 0.465537\n",
      "Train Epoch: 892\tLoss: 0.462500\n",
      "Train Epoch: 893\tLoss: 0.467934\n",
      "Train Epoch: 894\tLoss: 0.466074\n",
      "Train Epoch: 895\tLoss: 0.467778\n",
      "Train Epoch: 896\tLoss: 0.459281\n",
      "Train Epoch: 897\tLoss: 0.466966\n",
      "Train Epoch: 898\tLoss: 0.470572\n",
      "Train Epoch: 899\tLoss: 0.461408\n",
      "Train Epoch: 900\tLoss: 0.462278\n",
      "Train Epoch: 901\tLoss: 0.460677\n",
      "Train Epoch: 902\tLoss: 0.465428\n",
      "Train Epoch: 903\tLoss: 0.464723\n",
      "Train Epoch: 904\tLoss: 0.465471\n",
      "Train Epoch: 905\tLoss: 0.467877\n",
      "Train Epoch: 906\tLoss: 0.462559\n",
      "Train Epoch: 907\tLoss: 0.463853\n",
      "Train Epoch: 908\tLoss: 0.464902\n",
      "Train Epoch: 909\tLoss: 0.464410\n",
      "Train Epoch: 910\tLoss: 0.466560\n",
      "Train Epoch: 911\tLoss: 0.467220\n",
      "Train Epoch: 912\tLoss: 0.462807\n",
      "Train Epoch: 913\tLoss: 0.463228\n",
      "Train Epoch: 914\tLoss: 0.469324\n",
      "Train Epoch: 915\tLoss: 0.464595\n",
      "Train Epoch: 916\tLoss: 0.466779\n",
      "Train Epoch: 917\tLoss: 0.458690\n",
      "Train Epoch: 918\tLoss: 0.459389\n",
      "Train Epoch: 919\tLoss: 0.465125\n",
      "Train Epoch: 920\tLoss: 0.463367\n",
      "Train Epoch: 921\tLoss: 0.466814\n",
      "Train Epoch: 922\tLoss: 0.464647\n",
      "Train Epoch: 923\tLoss: 0.470691\n",
      "Train Epoch: 924\tLoss: 0.462275\n",
      "Train Epoch: 925\tLoss: 0.465284\n",
      "Train Epoch: 926\tLoss: 0.465005\n",
      "Train Epoch: 927\tLoss: 0.468144\n",
      "Train Epoch: 928\tLoss: 0.465448\n",
      "Train Epoch: 929\tLoss: 0.462151\n",
      "Train Epoch: 930\tLoss: 0.467591\n",
      "Train Epoch: 931\tLoss: 0.467577\n",
      "Train Epoch: 932\tLoss: 0.465982\n",
      "Train Epoch: 933\tLoss: 0.462669\n",
      "Train Epoch: 934\tLoss: 0.468397\n",
      "Train Epoch: 935\tLoss: 0.460328\n",
      "Train Epoch: 936\tLoss: 0.471447\n",
      "Train Epoch: 937\tLoss: 0.459688\n",
      "Train Epoch: 938\tLoss: 0.463385\n",
      "Train Epoch: 939\tLoss: 0.458109\n",
      "Train Epoch: 940\tLoss: 0.460218\n",
      "Train Epoch: 941\tLoss: 0.465125\n",
      "Train Epoch: 942\tLoss: 0.460767\n",
      "Train Epoch: 943\tLoss: 0.463319\n",
      "Train Epoch: 944\tLoss: 0.465742\n",
      "Train Epoch: 945\tLoss: 0.462499\n",
      "Train Epoch: 946\tLoss: 0.456787\n",
      "Train Epoch: 947\tLoss: 0.466339\n",
      "Train Epoch: 948\tLoss: 0.468950\n",
      "Train Epoch: 949\tLoss: 0.460964\n",
      "Train Epoch: 950\tLoss: 0.457762\n",
      "Train Epoch: 951\tLoss: 0.460922\n",
      "Train Epoch: 952\tLoss: 0.466989\n",
      "Train Epoch: 953\tLoss: 0.463489\n",
      "Train Epoch: 954\tLoss: 0.467891\n",
      "Train Epoch: 955\tLoss: 0.466395\n",
      "Train Epoch: 956\tLoss: 0.460848\n",
      "Train Epoch: 957\tLoss: 0.464866\n",
      "Train Epoch: 958\tLoss: 0.466887\n",
      "Train Epoch: 959\tLoss: 0.468688\n",
      "Train Epoch: 960\tLoss: 0.467466\n",
      "Train Epoch: 961\tLoss: 0.461723\n",
      "Train Epoch: 962\tLoss: 0.471218\n",
      "Train Epoch: 963\tLoss: 0.465757\n",
      "Train Epoch: 964\tLoss: 0.464784\n",
      "Train Epoch: 965\tLoss: 0.463046\n",
      "Train Epoch: 966\tLoss: 0.465184\n",
      "Train Epoch: 967\tLoss: 0.460871\n",
      "Train Epoch: 968\tLoss: 0.458120\n",
      "Train Epoch: 969\tLoss: 0.457571\n",
      "Train Epoch: 970\tLoss: 0.460749\n",
      "Train Epoch: 971\tLoss: 0.459576\n",
      "Train Epoch: 972\tLoss: 0.457995\n",
      "Train Epoch: 973\tLoss: 0.461243\n",
      "Train Epoch: 974\tLoss: 0.463967\n",
      "Train Epoch: 975\tLoss: 0.462799\n",
      "Train Epoch: 976\tLoss: 0.463393\n",
      "Train Epoch: 977\tLoss: 0.456326\n",
      "Train Epoch: 978\tLoss: 0.458784\n",
      "Train Epoch: 979\tLoss: 0.467040\n",
      "Train Epoch: 980\tLoss: 0.463321\n",
      "Train Epoch: 981\tLoss: 0.463886\n",
      "Train Epoch: 982\tLoss: 0.464054\n",
      "Train Epoch: 983\tLoss: 0.459835\n",
      "Train Epoch: 984\tLoss: 0.465474\n",
      "Train Epoch: 985\tLoss: 0.461163\n",
      "Train Epoch: 986\tLoss: 0.460523\n",
      "Train Epoch: 987\tLoss: 0.466033\n",
      "Train Epoch: 988\tLoss: 0.461877\n",
      "Train Epoch: 989\tLoss: 0.457655\n",
      "Train Epoch: 990\tLoss: 0.464131\n",
      "Train Epoch: 991\tLoss: 0.459619\n",
      "Train Epoch: 992\tLoss: 0.455150\n",
      "Train Epoch: 993\tLoss: 0.460244\n",
      "Train Epoch: 994\tLoss: 0.458226\n",
      "Train Epoch: 995\tLoss: 0.463874\n",
      "Train Epoch: 996\tLoss: 0.460854\n",
      "Train Epoch: 997\tLoss: 0.461412\n",
      "Train Epoch: 998\tLoss: 0.462584\n",
      "Train Epoch: 999\tLoss: 0.457095\n",
      "Train Epoch: 1000\tLoss: 0.461983\n",
      "Train Epoch: 1001\tLoss: 0.460226\n",
      "Train Epoch: 1002\tLoss: 0.460432\n",
      "Train Epoch: 1003\tLoss: 0.463527\n",
      "Train Epoch: 1004\tLoss: 0.462914\n",
      "Train Epoch: 1005\tLoss: 0.465759\n",
      "Train Epoch: 1006\tLoss: 0.469742\n",
      "Train Epoch: 1007\tLoss: 0.459546\n",
      "Train Epoch: 1008\tLoss: 0.468995\n",
      "Train Epoch: 1009\tLoss: 0.461265\n",
      "Train Epoch: 1010\tLoss: 0.458892\n",
      "Train Epoch: 1011\tLoss: 0.465366\n",
      "Train Epoch: 1012\tLoss: 0.463315\n",
      "Train Epoch: 1013\tLoss: 0.459477\n",
      "Train Epoch: 1014\tLoss: 0.460053\n",
      "Train Epoch: 1015\tLoss: 0.458171\n",
      "Train Epoch: 1016\tLoss: 0.461377\n",
      "Train Epoch: 1017\tLoss: 0.456734\n",
      "Train Epoch: 1018\tLoss: 0.461909\n",
      "Train Epoch: 1019\tLoss: 0.459896\n",
      "Train Epoch: 1020\tLoss: 0.464029\n",
      "Train Epoch: 1021\tLoss: 0.462974\n",
      "Train Epoch: 1022\tLoss: 0.464030\n",
      "Train Epoch: 1023\tLoss: 0.458009\n",
      "Train Epoch: 1024\tLoss: 0.466026\n",
      "Train Epoch: 1025\tLoss: 0.461053\n",
      "Train Epoch: 1026\tLoss: 0.459969\n",
      "Train Epoch: 1027\tLoss: 0.456339\n",
      "Train Epoch: 1028\tLoss: 0.460026\n",
      "Train Epoch: 1029\tLoss: 0.465207\n",
      "Train Epoch: 1030\tLoss: 0.460786\n",
      "Train Epoch: 1031\tLoss: 0.463112\n",
      "Train Epoch: 1032\tLoss: 0.464255\n",
      "Train Epoch: 1033\tLoss: 0.468128\n",
      "Train Epoch: 1034\tLoss: 0.461193\n",
      "Train Epoch: 1035\tLoss: 0.463838\n",
      "Train Epoch: 1036\tLoss: 0.463545\n",
      "Train Epoch: 1037\tLoss: 0.457662\n",
      "Train Epoch: 1038\tLoss: 0.456843\n",
      "Train Epoch: 1039\tLoss: 0.457044\n",
      "Train Epoch: 1040\tLoss: 0.458079\n",
      "Train Epoch: 1041\tLoss: 0.463311\n",
      "Train Epoch: 1042\tLoss: 0.461033\n",
      "Train Epoch: 1043\tLoss: 0.462215\n",
      "Train Epoch: 1044\tLoss: 0.456559\n",
      "Train Epoch: 1045\tLoss: 0.456482\n",
      "Train Epoch: 1046\tLoss: 0.459769\n",
      "Train Epoch: 1047\tLoss: 0.459502\n",
      "Train Epoch: 1048\tLoss: 0.460572\n",
      "Train Epoch: 1049\tLoss: 0.462369\n",
      "Train Epoch: 1050\tLoss: 0.454385\n",
      "Train Epoch: 1051\tLoss: 0.457914\n",
      "Train Epoch: 1052\tLoss: 0.458314\n",
      "Train Epoch: 1053\tLoss: 0.461716\n",
      "Train Epoch: 1054\tLoss: 0.461242\n",
      "Train Epoch: 1055\tLoss: 0.458539\n",
      "Train Epoch: 1056\tLoss: 0.460184\n",
      "Train Epoch: 1057\tLoss: 0.459104\n",
      "Train Epoch: 1058\tLoss: 0.461634\n",
      "Train Epoch: 1059\tLoss: 0.467491\n",
      "Train Epoch: 1060\tLoss: 0.464224\n",
      "Train Epoch: 1061\tLoss: 0.460748\n",
      "Train Epoch: 1062\tLoss: 0.463078\n",
      "Train Epoch: 1063\tLoss: 0.455241\n",
      "Train Epoch: 1064\tLoss: 0.453659\n",
      "Train Epoch: 1065\tLoss: 0.455090\n",
      "Train Epoch: 1066\tLoss: 0.458526\n",
      "Train Epoch: 1067\tLoss: 0.463794\n",
      "Train Epoch: 1068\tLoss: 0.458652\n",
      "Train Epoch: 1069\tLoss: 0.458148\n",
      "Train Epoch: 1070\tLoss: 0.461438\n",
      "Train Epoch: 1071\tLoss: 0.460156\n",
      "Train Epoch: 1072\tLoss: 0.454067\n",
      "Train Epoch: 1073\tLoss: 0.452972\n",
      "Train Epoch: 1074\tLoss: 0.458313\n",
      "Train Epoch: 1075\tLoss: 0.456795\n",
      "Train Epoch: 1076\tLoss: 0.461791\n",
      "Train Epoch: 1077\tLoss: 0.453776\n",
      "Train Epoch: 1078\tLoss: 0.458649\n",
      "Train Epoch: 1079\tLoss: 0.458649\n",
      "Train Epoch: 1080\tLoss: 0.459352\n",
      "Train Epoch: 1081\tLoss: 0.457877\n",
      "Train Epoch: 1082\tLoss: 0.460661\n",
      "Train Epoch: 1083\tLoss: 0.457747\n",
      "Train Epoch: 1084\tLoss: 0.462190\n",
      "Train Epoch: 1085\tLoss: 0.465601\n",
      "Train Epoch: 1086\tLoss: 0.461893\n",
      "Train Epoch: 1087\tLoss: 0.465888\n",
      "Train Epoch: 1088\tLoss: 0.461005\n",
      "Train Epoch: 1089\tLoss: 0.460205\n",
      "Train Epoch: 1090\tLoss: 0.455071\n",
      "Train Epoch: 1091\tLoss: 0.454156\n",
      "Train Epoch: 1092\tLoss: 0.460252\n",
      "Train Epoch: 1093\tLoss: 0.463391\n",
      "Train Epoch: 1094\tLoss: 0.455437\n",
      "Train Epoch: 1095\tLoss: 0.456575\n",
      "Train Epoch: 1096\tLoss: 0.458520\n",
      "Train Epoch: 1097\tLoss: 0.455284\n",
      "Train Epoch: 1098\tLoss: 0.455450\n",
      "Train Epoch: 1099\tLoss: 0.456079\n",
      "Train Epoch: 1100\tLoss: 0.463424\n",
      "Train Epoch: 1101\tLoss: 0.465280\n",
      "Train Epoch: 1102\tLoss: 0.458071\n",
      "Train Epoch: 1103\tLoss: 0.458986\n",
      "Train Epoch: 1104\tLoss: 0.456792\n",
      "Train Epoch: 1105\tLoss: 0.456891\n",
      "Train Epoch: 1106\tLoss: 0.457821\n",
      "Train Epoch: 1107\tLoss: 0.452751\n",
      "Train Epoch: 1108\tLoss: 0.456628\n",
      "Train Epoch: 1109\tLoss: 0.449003\n",
      "Train Epoch: 1110\tLoss: 0.455206\n",
      "Train Epoch: 1111\tLoss: 0.452002\n",
      "Train Epoch: 1112\tLoss: 0.460150\n",
      "Train Epoch: 1113\tLoss: 0.455331\n",
      "Train Epoch: 1114\tLoss: 0.457672\n",
      "Train Epoch: 1115\tLoss: 0.452968\n",
      "Train Epoch: 1116\tLoss: 0.460974\n",
      "Train Epoch: 1117\tLoss: 0.462171\n",
      "Train Epoch: 1118\tLoss: 0.460597\n",
      "Train Epoch: 1119\tLoss: 0.460192\n",
      "Train Epoch: 1120\tLoss: 0.458058\n",
      "Train Epoch: 1121\tLoss: 0.458770\n",
      "Train Epoch: 1122\tLoss: 0.456301\n",
      "Train Epoch: 1123\tLoss: 0.453433\n",
      "Train Epoch: 1124\tLoss: 0.459438\n",
      "Train Epoch: 1125\tLoss: 0.466204\n",
      "Train Epoch: 1126\tLoss: 0.461053\n",
      "Train Epoch: 1127\tLoss: 0.457318\n",
      "Train Epoch: 1128\tLoss: 0.458749\n",
      "Train Epoch: 1129\tLoss: 0.458254\n",
      "Train Epoch: 1130\tLoss: 0.451051\n",
      "Train Epoch: 1131\tLoss: 0.457584\n",
      "Train Epoch: 1132\tLoss: 0.455585\n",
      "Train Epoch: 1133\tLoss: 0.463874\n",
      "Train Epoch: 1134\tLoss: 0.458003\n",
      "Train Epoch: 1135\tLoss: 0.456149\n",
      "Train Epoch: 1136\tLoss: 0.454095\n",
      "Train Epoch: 1137\tLoss: 0.457688\n",
      "Train Epoch: 1138\tLoss: 0.459823\n",
      "Train Epoch: 1139\tLoss: 0.458926\n",
      "Train Epoch: 1140\tLoss: 0.458563\n",
      "Train Epoch: 1141\tLoss: 0.457131\n",
      "Train Epoch: 1142\tLoss: 0.460563\n",
      "Train Epoch: 1143\tLoss: 0.462263\n",
      "Train Epoch: 1144\tLoss: 0.457503\n",
      "Train Epoch: 1145\tLoss: 0.456145\n",
      "Train Epoch: 1146\tLoss: 0.463107\n",
      "Train Epoch: 1147\tLoss: 0.462105\n",
      "Train Epoch: 1148\tLoss: 0.464591\n",
      "Train Epoch: 1149\tLoss: 0.463480\n",
      "Train Epoch: 1150\tLoss: 0.457923\n",
      "Train Epoch: 1151\tLoss: 0.454278\n",
      "Train Epoch: 1152\tLoss: 0.455851\n",
      "Train Epoch: 1153\tLoss: 0.458584\n",
      "Train Epoch: 1154\tLoss: 0.451705\n",
      "Train Epoch: 1155\tLoss: 0.456094\n",
      "Train Epoch: 1156\tLoss: 0.454408\n",
      "Train Epoch: 1157\tLoss: 0.454894\n",
      "Train Epoch: 1158\tLoss: 0.457228\n",
      "Train Epoch: 1159\tLoss: 0.461293\n",
      "Train Epoch: 1160\tLoss: 0.453005\n",
      "Train Epoch: 1161\tLoss: 0.463423\n",
      "Train Epoch: 1162\tLoss: 0.455367\n",
      "Train Epoch: 1163\tLoss: 0.453521\n",
      "Train Epoch: 1164\tLoss: 0.454811\n",
      "Train Epoch: 1165\tLoss: 0.457892\n",
      "Train Epoch: 1166\tLoss: 0.458739\n",
      "Train Epoch: 1167\tLoss: 0.459256\n",
      "Train Epoch: 1168\tLoss: 0.452742\n",
      "Train Epoch: 1169\tLoss: 0.453859\n",
      "Train Epoch: 1170\tLoss: 0.452579\n",
      "Train Epoch: 1171\tLoss: 0.452560\n",
      "Train Epoch: 1172\tLoss: 0.454757\n",
      "Train Epoch: 1173\tLoss: 0.455972\n",
      "Train Epoch: 1174\tLoss: 0.456902\n",
      "Train Epoch: 1175\tLoss: 0.456205\n",
      "Train Epoch: 1176\tLoss: 0.448751\n",
      "Train Epoch: 1177\tLoss: 0.451778\n",
      "Train Epoch: 1178\tLoss: 0.457926\n",
      "Train Epoch: 1179\tLoss: 0.458803\n",
      "Train Epoch: 1180\tLoss: 0.453009\n",
      "Train Epoch: 1181\tLoss: 0.457849\n",
      "Train Epoch: 1182\tLoss: 0.461676\n",
      "Train Epoch: 1183\tLoss: 0.457533\n",
      "Train Epoch: 1184\tLoss: 0.456321\n",
      "Train Epoch: 1185\tLoss: 0.456341\n",
      "Train Epoch: 1186\tLoss: 0.458983\n",
      "Train Epoch: 1187\tLoss: 0.458419\n",
      "Train Epoch: 1188\tLoss: 0.460409\n",
      "Train Epoch: 1189\tLoss: 0.456108\n",
      "Train Epoch: 1190\tLoss: 0.457769\n",
      "Train Epoch: 1191\tLoss: 0.460534\n",
      "Train Epoch: 1192\tLoss: 0.457955\n",
      "Train Epoch: 1193\tLoss: 0.456037\n",
      "Train Epoch: 1194\tLoss: 0.456572\n",
      "Train Epoch: 1195\tLoss: 0.458624\n",
      "Train Epoch: 1196\tLoss: 0.455277\n",
      "Train Epoch: 1197\tLoss: 0.457227\n",
      "Train Epoch: 1198\tLoss: 0.452792\n",
      "Train Epoch: 1199\tLoss: 0.455960\n",
      "Train Epoch: 1200\tLoss: 0.451322\n",
      "Train Epoch: 1201\tLoss: 0.450621\n",
      "Train Epoch: 1202\tLoss: 0.455859\n",
      "Train Epoch: 1203\tLoss: 0.455410\n",
      "Train Epoch: 1204\tLoss: 0.455240\n",
      "Train Epoch: 1205\tLoss: 0.458975\n",
      "Train Epoch: 1206\tLoss: 0.455541\n",
      "Train Epoch: 1207\tLoss: 0.456842\n",
      "Train Epoch: 1208\tLoss: 0.454611\n",
      "Train Epoch: 1209\tLoss: 0.453306\n",
      "Train Epoch: 1210\tLoss: 0.454483\n",
      "Train Epoch: 1211\tLoss: 0.455723\n",
      "Train Epoch: 1212\tLoss: 0.455334\n",
      "Train Epoch: 1213\tLoss: 0.460560\n",
      "Train Epoch: 1214\tLoss: 0.453758\n",
      "Train Epoch: 1215\tLoss: 0.452726\n",
      "Train Epoch: 1216\tLoss: 0.457546\n",
      "Train Epoch: 1217\tLoss: 0.454281\n",
      "Train Epoch: 1218\tLoss: 0.461941\n",
      "Train Epoch: 1219\tLoss: 0.460277\n",
      "Train Epoch: 1220\tLoss: 0.454329\n",
      "Train Epoch: 1221\tLoss: 0.459972\n",
      "Train Epoch: 1222\tLoss: 0.455954\n",
      "Train Epoch: 1223\tLoss: 0.455197\n",
      "Train Epoch: 1224\tLoss: 0.459484\n",
      "Train Epoch: 1225\tLoss: 0.456011\n",
      "Train Epoch: 1226\tLoss: 0.450310\n",
      "Train Epoch: 1227\tLoss: 0.458484\n",
      "Train Epoch: 1228\tLoss: 0.455309\n",
      "Train Epoch: 1229\tLoss: 0.449930\n",
      "Train Epoch: 1230\tLoss: 0.454729\n",
      "Train Epoch: 1231\tLoss: 0.452907\n",
      "Train Epoch: 1232\tLoss: 0.460464\n",
      "Train Epoch: 1233\tLoss: 0.459120\n",
      "Train Epoch: 1234\tLoss: 0.458692\n",
      "Train Epoch: 1235\tLoss: 0.453417\n",
      "Train Epoch: 1236\tLoss: 0.455039\n",
      "Train Epoch: 1237\tLoss: 0.459772\n",
      "Train Epoch: 1238\tLoss: 0.452562\n",
      "Train Epoch: 1239\tLoss: 0.454510\n",
      "Train Epoch: 1240\tLoss: 0.459173\n",
      "Train Epoch: 1241\tLoss: 0.453765\n",
      "Train Epoch: 1242\tLoss: 0.456094\n",
      "Train Epoch: 1243\tLoss: 0.453677\n",
      "Train Epoch: 1244\tLoss: 0.458409\n",
      "Train Epoch: 1245\tLoss: 0.458070\n",
      "Train Epoch: 1246\tLoss: 0.463288\n",
      "Train Epoch: 1247\tLoss: 0.456871\n",
      "Train Epoch: 1248\tLoss: 0.459388\n",
      "Train Epoch: 1249\tLoss: 0.459791\n",
      "Train Epoch: 1250\tLoss: 0.451338\n",
      "Train Epoch: 1251\tLoss: 0.453323\n",
      "Train Epoch: 1252\tLoss: 0.458376\n",
      "Train Epoch: 1253\tLoss: 0.456954\n",
      "Train Epoch: 1254\tLoss: 0.452382\n",
      "Train Epoch: 1255\tLoss: 0.451653\n",
      "Train Epoch: 1256\tLoss: 0.451895\n",
      "Train Epoch: 1257\tLoss: 0.452865\n",
      "Train Epoch: 1258\tLoss: 0.455300\n",
      "Train Epoch: 1259\tLoss: 0.456042\n",
      "Train Epoch: 1260\tLoss: 0.457935\n",
      "Train Epoch: 1261\tLoss: 0.455888\n",
      "Train Epoch: 1262\tLoss: 0.460066\n",
      "Train Epoch: 1263\tLoss: 0.458924\n",
      "Train Epoch: 1264\tLoss: 0.454486\n",
      "Train Epoch: 1265\tLoss: 0.452529\n",
      "Train Epoch: 1266\tLoss: 0.451986\n",
      "Train Epoch: 1267\tLoss: 0.449131\n",
      "Train Epoch: 1268\tLoss: 0.453204\n",
      "Train Epoch: 1269\tLoss: 0.453633\n",
      "Train Epoch: 1270\tLoss: 0.456000\n",
      "Train Epoch: 1271\tLoss: 0.446962\n",
      "Train Epoch: 1272\tLoss: 0.457257\n",
      "Train Epoch: 1273\tLoss: 0.458335\n",
      "Train Epoch: 1274\tLoss: 0.460528\n",
      "Train Epoch: 1275\tLoss: 0.455315\n",
      "Train Epoch: 1276\tLoss: 0.461447\n",
      "Train Epoch: 1277\tLoss: 0.453710\n",
      "Train Epoch: 1278\tLoss: 0.452855\n",
      "Train Epoch: 1279\tLoss: 0.454802\n",
      "Train Epoch: 1280\tLoss: 0.454277\n",
      "Train Epoch: 1281\tLoss: 0.458538\n",
      "Train Epoch: 1282\tLoss: 0.449520\n",
      "Train Epoch: 1283\tLoss: 0.451533\n",
      "Train Epoch: 1284\tLoss: 0.459698\n",
      "Train Epoch: 1285\tLoss: 0.455298\n",
      "Train Epoch: 1286\tLoss: 0.460990\n",
      "Train Epoch: 1287\tLoss: 0.453516\n",
      "Train Epoch: 1288\tLoss: 0.451694\n",
      "Train Epoch: 1289\tLoss: 0.460817\n",
      "Train Epoch: 1290\tLoss: 0.455327\n",
      "Train Epoch: 1291\tLoss: 0.454299\n",
      "Train Epoch: 1292\tLoss: 0.454478\n",
      "Train Epoch: 1293\tLoss: 0.456662\n",
      "Train Epoch: 1294\tLoss: 0.455687\n",
      "Train Epoch: 1295\tLoss: 0.455762\n",
      "Train Epoch: 1296\tLoss: 0.460335\n",
      "Train Epoch: 1297\tLoss: 0.456521\n",
      "Train Epoch: 1298\tLoss: 0.452479\n",
      "Train Epoch: 1299\tLoss: 0.451775\n",
      "Train Epoch: 1300\tLoss: 0.458016\n",
      "Train Epoch: 1301\tLoss: 0.455937\n",
      "Train Epoch: 1302\tLoss: 0.459281\n",
      "Train Epoch: 1303\tLoss: 0.453914\n",
      "Train Epoch: 1304\tLoss: 0.455206\n",
      "Train Epoch: 1305\tLoss: 0.455563\n",
      "Train Epoch: 1306\tLoss: 0.451360\n",
      "Train Epoch: 1307\tLoss: 0.456192\n",
      "Train Epoch: 1308\tLoss: 0.461998\n",
      "Train Epoch: 1309\tLoss: 0.450145\n",
      "Train Epoch: 1310\tLoss: 0.450612\n",
      "Train Epoch: 1311\tLoss: 0.455478\n",
      "Train Epoch: 1312\tLoss: 0.460024\n",
      "Train Epoch: 1313\tLoss: 0.453903\n",
      "Train Epoch: 1314\tLoss: 0.452541\n",
      "Train Epoch: 1315\tLoss: 0.449869\n",
      "Train Epoch: 1316\tLoss: 0.449433\n",
      "Train Epoch: 1317\tLoss: 0.452822\n",
      "Train Epoch: 1318\tLoss: 0.459279\n",
      "Train Epoch: 1319\tLoss: 0.452417\n",
      "Train Epoch: 1320\tLoss: 0.454876\n",
      "Train Epoch: 1321\tLoss: 0.454674\n",
      "Train Epoch: 1322\tLoss: 0.455108\n",
      "Train Epoch: 1323\tLoss: 0.453817\n",
      "Train Epoch: 1324\tLoss: 0.459288\n",
      "Train Epoch: 1325\tLoss: 0.449315\n",
      "Train Epoch: 1326\tLoss: 0.455197\n",
      "Train Epoch: 1327\tLoss: 0.451555\n",
      "Train Epoch: 1328\tLoss: 0.450724\n",
      "Train Epoch: 1329\tLoss: 0.456951\n",
      "Train Epoch: 1330\tLoss: 0.452038\n",
      "Train Epoch: 1331\tLoss: 0.452477\n",
      "Train Epoch: 1332\tLoss: 0.460512\n",
      "Train Epoch: 1333\tLoss: 0.454971\n",
      "Train Epoch: 1334\tLoss: 0.457078\n",
      "Train Epoch: 1335\tLoss: 0.454496\n",
      "Train Epoch: 1336\tLoss: 0.454542\n",
      "Train Epoch: 1337\tLoss: 0.447627\n",
      "Train Epoch: 1338\tLoss: 0.448237\n",
      "Train Epoch: 1339\tLoss: 0.447408\n",
      "Train Epoch: 1340\tLoss: 0.447096\n",
      "Train Epoch: 1341\tLoss: 0.455475\n",
      "Train Epoch: 1342\tLoss: 0.451559\n",
      "Train Epoch: 1343\tLoss: 0.453426\n",
      "Train Epoch: 1344\tLoss: 0.451170\n",
      "Train Epoch: 1345\tLoss: 0.449247\n",
      "Train Epoch: 1346\tLoss: 0.449318\n",
      "Train Epoch: 1347\tLoss: 0.444607\n",
      "Train Epoch: 1348\tLoss: 0.459245\n",
      "Train Epoch: 1349\tLoss: 0.454143\n",
      "Train Epoch: 1350\tLoss: 0.456874\n",
      "Train Epoch: 1351\tLoss: 0.456544\n",
      "Train Epoch: 1352\tLoss: 0.450305\n",
      "Train Epoch: 1353\tLoss: 0.451009\n",
      "Train Epoch: 1354\tLoss: 0.444859\n",
      "Train Epoch: 1355\tLoss: 0.450170\n",
      "Train Epoch: 1356\tLoss: 0.448315\n",
      "Train Epoch: 1357\tLoss: 0.455948\n",
      "Train Epoch: 1358\tLoss: 0.450443\n",
      "Train Epoch: 1359\tLoss: 0.452433\n",
      "Train Epoch: 1360\tLoss: 0.448471\n",
      "Train Epoch: 1361\tLoss: 0.450496\n",
      "Train Epoch: 1362\tLoss: 0.451467\n",
      "Train Epoch: 1363\tLoss: 0.449253\n",
      "Train Epoch: 1364\tLoss: 0.448525\n",
      "Train Epoch: 1365\tLoss: 0.453433\n",
      "Train Epoch: 1366\tLoss: 0.453084\n",
      "Train Epoch: 1367\tLoss: 0.454575\n",
      "Train Epoch: 1368\tLoss: 0.452009\n",
      "Train Epoch: 1369\tLoss: 0.447782\n",
      "Train Epoch: 1370\tLoss: 0.452198\n",
      "Train Epoch: 1371\tLoss: 0.456439\n",
      "Train Epoch: 1372\tLoss: 0.453144\n",
      "Train Epoch: 1373\tLoss: 0.448844\n",
      "Train Epoch: 1374\tLoss: 0.451193\n",
      "Train Epoch: 1375\tLoss: 0.458108\n",
      "Train Epoch: 1376\tLoss: 0.458701\n",
      "Train Epoch: 1377\tLoss: 0.454378\n",
      "Train Epoch: 1378\tLoss: 0.454739\n",
      "Train Epoch: 1379\tLoss: 0.448733\n",
      "Train Epoch: 1380\tLoss: 0.447455\n",
      "Train Epoch: 1381\tLoss: 0.455053\n",
      "Train Epoch: 1382\tLoss: 0.452103\n",
      "Train Epoch: 1383\tLoss: 0.451726\n",
      "Train Epoch: 1384\tLoss: 0.447438\n",
      "Train Epoch: 1385\tLoss: 0.451254\n",
      "Train Epoch: 1386\tLoss: 0.444761\n",
      "Train Epoch: 1387\tLoss: 0.451627\n",
      "Train Epoch: 1388\tLoss: 0.451287\n",
      "Train Epoch: 1389\tLoss: 0.446624\n",
      "Train Epoch: 1390\tLoss: 0.449320\n",
      "Train Epoch: 1391\tLoss: 0.457949\n",
      "Train Epoch: 1392\tLoss: 0.449106\n",
      "Train Epoch: 1393\tLoss: 0.453697\n",
      "Train Epoch: 1394\tLoss: 0.446737\n",
      "Train Epoch: 1395\tLoss: 0.448739\n",
      "Train Epoch: 1396\tLoss: 0.452264\n",
      "Train Epoch: 1397\tLoss: 0.444874\n",
      "Train Epoch: 1398\tLoss: 0.455676\n",
      "Train Epoch: 1399\tLoss: 0.449690\n",
      "Train Epoch: 1400\tLoss: 0.453101\n",
      "Train Epoch: 1401\tLoss: 0.451251\n",
      "Train Epoch: 1402\tLoss: 0.450311\n",
      "Train Epoch: 1403\tLoss: 0.453740\n",
      "Train Epoch: 1404\tLoss: 0.457462\n",
      "Train Epoch: 1405\tLoss: 0.451950\n",
      "Train Epoch: 1406\tLoss: 0.451229\n",
      "Train Epoch: 1407\tLoss: 0.446762\n",
      "Train Epoch: 1408\tLoss: 0.449346\n",
      "Train Epoch: 1409\tLoss: 0.454291\n",
      "Train Epoch: 1410\tLoss: 0.449991\n",
      "Train Epoch: 1411\tLoss: 0.453786\n",
      "Train Epoch: 1412\tLoss: 0.450801\n",
      "Train Epoch: 1413\tLoss: 0.450642\n",
      "Train Epoch: 1414\tLoss: 0.459695\n",
      "Train Epoch: 1415\tLoss: 0.455814\n",
      "Train Epoch: 1416\tLoss: 0.459862\n",
      "Train Epoch: 1417\tLoss: 0.451115\n",
      "Train Epoch: 1418\tLoss: 0.454122\n",
      "Train Epoch: 1419\tLoss: 0.451736\n",
      "Train Epoch: 1420\tLoss: 0.450185\n",
      "Train Epoch: 1421\tLoss: 0.450211\n",
      "Train Epoch: 1422\tLoss: 0.452521\n",
      "Train Epoch: 1423\tLoss: 0.456379\n",
      "Train Epoch: 1424\tLoss: 0.454359\n",
      "Train Epoch: 1425\tLoss: 0.454010\n",
      "Train Epoch: 1426\tLoss: 0.451628\n",
      "Train Epoch: 1427\tLoss: 0.453183\n",
      "Train Epoch: 1428\tLoss: 0.450784\n",
      "Train Epoch: 1429\tLoss: 0.453692\n",
      "Train Epoch: 1430\tLoss: 0.453709\n",
      "Train Epoch: 1431\tLoss: 0.454412\n",
      "Train Epoch: 1432\tLoss: 0.454145\n",
      "Train Epoch: 1433\tLoss: 0.452162\n",
      "Train Epoch: 1434\tLoss: 0.452390\n",
      "Train Epoch: 1435\tLoss: 0.455626\n",
      "Train Epoch: 1436\tLoss: 0.450291\n",
      "Train Epoch: 1437\tLoss: 0.451660\n",
      "Train Epoch: 1438\tLoss: 0.451712\n",
      "Train Epoch: 1439\tLoss: 0.447204\n",
      "Train Epoch: 1440\tLoss: 0.455149\n",
      "Train Epoch: 1441\tLoss: 0.447570\n",
      "Train Epoch: 1442\tLoss: 0.454318\n",
      "Train Epoch: 1443\tLoss: 0.454333\n",
      "Train Epoch: 1444\tLoss: 0.452314\n",
      "Train Epoch: 1445\tLoss: 0.448903\n",
      "Train Epoch: 1446\tLoss: 0.450119\n",
      "Train Epoch: 1447\tLoss: 0.451463\n",
      "Train Epoch: 1448\tLoss: 0.448041\n",
      "Train Epoch: 1449\tLoss: 0.450096\n",
      "Train Epoch: 1450\tLoss: 0.451292\n",
      "Train Epoch: 1451\tLoss: 0.450102\n",
      "Train Epoch: 1452\tLoss: 0.449069\n",
      "Train Epoch: 1453\tLoss: 0.446813\n",
      "Train Epoch: 1454\tLoss: 0.462191\n",
      "Train Epoch: 1455\tLoss: 0.452042\n",
      "Train Epoch: 1456\tLoss: 0.455876\n",
      "Train Epoch: 1457\tLoss: 0.448590\n",
      "Train Epoch: 1458\tLoss: 0.454370\n",
      "Train Epoch: 1459\tLoss: 0.451451\n",
      "Train Epoch: 1460\tLoss: 0.453736\n",
      "Train Epoch: 1461\tLoss: 0.451850\n",
      "Train Epoch: 1462\tLoss: 0.450758\n",
      "Train Epoch: 1463\tLoss: 0.447509\n",
      "Train Epoch: 1464\tLoss: 0.447170\n",
      "Train Epoch: 1465\tLoss: 0.449182\n",
      "Train Epoch: 1466\tLoss: 0.449968\n",
      "Train Epoch: 1467\tLoss: 0.452571\n",
      "Train Epoch: 1468\tLoss: 0.448020\n",
      "Train Epoch: 1469\tLoss: 0.451342\n",
      "Train Epoch: 1470\tLoss: 0.450765\n",
      "Train Epoch: 1471\tLoss: 0.450762\n",
      "Train Epoch: 1472\tLoss: 0.449960\n",
      "Train Epoch: 1473\tLoss: 0.442975\n",
      "Train Epoch: 1474\tLoss: 0.452587\n",
      "Train Epoch: 1475\tLoss: 0.447885\n",
      "Train Epoch: 1476\tLoss: 0.453296\n",
      "Train Epoch: 1477\tLoss: 0.452370\n",
      "Train Epoch: 1478\tLoss: 0.444995\n",
      "Train Epoch: 1479\tLoss: 0.450905\n",
      "Train Epoch: 1480\tLoss: 0.450979\n",
      "Train Epoch: 1481\tLoss: 0.446458\n",
      "Train Epoch: 1482\tLoss: 0.445940\n",
      "Train Epoch: 1483\tLoss: 0.459262\n",
      "Train Epoch: 1484\tLoss: 0.451659\n",
      "Train Epoch: 1485\tLoss: 0.457315\n",
      "Train Epoch: 1486\tLoss: 0.457323\n",
      "Train Epoch: 1487\tLoss: 0.446850\n",
      "Train Epoch: 1488\tLoss: 0.448169\n",
      "Train Epoch: 1489\tLoss: 0.456097\n",
      "Train Epoch: 1490\tLoss: 0.448051\n",
      "Train Epoch: 1491\tLoss: 0.449933\n",
      "Train Epoch: 1492\tLoss: 0.450021\n",
      "Train Epoch: 1493\tLoss: 0.448806\n",
      "Train Epoch: 1494\tLoss: 0.453713\n",
      "Train Epoch: 1495\tLoss: 0.452222\n",
      "Train Epoch: 1496\tLoss: 0.451898\n",
      "Train Epoch: 1497\tLoss: 0.446446\n",
      "Train Epoch: 1498\tLoss: 0.442135\n",
      "Train Epoch: 1499\tLoss: 0.451861\n",
      "Train Epoch: 1500\tLoss: 0.448828\n",
      "Train Epoch: 1501\tLoss: 0.449794\n",
      "Train Epoch: 1502\tLoss: 0.449127\n",
      "Train Epoch: 1503\tLoss: 0.447429\n",
      "Train Epoch: 1504\tLoss: 0.444052\n",
      "Train Epoch: 1505\tLoss: 0.456271\n",
      "Train Epoch: 1506\tLoss: 0.453673\n",
      "Train Epoch: 1507\tLoss: 0.448313\n",
      "Train Epoch: 1508\tLoss: 0.447305\n",
      "Train Epoch: 1509\tLoss: 0.450507\n",
      "Train Epoch: 1510\tLoss: 0.446192\n",
      "Train Epoch: 1511\tLoss: 0.454248\n",
      "Train Epoch: 1512\tLoss: 0.451619\n",
      "Train Epoch: 1513\tLoss: 0.450393\n",
      "Train Epoch: 1514\tLoss: 0.445538\n",
      "Train Epoch: 1515\tLoss: 0.448622\n",
      "Train Epoch: 1516\tLoss: 0.449708\n",
      "Train Epoch: 1517\tLoss: 0.449379\n",
      "Train Epoch: 1518\tLoss: 0.449366\n",
      "Train Epoch: 1519\tLoss: 0.452977\n",
      "Train Epoch: 1520\tLoss: 0.451248\n",
      "Train Epoch: 1521\tLoss: 0.457663\n",
      "Train Epoch: 1522\tLoss: 0.454332\n",
      "Train Epoch: 1523\tLoss: 0.451190\n",
      "Train Epoch: 1524\tLoss: 0.451378\n",
      "Train Epoch: 1525\tLoss: 0.453091\n",
      "Train Epoch: 1526\tLoss: 0.450027\n",
      "Train Epoch: 1527\tLoss: 0.451383\n",
      "Train Epoch: 1528\tLoss: 0.451985\n",
      "Train Epoch: 1529\tLoss: 0.445096\n",
      "Train Epoch: 1530\tLoss: 0.445934\n",
      "Train Epoch: 1531\tLoss: 0.446698\n",
      "Train Epoch: 1532\tLoss: 0.441786\n",
      "Train Epoch: 1533\tLoss: 0.449128\n",
      "Train Epoch: 1534\tLoss: 0.451634\n",
      "Train Epoch: 1535\tLoss: 0.453879\n",
      "Train Epoch: 1536\tLoss: 0.454800\n",
      "Train Epoch: 1537\tLoss: 0.443875\n",
      "Train Epoch: 1538\tLoss: 0.452050\n",
      "Train Epoch: 1539\tLoss: 0.447283\n",
      "Train Epoch: 1540\tLoss: 0.447809\n",
      "Train Epoch: 1541\tLoss: 0.450804\n",
      "Train Epoch: 1542\tLoss: 0.447707\n",
      "Train Epoch: 1543\tLoss: 0.451576\n",
      "Train Epoch: 1544\tLoss: 0.447279\n",
      "Train Epoch: 1545\tLoss: 0.454310\n",
      "Train Epoch: 1546\tLoss: 0.454062\n",
      "Train Epoch: 1547\tLoss: 0.441874\n",
      "Train Epoch: 1548\tLoss: 0.450786\n",
      "Train Epoch: 1549\tLoss: 0.453695\n",
      "Train Epoch: 1550\tLoss: 0.458055\n",
      "Train Epoch: 1551\tLoss: 0.450248\n",
      "Train Epoch: 1552\tLoss: 0.453217\n",
      "Train Epoch: 1553\tLoss: 0.449425\n",
      "Train Epoch: 1554\tLoss: 0.452923\n",
      "Train Epoch: 1555\tLoss: 0.443683\n",
      "Train Epoch: 1556\tLoss: 0.448518\n",
      "Train Epoch: 1557\tLoss: 0.453173\n",
      "Train Epoch: 1558\tLoss: 0.447547\n",
      "Train Epoch: 1559\tLoss: 0.448457\n",
      "Train Epoch: 1560\tLoss: 0.452715\n",
      "Train Epoch: 1561\tLoss: 0.444385\n",
      "Train Epoch: 1562\tLoss: 0.448512\n",
      "Train Epoch: 1563\tLoss: 0.445540\n",
      "Train Epoch: 1564\tLoss: 0.451581\n",
      "Train Epoch: 1565\tLoss: 0.450395\n",
      "Train Epoch: 1566\tLoss: 0.451919\n",
      "Train Epoch: 1567\tLoss: 0.445503\n",
      "Train Epoch: 1568\tLoss: 0.452361\n",
      "Train Epoch: 1569\tLoss: 0.447428\n",
      "Train Epoch: 1570\tLoss: 0.445253\n",
      "Train Epoch: 1571\tLoss: 0.442294\n",
      "Train Epoch: 1572\tLoss: 0.457581\n",
      "Train Epoch: 1573\tLoss: 0.449577\n",
      "Train Epoch: 1574\tLoss: 0.449783\n",
      "Train Epoch: 1575\tLoss: 0.448865\n",
      "Train Epoch: 1576\tLoss: 0.449379\n",
      "Train Epoch: 1577\tLoss: 0.445339\n",
      "Train Epoch: 1578\tLoss: 0.447434\n",
      "Train Epoch: 1579\tLoss: 0.446176\n",
      "Train Epoch: 1580\tLoss: 0.443471\n",
      "Train Epoch: 1581\tLoss: 0.448497\n",
      "Train Epoch: 1582\tLoss: 0.446749\n",
      "Train Epoch: 1583\tLoss: 0.448166\n",
      "Train Epoch: 1584\tLoss: 0.455838\n",
      "Train Epoch: 1585\tLoss: 0.449949\n",
      "Train Epoch: 1586\tLoss: 0.452787\n",
      "Train Epoch: 1587\tLoss: 0.450858\n",
      "Train Epoch: 1588\tLoss: 0.453192\n",
      "Train Epoch: 1589\tLoss: 0.450315\n",
      "Train Epoch: 1590\tLoss: 0.447400\n",
      "Train Epoch: 1591\tLoss: 0.446248\n",
      "Train Epoch: 1592\tLoss: 0.453742\n",
      "Train Epoch: 1593\tLoss: 0.451976\n",
      "Train Epoch: 1594\tLoss: 0.450542\n",
      "Train Epoch: 1595\tLoss: 0.446771\n",
      "Train Epoch: 1596\tLoss: 0.446145\n",
      "Train Epoch: 1597\tLoss: 0.453095\n",
      "Train Epoch: 1598\tLoss: 0.449456\n",
      "Train Epoch: 1599\tLoss: 0.443130\n",
      "Train Epoch: 1600\tLoss: 0.451118\n",
      "Train Epoch: 1601\tLoss: 0.449189\n",
      "Train Epoch: 1602\tLoss: 0.450770\n",
      "Train Epoch: 1603\tLoss: 0.437784\n",
      "Train Epoch: 1604\tLoss: 0.452598\n",
      "Train Epoch: 1605\tLoss: 0.448071\n",
      "Train Epoch: 1606\tLoss: 0.448841\n",
      "Train Epoch: 1607\tLoss: 0.451387\n",
      "Train Epoch: 1608\tLoss: 0.451565\n",
      "Train Epoch: 1609\tLoss: 0.456319\n",
      "Train Epoch: 1610\tLoss: 0.443481\n",
      "Train Epoch: 1611\tLoss: 0.448035\n",
      "Train Epoch: 1612\tLoss: 0.451186\n",
      "Train Epoch: 1613\tLoss: 0.451906\n",
      "Train Epoch: 1614\tLoss: 0.447992\n",
      "Train Epoch: 1615\tLoss: 0.449470\n",
      "Train Epoch: 1616\tLoss: 0.452610\n",
      "Train Epoch: 1617\tLoss: 0.451739\n",
      "Train Epoch: 1618\tLoss: 0.449144\n",
      "Train Epoch: 1619\tLoss: 0.440411\n",
      "Train Epoch: 1620\tLoss: 0.444899\n",
      "Train Epoch: 1621\tLoss: 0.447950\n",
      "Train Epoch: 1622\tLoss: 0.449964\n",
      "Train Epoch: 1623\tLoss: 0.446137\n",
      "Train Epoch: 1624\tLoss: 0.448432\n",
      "Train Epoch: 1625\tLoss: 0.450340\n",
      "Train Epoch: 1626\tLoss: 0.448290\n",
      "Train Epoch: 1627\tLoss: 0.446335\n",
      "Train Epoch: 1628\tLoss: 0.448246\n",
      "Train Epoch: 1629\tLoss: 0.453278\n",
      "Train Epoch: 1630\tLoss: 0.447723\n",
      "Train Epoch: 1631\tLoss: 0.447935\n",
      "Train Epoch: 1632\tLoss: 0.452883\n",
      "Train Epoch: 1633\tLoss: 0.456051\n",
      "Train Epoch: 1634\tLoss: 0.451763\n",
      "Train Epoch: 1635\tLoss: 0.447062\n",
      "Train Epoch: 1636\tLoss: 0.452621\n",
      "Train Epoch: 1637\tLoss: 0.447925\n",
      "Train Epoch: 1638\tLoss: 0.454344\n",
      "Train Epoch: 1639\tLoss: 0.449228\n",
      "Train Epoch: 1640\tLoss: 0.444624\n",
      "Train Epoch: 1641\tLoss: 0.441406\n",
      "Train Epoch: 1642\tLoss: 0.445756\n",
      "Train Epoch: 1643\tLoss: 0.446273\n",
      "Train Epoch: 1644\tLoss: 0.449939\n",
      "Train Epoch: 1645\tLoss: 0.448727\n",
      "Train Epoch: 1646\tLoss: 0.451351\n",
      "Train Epoch: 1647\tLoss: 0.452561\n",
      "Train Epoch: 1648\tLoss: 0.443159\n",
      "Train Epoch: 1649\tLoss: 0.447247\n",
      "Train Epoch: 1650\tLoss: 0.444877\n",
      "Train Epoch: 1651\tLoss: 0.440455\n",
      "Train Epoch: 1652\tLoss: 0.446231\n",
      "Train Epoch: 1653\tLoss: 0.456457\n",
      "Train Epoch: 1654\tLoss: 0.448027\n",
      "Train Epoch: 1655\tLoss: 0.441737\n",
      "Train Epoch: 1656\tLoss: 0.447094\n",
      "Train Epoch: 1657\tLoss: 0.445791\n",
      "Train Epoch: 1658\tLoss: 0.450495\n",
      "Train Epoch: 1659\tLoss: 0.452667\n",
      "Train Epoch: 1660\tLoss: 0.450372\n",
      "Train Epoch: 1661\tLoss: 0.447541\n",
      "Train Epoch: 1662\tLoss: 0.447988\n",
      "Train Epoch: 1663\tLoss: 0.447447\n",
      "Train Epoch: 1664\tLoss: 0.447099\n",
      "Train Epoch: 1665\tLoss: 0.448208\n",
      "Train Epoch: 1666\tLoss: 0.448222\n",
      "Train Epoch: 1667\tLoss: 0.449302\n",
      "Train Epoch: 1668\tLoss: 0.447277\n",
      "Train Epoch: 1669\tLoss: 0.448401\n",
      "Train Epoch: 1670\tLoss: 0.444320\n",
      "Train Epoch: 1671\tLoss: 0.450006\n",
      "Train Epoch: 1672\tLoss: 0.449143\n",
      "Train Epoch: 1673\tLoss: 0.440522\n",
      "Train Epoch: 1674\tLoss: 0.447748\n",
      "Train Epoch: 1675\tLoss: 0.450418\n",
      "Train Epoch: 1676\tLoss: 0.453752\n",
      "Train Epoch: 1677\tLoss: 0.450263\n",
      "Train Epoch: 1678\tLoss: 0.451969\n",
      "Train Epoch: 1679\tLoss: 0.444920\n",
      "Train Epoch: 1680\tLoss: 0.447733\n",
      "Train Epoch: 1681\tLoss: 0.448486\n",
      "Train Epoch: 1682\tLoss: 0.445607\n",
      "Train Epoch: 1683\tLoss: 0.445832\n",
      "Train Epoch: 1684\tLoss: 0.446027\n",
      "Train Epoch: 1685\tLoss: 0.447698\n",
      "Train Epoch: 1686\tLoss: 0.444171\n",
      "Train Epoch: 1687\tLoss: 0.450162\n",
      "Train Epoch: 1688\tLoss: 0.453090\n",
      "Train Epoch: 1689\tLoss: 0.453963\n",
      "Train Epoch: 1690\tLoss: 0.455075\n",
      "Train Epoch: 1691\tLoss: 0.448541\n",
      "Train Epoch: 1692\tLoss: 0.447650\n",
      "Train Epoch: 1693\tLoss: 0.444689\n",
      "Train Epoch: 1694\tLoss: 0.444261\n",
      "Train Epoch: 1695\tLoss: 0.443227\n",
      "Train Epoch: 1696\tLoss: 0.446401\n",
      "Train Epoch: 1697\tLoss: 0.446120\n",
      "Train Epoch: 1698\tLoss: 0.446615\n",
      "Train Epoch: 1699\tLoss: 0.445518\n",
      "Train Epoch: 1700\tLoss: 0.444736\n",
      "Train Epoch: 1701\tLoss: 0.449140\n",
      "Train Epoch: 1702\tLoss: 0.447246\n",
      "Train Epoch: 1703\tLoss: 0.447776\n",
      "Train Epoch: 1704\tLoss: 0.453013\n",
      "Train Epoch: 1705\tLoss: 0.450293\n",
      "Train Epoch: 1706\tLoss: 0.451685\n",
      "Train Epoch: 1707\tLoss: 0.448571\n",
      "Train Epoch: 1708\tLoss: 0.443847\n",
      "Train Epoch: 1709\tLoss: 0.451293\n",
      "Train Epoch: 1710\tLoss: 0.446935\n",
      "Train Epoch: 1711\tLoss: 0.444615\n",
      "Train Epoch: 1712\tLoss: 0.449604\n",
      "Train Epoch: 1713\tLoss: 0.441473\n",
      "Train Epoch: 1714\tLoss: 0.451317\n",
      "Train Epoch: 1715\tLoss: 0.445837\n",
      "Train Epoch: 1716\tLoss: 0.442171\n",
      "Train Epoch: 1717\tLoss: 0.444352\n",
      "Train Epoch: 1718\tLoss: 0.446069\n",
      "Train Epoch: 1719\tLoss: 0.441903\n",
      "Train Epoch: 1720\tLoss: 0.447712\n",
      "Train Epoch: 1721\tLoss: 0.447372\n",
      "Train Epoch: 1722\tLoss: 0.450998\n",
      "Train Epoch: 1723\tLoss: 0.451463\n",
      "Train Epoch: 1724\tLoss: 0.452000\n",
      "Train Epoch: 1725\tLoss: 0.448411\n",
      "Train Epoch: 1726\tLoss: 0.444183\n",
      "Train Epoch: 1727\tLoss: 0.445240\n",
      "Train Epoch: 1728\tLoss: 0.448808\n",
      "Train Epoch: 1729\tLoss: 0.452012\n",
      "Train Epoch: 1730\tLoss: 0.447379\n",
      "Train Epoch: 1731\tLoss: 0.444476\n",
      "Train Epoch: 1732\tLoss: 0.445936\n",
      "Train Epoch: 1733\tLoss: 0.450864\n",
      "Train Epoch: 1734\tLoss: 0.449025\n",
      "Train Epoch: 1735\tLoss: 0.449412\n",
      "Train Epoch: 1736\tLoss: 0.450403\n",
      "Train Epoch: 1737\tLoss: 0.449415\n",
      "Train Epoch: 1738\tLoss: 0.443118\n",
      "Train Epoch: 1739\tLoss: 0.447012\n",
      "Train Epoch: 1740\tLoss: 0.442325\n",
      "Train Epoch: 1741\tLoss: 0.439005\n",
      "Train Epoch: 1742\tLoss: 0.444891\n",
      "Train Epoch: 1743\tLoss: 0.452456\n",
      "Train Epoch: 1744\tLoss: 0.451443\n",
      "Train Epoch: 1745\tLoss: 0.450605\n",
      "Train Epoch: 1746\tLoss: 0.442345\n",
      "Train Epoch: 1747\tLoss: 0.448322\n",
      "Train Epoch: 1748\tLoss: 0.449528\n",
      "Train Epoch: 1749\tLoss: 0.442861\n",
      "Train Epoch: 1750\tLoss: 0.447834\n",
      "Train Epoch: 1751\tLoss: 0.444490\n",
      "Train Epoch: 1752\tLoss: 0.447184\n",
      "Train Epoch: 1753\tLoss: 0.444746\n",
      "Train Epoch: 1754\tLoss: 0.447823\n",
      "Train Epoch: 1755\tLoss: 0.446346\n",
      "Train Epoch: 1756\tLoss: 0.444387\n",
      "Train Epoch: 1757\tLoss: 0.444891\n",
      "Train Epoch: 1758\tLoss: 0.448761\n",
      "Train Epoch: 1759\tLoss: 0.444101\n",
      "Train Epoch: 1760\tLoss: 0.442229\n",
      "Train Epoch: 1761\tLoss: 0.452022\n",
      "Train Epoch: 1762\tLoss: 0.445627\n",
      "Train Epoch: 1763\tLoss: 0.448621\n",
      "Train Epoch: 1764\tLoss: 0.440009\n",
      "Train Epoch: 1765\tLoss: 0.446788\n",
      "Train Epoch: 1766\tLoss: 0.448090\n",
      "Train Epoch: 1767\tLoss: 0.451507\n",
      "Train Epoch: 1768\tLoss: 0.447550\n",
      "Train Epoch: 1769\tLoss: 0.447032\n",
      "Train Epoch: 1770\tLoss: 0.445873\n",
      "Train Epoch: 1771\tLoss: 0.448872\n",
      "Train Epoch: 1772\tLoss: 0.446004\n",
      "Train Epoch: 1773\tLoss: 0.440531\n",
      "Train Epoch: 1774\tLoss: 0.445185\n",
      "Train Epoch: 1775\tLoss: 0.446780\n",
      "Train Epoch: 1776\tLoss: 0.451674\n",
      "Train Epoch: 1777\tLoss: 0.445176\n",
      "Train Epoch: 1778\tLoss: 0.443935\n",
      "Train Epoch: 1779\tLoss: 0.445083\n",
      "Train Epoch: 1780\tLoss: 0.451046\n",
      "Train Epoch: 1781\tLoss: 0.445105\n",
      "Train Epoch: 1782\tLoss: 0.441980\n",
      "Train Epoch: 1783\tLoss: 0.443618\n",
      "Train Epoch: 1784\tLoss: 0.446805\n",
      "Train Epoch: 1785\tLoss: 0.448445\n",
      "Train Epoch: 1786\tLoss: 0.453168\n",
      "Train Epoch: 1787\tLoss: 0.451925\n",
      "Train Epoch: 1788\tLoss: 0.441372\n",
      "Train Epoch: 1789\tLoss: 0.449358\n",
      "Train Epoch: 1790\tLoss: 0.447217\n",
      "Train Epoch: 1791\tLoss: 0.449196\n",
      "Train Epoch: 1792\tLoss: 0.437200\n",
      "Train Epoch: 1793\tLoss: 0.445271\n",
      "Train Epoch: 1794\tLoss: 0.444040\n",
      "Train Epoch: 1795\tLoss: 0.443182\n",
      "Train Epoch: 1796\tLoss: 0.449930\n",
      "Train Epoch: 1797\tLoss: 0.448739\n",
      "Train Epoch: 1798\tLoss: 0.444376\n",
      "Train Epoch: 1799\tLoss: 0.448991\n",
      "Train Epoch: 1800\tLoss: 0.449953\n",
      "Train Epoch: 1801\tLoss: 0.448759\n",
      "Train Epoch: 1802\tLoss: 0.437290\n",
      "Train Epoch: 1803\tLoss: 0.448838\n",
      "Train Epoch: 1804\tLoss: 0.446444\n",
      "Train Epoch: 1805\tLoss: 0.450771\n",
      "Train Epoch: 1806\tLoss: 0.445018\n",
      "Train Epoch: 1807\tLoss: 0.444228\n",
      "Train Epoch: 1808\tLoss: 0.444681\n",
      "Train Epoch: 1809\tLoss: 0.444123\n",
      "Train Epoch: 1810\tLoss: 0.447805\n",
      "Train Epoch: 1811\tLoss: 0.445763\n",
      "Train Epoch: 1812\tLoss: 0.450264\n",
      "Train Epoch: 1813\tLoss: 0.440955\n",
      "Train Epoch: 1814\tLoss: 0.448393\n",
      "Train Epoch: 1815\tLoss: 0.447939\n",
      "Train Epoch: 1816\tLoss: 0.442259\n",
      "Train Epoch: 1817\tLoss: 0.451576\n",
      "Train Epoch: 1818\tLoss: 0.443674\n",
      "Train Epoch: 1819\tLoss: 0.445793\n",
      "Train Epoch: 1820\tLoss: 0.444395\n",
      "Train Epoch: 1821\tLoss: 0.449777\n",
      "Train Epoch: 1822\tLoss: 0.445012\n",
      "Train Epoch: 1823\tLoss: 0.446771\n",
      "Train Epoch: 1824\tLoss: 0.440783\n",
      "Train Epoch: 1825\tLoss: 0.441726\n",
      "Train Epoch: 1826\tLoss: 0.443912\n",
      "Train Epoch: 1827\tLoss: 0.450108\n",
      "Train Epoch: 1828\tLoss: 0.449897\n",
      "Train Epoch: 1829\tLoss: 0.443977\n",
      "Train Epoch: 1830\tLoss: 0.444166\n",
      "Train Epoch: 1831\tLoss: 0.445458\n",
      "Train Epoch: 1832\tLoss: 0.445883\n",
      "Train Epoch: 1833\tLoss: 0.442176\n",
      "Train Epoch: 1834\tLoss: 0.447235\n",
      "Train Epoch: 1835\tLoss: 0.443286\n",
      "Train Epoch: 1836\tLoss: 0.446900\n",
      "Train Epoch: 1837\tLoss: 0.446001\n",
      "Train Epoch: 1838\tLoss: 0.447791\n",
      "Train Epoch: 1839\tLoss: 0.438294\n",
      "Train Epoch: 1840\tLoss: 0.447037\n",
      "Train Epoch: 1841\tLoss: 0.439731\n",
      "Train Epoch: 1842\tLoss: 0.447666\n",
      "Train Epoch: 1843\tLoss: 0.445085\n",
      "Train Epoch: 1844\tLoss: 0.447605\n",
      "Train Epoch: 1845\tLoss: 0.449822\n",
      "Train Epoch: 1846\tLoss: 0.444581\n",
      "Train Epoch: 1847\tLoss: 0.443323\n",
      "Train Epoch: 1848\tLoss: 0.440099\n",
      "Train Epoch: 1849\tLoss: 0.444663\n",
      "Train Epoch: 1850\tLoss: 0.447372\n",
      "Train Epoch: 1851\tLoss: 0.453184\n",
      "Train Epoch: 1852\tLoss: 0.447444\n",
      "Train Epoch: 1853\tLoss: 0.436404\n",
      "Train Epoch: 1854\tLoss: 0.449940\n",
      "Train Epoch: 1855\tLoss: 0.447274\n",
      "Train Epoch: 1856\tLoss: 0.441695\n",
      "Train Epoch: 1857\tLoss: 0.443627\n",
      "Train Epoch: 1858\tLoss: 0.446236\n",
      "Train Epoch: 1859\tLoss: 0.451884\n",
      "Train Epoch: 1860\tLoss: 0.444339\n",
      "Train Epoch: 1861\tLoss: 0.444600\n",
      "Train Epoch: 1862\tLoss: 0.450726\n",
      "Train Epoch: 1863\tLoss: 0.442245\n",
      "Train Epoch: 1864\tLoss: 0.450480\n",
      "Train Epoch: 1865\tLoss: 0.448114\n",
      "Train Epoch: 1866\tLoss: 0.445320\n",
      "Train Epoch: 1867\tLoss: 0.442203\n",
      "Train Epoch: 1868\tLoss: 0.446283\n",
      "Train Epoch: 1869\tLoss: 0.442105\n",
      "Train Epoch: 1870\tLoss: 0.446735\n",
      "Train Epoch: 1871\tLoss: 0.445141\n",
      "Train Epoch: 1872\tLoss: 0.446309\n",
      "Train Epoch: 1873\tLoss: 0.453010\n",
      "Train Epoch: 1874\tLoss: 0.443510\n",
      "Train Epoch: 1875\tLoss: 0.445556\n",
      "Train Epoch: 1876\tLoss: 0.445842\n",
      "Train Epoch: 1877\tLoss: 0.444890\n",
      "Train Epoch: 1878\tLoss: 0.443697\n",
      "Train Epoch: 1879\tLoss: 0.439545\n",
      "Train Epoch: 1880\tLoss: 0.441043\n",
      "Train Epoch: 1881\tLoss: 0.445774\n",
      "Train Epoch: 1882\tLoss: 0.446116\n",
      "Train Epoch: 1883\tLoss: 0.452877\n",
      "Train Epoch: 1884\tLoss: 0.439582\n",
      "Train Epoch: 1885\tLoss: 0.445631\n",
      "Train Epoch: 1886\tLoss: 0.443107\n",
      "Train Epoch: 1887\tLoss: 0.439913\n",
      "Train Epoch: 1888\tLoss: 0.441600\n",
      "Train Epoch: 1889\tLoss: 0.442030\n",
      "Train Epoch: 1890\tLoss: 0.448044\n",
      "Train Epoch: 1891\tLoss: 0.442627\n",
      "Train Epoch: 1892\tLoss: 0.446673\n",
      "Train Epoch: 1893\tLoss: 0.444967\n",
      "Train Epoch: 1894\tLoss: 0.442689\n",
      "Train Epoch: 1895\tLoss: 0.443926\n",
      "Train Epoch: 1896\tLoss: 0.447772\n",
      "Train Epoch: 1897\tLoss: 0.449850\n",
      "Train Epoch: 1898\tLoss: 0.440446\n",
      "Train Epoch: 1899\tLoss: 0.441678\n",
      "Train Epoch: 1900\tLoss: 0.441839\n",
      "elapsed: 2118.357212781906\n",
      "\n",
      " Validity is 77.61%: valency issue: 15.90%, connectivity issue: 7.52%. \n",
      " Uniqueness is 71.79%. \n",
      " Novelty is 84.87%. \n",
      " Number of 3-member cycles is: 3647. \n",
      " Number of cycles with triple bonds is: 134. \n",
      "\n",
      "Functional Group frequencies (percentage):\n",
      " Acetylenic carbon: 10.37\n",
      " Aldehyde         : 9.44\n",
      " Alkyl carbon     : 97.95\n",
      " Amide            : 3.66\n",
      " Amino acid       : 0.38\n",
      " Carbonyl         : 28.59\n",
      " Carboxylic acid  : 1.06\n",
      " Ester            : 2.05\n",
      " Ether            : 41.26\n",
      " Halide           : 0.43\n",
      " Hydrazone        : 1.15\n",
      " Hydroxyl         : 41.56\n",
      " Ketone           : 8.70\n",
      " Nitrile          : 8.18\n",
      " Primary amines   : 13.28\n",
      " Secondary amines : 32.90\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CcvSOveG4dY"
   },
   "source": [
    "to train the base model. To run the constrained model, set the regularization parameters `mu_reg_1`, `mu_reg_2`, `mu_reg_3`, and `mu_reg_4` to a positive value and tune them based on the output statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhhG0DKfG8gl"
   },
   "source": [
    "### Conditional design\n",
    "\n",
    "This code performs conditional design by setting a target property value for the sampled molecules. Set the property ID with argument `y_id` (0: PSA, 1: MolWt, 2: LogP) and the target value with `y_target`.\n",
    "\n",
    "### Quantifying uncertainties\n",
    "\n",
    "To perform UQ analysis, use `utils.py`. The `utils.py` script accepts the following arguments:\n",
    "\n",
    "```bash\n",
    "optional arguments:\n",
    "  --BB_samples          number of samples for uncertainty quantification (default: 0)\n",
    "  --N                   number of training data (default: 600)\n",
    "  --database            name of the training database (default: 'QM9')\n",
    "  --sample_file         predictive samples directory (default: 'BB_600')\n",
    "  --gpu_mode            accelerate the script using GPU (default: 0)\n",
    "```\n",
    "\n",
    "To compute the confidence interval, use the following example script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4o19LpNG4vg"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ITR=25\n",
    "DIR=B_200\n",
    "N=200\n",
    "\n",
    "for i in `seq 1 ${ITR}`;\n",
    "do\n",
    "    python main.py --N \"$N\" --BB_samples \"$i\" --res results/\"${DIR}\"\n",
    "done\n",
    "\n",
    "mkdir data/samples\n",
    "mkdir data/samples/${DIR}\n",
    "\n",
    "mv results/\"${DIR}\"/*/samples_*.data data/samples/${DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gZDyuuBHLaa"
   },
   "outputs": [],
   "source": [
    "!python utils.py --BB_samples \"$ITR\" --N \"$N\" --sample_file \"${DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zx0KhbYIOMb"
   },
   "source": [
    "### Filters\n",
    "\n",
    "You can run `filter.py` independently in order to perform scattering transform and visualize graph filters. The `filter.py` script accepts the following arguments:\n",
    "\n",
    "```bash\n",
    "optional arguments:\n",
    "  --gpu_mode            accelerate the script using GPU (default: 0)\n",
    "  --wlt_scales          number of wavelet scales (default: 12)\n",
    "  --scat_layers         number of scattering layers (default: 4)\n",
    "  --N                   number of training data (default: 600)\n",
    "  --database            name of the training database (default: 'QM9')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GSVAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
