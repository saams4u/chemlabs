{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkMsn_QBLDG_"
   },
   "source": [
    "## Deep Inverse Reinforcement Learning for Structural Evolution of Small Molecules\n",
    "\n",
    "ABSTRACT: The size and quality of chemical libraries to the drug discovery pipeline are crucial for developing\n",
    "new drugs or repurposing existing drugs. Existing techniques such as combinatorial organic synthesis and High-Throughput Screening usually make the process extraordinarily tough and complicated\n",
    "since the search space of synthetically feasible drugs is exorbitantly huge. While reinforcement\n",
    "learning has been mostly exploited in the literature for generating novel compounds, the requirement of designing a reward function that succinctly represents the learning objective could prove\n",
    "daunting in certain complex domains. Generative Adversarial Network-based methods also mostly\n",
    "discard the discriminator after training and could be hard to train. In this study, we propose a framework for training a compound generator and learning a transferable reward function based on the\n",
    "entropy maximization inverse reinforcement learning paradigm. We show from our experiments\n",
    "that the inverse reinforcement learning route offers a rational alternative for generating chemical\n",
    "compounds in domains where reward function engineering may be less appealing or impossible\n",
    "while data exhibiting the desired objective is readily available.\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/2008.11804v2.pdf\n",
    "\n",
    "Credit: https://github.com/bbrighttaer/irelease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpG8CEReLBv_",
    "outputId": "1e312a97-bb90-4630-e60e-d9e95fba24ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'irelease'...\n",
      "remote: Enumerating objects: 2268, done.\u001b[K\n",
      "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
      "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
      "remote: Total 2268 (delta 131), reused 118 (delta 44), pack-reused 2059\u001b[K\n",
      "Receiving objects: 100% (2268/2268), 120.11 MiB | 30.61 MiB/s, done.\n",
      "Resolving deltas: 100% (1683/1683), done.\n",
      "Checking out files: 100% (277/277), done.\n",
      "/content/irelease\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository and cd into directory\n",
    "!git clone https://github.com/bbrighttaer/irelease.git\n",
    "%cd irelease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TkM3eeALDre"
   },
   "outputs": [],
   "source": [
    "# Install dependencies / requirements\n",
    "!pip install gym==0.15.6 rdkit-pypi==2021.3.1.5 ptan==0.6 xgboost==0.90\n",
    "\n",
    "# Install soek module\n",
    "!git clone https://github.com/bbrighttaer/soek.git\n",
    "%cd irelease/soek\n",
    "!python setup.py install\n",
    "%cd irelease/proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwK7U9vMMXjB"
   },
   "source": [
    "### Pretraining\n",
    "The Stack-RNN model used in our work could be pretrained with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJ_JtM-AL00n",
    "outputId": "9c8bbef8-9ed7-4342-e516-9398016ad270"
   },
   "outputs": [],
   "source": [
    "!cp -a irelease/irelease /content/irelease/proj/\n",
    "%cd irelease/proj\n",
    "!python pretrain_rnn.py --data ../data/chembl.smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNsGg-maO3jX"
   },
   "source": [
    "### Evaluation Functions\n",
    "#### DRD2 Activity\n",
    "\n",
    "The evaluation function for the DRD2 experiment is an RNN classifier trained with the BCE loss function. The following is the command to train the model using 5-fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VorzYz_MaoP"
   },
   "outputs": [],
   "source": [
    "!python expert_rnn_bin.py --data_file ../data/drd2_bin_balanced.csv --cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfHm4gzqO-6o"
   },
   "source": [
    "After training, the evaluation can be done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D60Q5AyEO_RE"
   },
   "outputs": [],
   "source": [
    "! python expert_rnn_bin.py --data_file ../data/drd2_bin_balanced.csv --cv --eval --eval_model_dir ./model_dir/expert_rnn_bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aqg-xMwRHLs"
   },
   "source": [
    "The value of the `--eval_model_dir` flag is a directory which contains the 5 models saved from the CV training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHyWWbbaRN5I"
   },
   "source": [
    "#### LogP\n",
    "The evaluation function for the LogP optimization experiment is an RNN model trained using the MSE loss function. The following command invokes training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fClUhrVLPei3"
   },
   "outputs": [],
   "source": [
    "!python expert_rnn_reg.py --data_file ../data/logP_labels.csv --cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0EVklfCRSya"
   },
   "source": [
    "After training, the evaluation can be done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HzeyVKXRTGy"
   },
   "outputs": [],
   "source": [
    "!python expert_rnn_reg.py --data_file ../data/logP_labels.csv --cv --eval --eval_model_dir ./model_dir/expert_rnn_reg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P02R1J8fRZcN"
   },
   "source": [
    "#### JAK2\n",
    "We trained XGBoost models for the JAK2 maximization experiment. The same XGBoost models were used for the JAK2 minimization experiment, as mentioned in the paper.\n",
    "\n",
    "The following invokes the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q52Hf3HQRaDM"
   },
   "outputs": [],
   "source": [
    "!python expert_xgb_reg.py --data_file ../data/jak2_data.csv --cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgYKol0SRenz"
   },
   "source": [
    "And evaluation could be done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy7DmfF7RfUo"
   },
   "outputs": [],
   "source": [
    "!python expert_xgb_reg.py --data_file ../data/jak2_data.csv --cv --eval --eval_model_dir ./model_dir/expert_xgb_reg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNS4HYB5RtVF"
   },
   "source": [
    "### Training\n",
    "The following files are used for PPO training for both DIRL and IRL:\n",
    "\n",
    "- DRD2 Activity: `ppo_rl_drd2.py`\n",
    "- LogP Optimization: `ppo_rl_logp.py`\n",
    "- JAK2 Maximization: `ppo_rl_jak2_minmax.py`\n",
    "- JAK2 Minimization: `ppo_rl_jak2_min.py`\n",
    "\n",
    "For DRL training, the following files are used:\n",
    " \n",
    "- DRD2 Activity: `reinforce_rl_drd2.py`\n",
    "- LogP Optimization: `reinforce_rl_logp.py`\n",
    "- JAK2 Maximization: `reinforce_rl_jak2_minmax.py`\n",
    "- JAK2 Minimization: `ppo_rl_jak2_min.py`\n",
    "\n",
    "These files mostly share command line flags for training. For instance, to train\n",
    "a generator with the DRD2 demonstrations (DIRL) the following command could be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GE6sqG7RtsA"
   },
   "outputs": [],
   "source": [
    "!python ppo_rl_drd2.py  --exp_name drd2 --demo ../data/drd2_active_filtered.smi --unbiased ../data/unbiased_smiles.smi --prior_data ../data/chembl.smi --pretrained_model irelease_prior.mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USjNQwxUR2xy"
   },
   "source": [
    "For DRL just add the flag `--use_true_reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7oK3rVGR3GE"
   },
   "outputs": [],
   "source": [
    "!python ppo_rl_drd2.py  --exp_name drd2 --demo ../data/drd2_active_filtered.smi --unbiased ../data/unbiased_smiles.smi --prior_data ../data/chembl.smi --pretrained_model irelease_prior.mod --use_true_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVvmBpaiR8lB"
   },
   "source": [
    "### Compound Sampling\n",
    "Assuming the training phase produces the model `biased_generator.mod`, compound\n",
    "samples, in the form of SMILES, could be generated using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuDQod14R89J"
   },
   "outputs": [],
   "source": [
    "!python pretrain_rnn.py --data ../data/chembl.smi --eval --eval_model_name biased_generator.mod --num_smiles 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnqY7lKKSFaY"
   },
   "source": [
    "The `--num_smiles` flag controls the number of SMILES (valid and invalid) that would be sampled from the\n",
    "generator.\n",
    "\n",
    "After the generation, a JSON file is produced which contains valid and invalid\n",
    "SMILES. In our experiments, we process this `.json` file using \n",
    "[smiles_worker.py](https://github.com/bbrighttaer/irelease/blob/master/proj/smiles_worker.py) to save the valid SMILES into a CSV file. \n",
    "\n",
    "A sample file JSON file produced after SMILES generation is \n",
    "[here](https://github.com/bbrighttaer/irelease/blob/master/proj/analysis/DRD2_activity_smiles_biased_ppo_grl_eval.json).\n",
    "The corresponding processed CSV file containing the valid SMILES and \n",
    "the evaluation function's \n",
    "predictions is also [here](https://github.com/bbrighttaer/irelease/blob/master/proj/analysis/DRD2_activity_smiles_biased_ppo_grl_eval.csv)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IReLeaSE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
